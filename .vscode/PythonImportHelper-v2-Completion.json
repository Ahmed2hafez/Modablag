[
    {
        "label": "os",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "os",
        "description": "os",
        "detail": "os",
        "documentation": {}
    },
    {
        "label": "keras",
        "importPath": "tensorflow",
        "description": "tensorflow",
        "isExtraImport": true,
        "detail": "tensorflow",
        "documentation": {}
    },
    {
        "label": "float32",
        "importPath": "tensorflow",
        "description": "tensorflow",
        "isExtraImport": true,
        "detail": "tensorflow",
        "documentation": {}
    },
    {
        "label": "Tensor",
        "importPath": "tensorflow",
        "description": "tensorflow",
        "isExtraImport": true,
        "detail": "tensorflow",
        "documentation": {}
    },
    {
        "label": "is_tensor",
        "importPath": "tensorflow",
        "description": "tensorflow",
        "isExtraImport": true,
        "detail": "tensorflow",
        "documentation": {}
    },
    {
        "label": "convert_to_tensor",
        "importPath": "tensorflow",
        "description": "tensorflow",
        "isExtraImport": true,
        "detail": "tensorflow",
        "documentation": {}
    },
    {
        "label": "is_tensor",
        "importPath": "tensorflow",
        "description": "tensorflow",
        "isExtraImport": true,
        "detail": "tensorflow",
        "documentation": {}
    },
    {
        "label": "convert_to_tensor",
        "importPath": "tensorflow",
        "description": "tensorflow",
        "isExtraImport": true,
        "detail": "tensorflow",
        "documentation": {}
    },
    {
        "label": "float32",
        "importPath": "tensorflow",
        "description": "tensorflow",
        "isExtraImport": true,
        "detail": "tensorflow",
        "documentation": {}
    },
    {
        "label": "Tensor",
        "importPath": "tensorflow",
        "description": "tensorflow",
        "isExtraImport": true,
        "detail": "tensorflow",
        "documentation": {}
    },
    {
        "label": "cast",
        "importPath": "tensorflow",
        "description": "tensorflow",
        "isExtraImport": true,
        "detail": "tensorflow",
        "documentation": {}
    },
    {
        "label": "float32",
        "importPath": "tensorflow",
        "description": "tensorflow",
        "isExtraImport": true,
        "detail": "tensorflow",
        "documentation": {}
    },
    {
        "label": "newaxis",
        "importPath": "tensorflow",
        "description": "tensorflow",
        "isExtraImport": true,
        "detail": "tensorflow",
        "documentation": {}
    },
    {
        "label": "Tensor",
        "importPath": "tensorflow",
        "description": "tensorflow",
        "isExtraImport": true,
        "detail": "tensorflow",
        "documentation": {}
    },
    {
        "label": "is_tensor",
        "importPath": "tensorflow",
        "description": "tensorflow",
        "isExtraImport": true,
        "detail": "tensorflow",
        "documentation": {}
    },
    {
        "label": "convert_to_tensor",
        "importPath": "tensorflow",
        "description": "tensorflow",
        "isExtraImport": true,
        "detail": "tensorflow",
        "documentation": {}
    },
    {
        "label": "float32",
        "importPath": "tensorflow",
        "description": "tensorflow",
        "isExtraImport": true,
        "detail": "tensorflow",
        "documentation": {}
    },
    {
        "label": "Tensor",
        "importPath": "tensorflow",
        "description": "tensorflow",
        "isExtraImport": true,
        "detail": "tensorflow",
        "documentation": {}
    },
    {
        "label": "is_tensor",
        "importPath": "tensorflow",
        "description": "tensorflow",
        "isExtraImport": true,
        "detail": "tensorflow",
        "documentation": {}
    },
    {
        "label": "convert_to_tensor",
        "importPath": "tensorflow",
        "description": "tensorflow",
        "isExtraImport": true,
        "detail": "tensorflow",
        "documentation": {}
    },
    {
        "label": "numpy",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "numpy",
        "description": "numpy",
        "detail": "numpy",
        "documentation": {}
    },
    {
        "label": "ndarray",
        "importPath": "numpy",
        "description": "numpy",
        "isExtraImport": true,
        "detail": "numpy",
        "documentation": {}
    },
    {
        "label": "ndarray",
        "importPath": "numpy",
        "description": "numpy",
        "isExtraImport": true,
        "detail": "numpy",
        "documentation": {}
    },
    {
        "label": "ndarray",
        "importPath": "numpy",
        "description": "numpy",
        "isExtraImport": true,
        "detail": "numpy",
        "documentation": {}
    },
    {
        "label": "Union",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "List",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Union",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "List",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Union",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "List",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "SinuSoidal",
        "importPath": "posEncoders",
        "description": "posEncoders",
        "isExtraImport": true,
        "detail": "posEncoders",
        "documentation": {}
    },
    {
        "label": "SinuSoidal",
        "importPath": "posEncoders",
        "description": "posEncoders",
        "isExtraImport": true,
        "detail": "posEncoders",
        "documentation": {}
    },
    {
        "label": "EncoderLayer",
        "importPath": "utils",
        "description": "utils",
        "isExtraImport": true,
        "detail": "utils",
        "documentation": {}
    },
    {
        "label": "DecoderLayer",
        "importPath": "utils",
        "description": "utils",
        "isExtraImport": true,
        "detail": "utils",
        "documentation": {}
    },
    {
        "label": "*",
        "importPath": "utils",
        "description": "utils",
        "isExtraImport": true,
        "detail": "utils",
        "documentation": {}
    },
    {
        "label": "Sequential",
        "importPath": "tensorflow.keras",
        "description": "tensorflow.keras",
        "isExtraImport": true,
        "detail": "tensorflow.keras",
        "documentation": {}
    },
    {
        "label": "Layer",
        "importPath": "tensorflow.keras.layers",
        "description": "tensorflow.keras.layers",
        "isExtraImport": true,
        "detail": "tensorflow.keras.layers",
        "documentation": {}
    },
    {
        "label": "MultiHeadAttention",
        "importPath": "tensorflow.keras.layers",
        "description": "tensorflow.keras.layers",
        "isExtraImport": true,
        "detail": "tensorflow.keras.layers",
        "documentation": {}
    },
    {
        "label": "LayerNormalization",
        "importPath": "tensorflow.keras.layers",
        "description": "tensorflow.keras.layers",
        "isExtraImport": true,
        "detail": "tensorflow.keras.layers",
        "documentation": {}
    },
    {
        "label": "Add",
        "importPath": "tensorflow.keras.layers",
        "description": "tensorflow.keras.layers",
        "isExtraImport": true,
        "detail": "tensorflow.keras.layers",
        "documentation": {}
    },
    {
        "label": "serialize",
        "importPath": "tensorflow.keras.layers",
        "description": "tensorflow.keras.layers",
        "isExtraImport": true,
        "detail": "tensorflow.keras.layers",
        "documentation": {}
    },
    {
        "label": "deserialize",
        "importPath": "tensorflow.keras.layers",
        "description": "tensorflow.keras.layers",
        "isExtraImport": true,
        "detail": "tensorflow.keras.layers",
        "documentation": {}
    },
    {
        "label": "Dropout",
        "importPath": "tensorflow.keras.layers",
        "description": "tensorflow.keras.layers",
        "isExtraImport": true,
        "detail": "tensorflow.keras.layers",
        "documentation": {}
    },
    {
        "label": "Dense",
        "importPath": "tensorflow.keras.layers",
        "description": "tensorflow.keras.layers",
        "isExtraImport": true,
        "detail": "tensorflow.keras.layers",
        "documentation": {}
    },
    {
        "label": "Layer",
        "importPath": "tensorflow.keras.layers",
        "description": "tensorflow.keras.layers",
        "isExtraImport": true,
        "detail": "tensorflow.keras.layers",
        "documentation": {}
    },
    {
        "label": "Embedding",
        "importPath": "tensorflow.keras.layers",
        "description": "tensorflow.keras.layers",
        "isExtraImport": true,
        "detail": "tensorflow.keras.layers",
        "documentation": {}
    },
    {
        "label": "sqrt",
        "importPath": "tensorflow.math",
        "description": "tensorflow.math",
        "isExtraImport": true,
        "detail": "tensorflow.math",
        "documentation": {}
    },
    {
        "label": "tqdm",
        "importPath": "tqdm",
        "description": "tqdm",
        "isExtraImport": true,
        "detail": "tqdm",
        "documentation": {}
    },
    {
        "label": "*",
        "importPath": "baseT",
        "description": "baseT",
        "isExtraImport": true,
        "detail": "baseT",
        "documentation": {}
    },
    {
        "label": "Encoder",
        "kind": 6,
        "importPath": "transformer.baseT",
        "description": "transformer.baseT",
        "peekOfCode": "class Encoder(keras.Model):\n    '''\n    the encoder Module from the paper. consist of many stacked Encoder Layer\n    '''\n    def __init__(self, vocab_size: int, emp_dim: int, max_sent_lenght: int,\n                 key_dim: int, n_heads: int, n_layers: int=1):\n        '''\n        @params:\n                vocab_size:        int.  Size of the vocabulary, i.e. maximum integer index + 1.\n                emp_dim:         int.  Dimension of the dense embedding.",
        "detail": "transformer.baseT",
        "documentation": {}
    },
    {
        "label": "Decoder",
        "kind": 6,
        "importPath": "transformer.baseT",
        "description": "transformer.baseT",
        "peekOfCode": "class Decoder(keras.Model):\n    '''\n    the Decoder Module from the paper. consist of many stacked Decoder Layer\n    '''\n    def __init__(self, vocab_size: int, emp_dim: int, max_sent_lenght: int,\n                 key_dim: int, n_heads: int, n_layers: int=1):\n        '''\n        @params:\n                vocab_size:        int.  Size of the vocabulary, i.e. maximum integer index + 1.\n                emp_dim:         int.  Dimension of the dense embedding.",
        "detail": "transformer.baseT",
        "documentation": {}
    },
    {
        "label": "BTransformer",
        "kind": 6,
        "importPath": "transformer.baseT",
        "description": "transformer.baseT",
        "peekOfCode": "class BTransformer(keras.Model):\n    '''\n    the Transformer Module from the paper. consist of Encoder Module & Decoder Module. This class also Implement\n    Greedy and Beam search decoding.\n    '''\n    def __init__(self, e_config: tuple, d_config: tuple):\n        '''\n        @params:\n            e_config:              Tuple\n                - vocab_size:      int.  Size of the soruce lang vocabulary, i.e. maximum integer index + 1.",
        "detail": "transformer.baseT",
        "documentation": {}
    },
    {
        "label": "os.environ['TF_CPP_MIN_LOG_LEVEL']",
        "kind": 5,
        "importPath": "transformer.baseT",
        "description": "transformer.baseT",
        "peekOfCode": "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\nfrom tensorflow import keras\nfrom tensorflow import float32, Tensor, is_tensor, convert_to_tensor\nimport numpy as np\nfrom numpy import ndarray\nfrom typing import Union, List\nfrom posEncoders import SinuSoidal\nfrom utils import EncoderLayer, DecoderLayer\nclass Encoder(keras.Model):\n    '''",
        "detail": "transformer.baseT",
        "documentation": {}
    },
    {
        "label": "BaseAttention",
        "kind": 6,
        "importPath": "transformer.modules",
        "description": "transformer.modules",
        "peekOfCode": "class BaseAttention(Layer):\n    '''\n    the structure of self attention from the paper.\n    consist of MultiHeadAttention and Add&Normlize block \n    '''\n    def __init__(self, num_heads: int, key_dim: int, **kwargs):\n        '''\n        @params:\n                num_heads:                          int.    Number of attention heads. \n                key_dim:                            int.    Size of each attention head for query and key.",
        "detail": "transformer.modules",
        "documentation": {}
    },
    {
        "label": "SelfAttention",
        "kind": 6,
        "importPath": "transformer.modules",
        "description": "transformer.modules",
        "peekOfCode": "class SelfAttention(BaseAttention):\n    '''\n    self attention part in the encoder from the paper. consist of MultiHeadAttention and Add&Normlize block \n    '''\n    def call(self, query: Union[Tensor, ndarray, List], key: Union[Tensor, ndarray, List],\n             value: Union[Tensor, ndarray, List], causal_mask: bool=False, \n             return_score=False, training=False, **kwargs): \n        '''\n        @params\n            query:          3D matrix.  Query Tensor of shape (B, T, dim).",
        "detail": "transformer.modules",
        "documentation": {}
    },
    {
        "label": "FeedForward",
        "kind": 6,
        "importPath": "transformer.modules",
        "description": "transformer.modules",
        "peekOfCode": "class FeedForward(Layer):\n    '''\n    FeedForward part in the encoder from the paper.\n    '''\n    def __init__(self, units: list, dropout=0.1):\n        '''\n        @params:\n                units:   list.  Number of units at each dense layer, make sure that the number of\n                                      units at the last layer same as the inputs to the layer.\n                dropout: float. dropout ratio before Add&Normlize layer",
        "detail": "transformer.modules",
        "documentation": {}
    },
    {
        "label": "EncoderLayer",
        "kind": 6,
        "importPath": "transformer.modules",
        "description": "transformer.modules",
        "peekOfCode": "class EncoderLayer(Layer):\n    '''\n    the encoder structure from the paper. consist of SelfAttention & FeedForward\n    '''\n    def __init__(self, key_dim: int, num_heads: int, output_shape: int, dropout=0.1):\n        '''\n        num_heads:     int.    Number of attention heads. \n        key_dim:       int.    Size of each attention head for query and key.\n        dropout=0.1:   float.  Dropout probability.\n        output_shape:  int.    The expected shape of an output tensor, besides the batch and sequence dims. ",
        "detail": "transformer.modules",
        "documentation": {}
    },
    {
        "label": "DecoderLayer",
        "kind": 6,
        "importPath": "transformer.modules",
        "description": "transformer.modules",
        "peekOfCode": "class DecoderLayer(Layer):\n    '''\n    the decoder structure from the paper. consist of SelfAttention & CrossAttention & FeedForward\n    '''\n    def __init__(self, key_dim:int , num_heads: int, output_shape: int, dropout=0.1):\n        '''\n        num_heads:     int.    Number of attention heads. \n        key_dim:       int.    Size of each attention head for query and key.\n        dropout=0.1:   float.  Dropout probability.\n        output_shape:  int.    The expected shape of an output tensor, besides the batch and sequence dims. ",
        "detail": "transformer.modules",
        "documentation": {}
    },
    {
        "label": "os.environ['TF_CPP_MIN_LOG_LEVEL']",
        "kind": 5,
        "importPath": "transformer.modules",
        "description": "transformer.modules",
        "peekOfCode": "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\nfrom tensorflow.keras import Sequential\nfrom tensorflow.keras.layers import (Layer, MultiHeadAttention, LayerNormalization, \n                              Add, serialize, deserialize, Dropout, Dense)\nfrom tensorflow import is_tensor, convert_to_tensor, float32, Tensor\nfrom numpy import ndarray\nfrom typing import Union, List\nclass BaseAttention(Layer):\n    '''\n    the structure of self attention from the paper.",
        "detail": "transformer.modules",
        "documentation": {}
    },
    {
        "label": "SinuSoidal",
        "kind": 6,
        "importPath": "transformer.posEncoders",
        "description": "transformer.posEncoders",
        "peekOfCode": "class SinuSoidal(Layer):\n    '''\n    Custom layer that get the emppeding of words with positons info\n    as propoased on Attention is all you need paper -> https://arxiv.org/abs/1706.03762 \n    '''\n    def __init__(self, voc_size: int, output_dim: int, max_sent_lenght: int, mask_zero: bool=False, **kwargs):\n        '''instantiate Empedding layer and static positional encoding matrix\n        @params:\n                voc_size:        int.  Size of the vocabulary, i.e. maximum integer index + 1.\n                output_dim:      int.  Dimension of the dense embedding.",
        "detail": "transformer.posEncoders",
        "documentation": {}
    },
    {
        "label": "os.environ['TF_CPP_MIN_LOG_LEVEL']",
        "kind": 5,
        "importPath": "transformer.posEncoders",
        "description": "transformer.posEncoders",
        "peekOfCode": "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n# Don't worry of warning under Tensorflow it's ok.\nfrom tensorflow.keras.layers import Layer, Embedding\nfrom tensorflow import cast, float32, newaxis, Tensor, is_tensor, convert_to_tensor\nfrom tensorflow.math import sqrt\nfrom tensorflow import float32, Tensor, is_tensor, convert_to_tensor\nfrom numpy import ndarray\nfrom typing import Union, List\nimport numpy as np\nclass SinuSoidal(Layer):",
        "detail": "transformer.posEncoders",
        "documentation": {}
    },
    {
        "label": "create_dummy",
        "kind": 2,
        "importPath": "transformer.unit_test",
        "description": "transformer.unit_test",
        "peekOfCode": "def create_dummy(voc_size, batch, time_step, feature=None):\n    if feature is None:\n        return np.random.randint(0, voc_size, (batch, time_step))\n    return np.random.randn(batch, time_step, feature)\ndef test_SinuSoidal(): #voc_size: int, output_dim: int, max_sent_lenght: int, mask_zero\n    for v in tqdm(np.arange(1_000, 70_000, 1_000)):          #voc_size: int,\n        for o in np.arange(100, 700, 100):  #output_dim: int,\n            for m in np.arange(0, 2_000, 200):      #max_sent_lenght: int,\n                dummy = create_dummy(v, 128, m)\n                # for ms in mask_zero:",
        "detail": "transformer.unit_test",
        "documentation": {}
    },
    {
        "label": "test_SinuSoidal",
        "kind": 2,
        "importPath": "transformer.unit_test",
        "description": "transformer.unit_test",
        "peekOfCode": "def test_SinuSoidal(): #voc_size: int, output_dim: int, max_sent_lenght: int, mask_zero\n    for v in tqdm(np.arange(1_000, 70_000, 1_000)):          #voc_size: int,\n        for o in np.arange(100, 700, 100):  #output_dim: int,\n            for m in np.arange(0, 2_000, 200):      #max_sent_lenght: int,\n                dummy = create_dummy(v, 128, m)\n                # for ms in mask_zero:\n                opj = SinuSoidal(v, o, m)\n                try:\n                    opj(dummy)\n                    # opj.compute_mask(dummy)",
        "detail": "transformer.unit_test",
        "documentation": {}
    },
    {
        "label": "test_self_attention",
        "kind": 2,
        "importPath": "transformer.unit_test",
        "description": "transformer.unit_test",
        "peekOfCode": "def test_self_attention():\n    pass\ndef test_feed_forward():\n    pass\ndef test_enc_layer():\n    pass\ndef test_dec_layer():\n    pass\ndef test_enc():\n    pass",
        "detail": "transformer.unit_test",
        "documentation": {}
    },
    {
        "label": "test_feed_forward",
        "kind": 2,
        "importPath": "transformer.unit_test",
        "description": "transformer.unit_test",
        "peekOfCode": "def test_feed_forward():\n    pass\ndef test_enc_layer():\n    pass\ndef test_dec_layer():\n    pass\ndef test_enc():\n    pass\ndef test_dec():\n    pass",
        "detail": "transformer.unit_test",
        "documentation": {}
    },
    {
        "label": "test_enc_layer",
        "kind": 2,
        "importPath": "transformer.unit_test",
        "description": "transformer.unit_test",
        "peekOfCode": "def test_enc_layer():\n    pass\ndef test_dec_layer():\n    pass\ndef test_enc():\n    pass\ndef test_dec():\n    pass\ndef test_model():\n    dummy = create_dummy(10000, 128, 10)",
        "detail": "transformer.unit_test",
        "documentation": {}
    },
    {
        "label": "test_dec_layer",
        "kind": 2,
        "importPath": "transformer.unit_test",
        "description": "transformer.unit_test",
        "peekOfCode": "def test_dec_layer():\n    pass\ndef test_enc():\n    pass\ndef test_dec():\n    pass\ndef test_model():\n    dummy = create_dummy(10000, 128, 10)\n    # #, vocab_size, emp_dim, max_sent_lenght, key_dim, n_heads, n_layers=1\n    # #, vocab_size, emp_dim, max_sent_lenght, key_dim, n_heads, n_layers=1",
        "detail": "transformer.unit_test",
        "documentation": {}
    },
    {
        "label": "test_enc",
        "kind": 2,
        "importPath": "transformer.unit_test",
        "description": "transformer.unit_test",
        "peekOfCode": "def test_enc():\n    pass\ndef test_dec():\n    pass\ndef test_model():\n    dummy = create_dummy(10000, 128, 10)\n    # #, vocab_size, emp_dim, max_sent_lenght, key_dim, n_heads, n_layers=1\n    # #, vocab_size, emp_dim, max_sent_lenght, key_dim, n_heads, n_layers=1\n    # # e_config: vocab_size, emp_dim, max_sent_lenght, num_heads, key_dim, n_layers\n    # # d_config: vocab_size, emp_dim, max_sent_lenght, num_heads, key_dim, n_layers",
        "detail": "transformer.unit_test",
        "documentation": {}
    },
    {
        "label": "test_dec",
        "kind": 2,
        "importPath": "transformer.unit_test",
        "description": "transformer.unit_test",
        "peekOfCode": "def test_dec():\n    pass\ndef test_model():\n    dummy = create_dummy(10000, 128, 10)\n    # #, vocab_size, emp_dim, max_sent_lenght, key_dim, n_heads, n_layers=1\n    # #, vocab_size, emp_dim, max_sent_lenght, key_dim, n_heads, n_layers=1\n    # # e_config: vocab_size, emp_dim, max_sent_lenght, num_heads, key_dim, n_layers\n    # # d_config: vocab_size, emp_dim, max_sent_lenght, num_heads, key_dim, n_layers\n    config_e = (10000, 512, 50, 8, 256, 3)\n    config_d = (2000, 512, 40, 8, 256, 3)",
        "detail": "transformer.unit_test",
        "documentation": {}
    },
    {
        "label": "test_model",
        "kind": 2,
        "importPath": "transformer.unit_test",
        "description": "transformer.unit_test",
        "peekOfCode": "def test_model():\n    dummy = create_dummy(10000, 128, 10)\n    # #, vocab_size, emp_dim, max_sent_lenght, key_dim, n_heads, n_layers=1\n    # #, vocab_size, emp_dim, max_sent_lenght, key_dim, n_heads, n_layers=1\n    # # e_config: vocab_size, emp_dim, max_sent_lenght, num_heads, key_dim, n_layers\n    # # d_config: vocab_size, emp_dim, max_sent_lenght, num_heads, key_dim, n_layers\n    config_e = (10000, 512, 50, 8, 256, 3)\n    config_d = (2000, 512, 40, 8, 256, 3)\n    m = BTransformer(config_e, config_d)\n    try:",
        "detail": "transformer.unit_test",
        "documentation": {}
    },
    {
        "label": "Dog",
        "kind": 6,
        "importPath": "tricks",
        "description": "tricks",
        "peekOfCode": "class Dog:\n    def __init__(self, name) -> None:\n        self.name = name\n    @property\n    def name(self):\n        return self._name\n    @name.setter\n    def name(self, name):\n        assert name.isalpha()\n        self._name = name",
        "detail": "tricks",
        "documentation": {}
    }
]