[
    {
        "label": "os",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "os",
        "description": "os",
        "detail": "os",
        "documentation": {}
    },
    {
        "label": "Layer",
        "importPath": "tensorflow.keras.layers",
        "description": "tensorflow.keras.layers",
        "isExtraImport": true,
        "detail": "tensorflow.keras.layers",
        "documentation": {}
    },
    {
        "label": "Layer",
        "importPath": "tensorflow.keras.layers",
        "description": "tensorflow.keras.layers",
        "isExtraImport": true,
        "detail": "tensorflow.keras.layers",
        "documentation": {}
    },
    {
        "label": "Layer",
        "importPath": "tensorflow.keras.layers",
        "description": "tensorflow.keras.layers",
        "isExtraImport": true,
        "detail": "tensorflow.keras.layers",
        "documentation": {}
    },
    {
        "label": "LayerNormalization",
        "importPath": "tensorflow.keras.layers",
        "description": "tensorflow.keras.layers",
        "isExtraImport": true,
        "detail": "tensorflow.keras.layers",
        "documentation": {}
    },
    {
        "label": "Add",
        "importPath": "tensorflow.keras.layers",
        "description": "tensorflow.keras.layers",
        "isExtraImport": true,
        "detail": "tensorflow.keras.layers",
        "documentation": {}
    },
    {
        "label": "Dropout",
        "importPath": "tensorflow.keras.layers",
        "description": "tensorflow.keras.layers",
        "isExtraImport": true,
        "detail": "tensorflow.keras.layers",
        "documentation": {}
    },
    {
        "label": "Dense",
        "importPath": "tensorflow.keras.layers",
        "description": "tensorflow.keras.layers",
        "isExtraImport": true,
        "detail": "tensorflow.keras.layers",
        "documentation": {}
    },
    {
        "label": "Layer",
        "importPath": "tensorflow.keras.layers",
        "description": "tensorflow.keras.layers",
        "isExtraImport": true,
        "detail": "tensorflow.keras.layers",
        "documentation": {}
    },
    {
        "label": "LayerNormalization",
        "importPath": "tensorflow.keras.layers",
        "description": "tensorflow.keras.layers",
        "isExtraImport": true,
        "detail": "tensorflow.keras.layers",
        "documentation": {}
    },
    {
        "label": "Layer",
        "importPath": "tensorflow.keras.layers",
        "description": "tensorflow.keras.layers",
        "isExtraImport": true,
        "detail": "tensorflow.keras.layers",
        "documentation": {}
    },
    {
        "label": "MultiHeadAttention",
        "importPath": "tensorflow.keras.layers",
        "description": "tensorflow.keras.layers",
        "isExtraImport": true,
        "detail": "tensorflow.keras.layers",
        "documentation": {}
    },
    {
        "label": "LayerNormalization",
        "importPath": "tensorflow.keras.layers",
        "description": "tensorflow.keras.layers",
        "isExtraImport": true,
        "detail": "tensorflow.keras.layers",
        "documentation": {}
    },
    {
        "label": "Add",
        "importPath": "tensorflow.keras.layers",
        "description": "tensorflow.keras.layers",
        "isExtraImport": true,
        "detail": "tensorflow.keras.layers",
        "documentation": {}
    },
    {
        "label": "serialize",
        "importPath": "tensorflow.keras.layers",
        "description": "tensorflow.keras.layers",
        "isExtraImport": true,
        "detail": "tensorflow.keras.layers",
        "documentation": {}
    },
    {
        "label": "deserialize",
        "importPath": "tensorflow.keras.layers",
        "description": "tensorflow.keras.layers",
        "isExtraImport": true,
        "detail": "tensorflow.keras.layers",
        "documentation": {}
    },
    {
        "label": "tensorflow",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "tensorflow",
        "description": "tensorflow",
        "detail": "tensorflow",
        "documentation": {}
    },
    {
        "label": "Tensor",
        "importPath": "tensorflow",
        "description": "tensorflow",
        "isExtraImport": true,
        "detail": "tensorflow",
        "documentation": {}
    },
    {
        "label": "Tensor",
        "importPath": "tensorflow",
        "description": "tensorflow",
        "isExtraImport": true,
        "detail": "tensorflow",
        "documentation": {}
    },
    {
        "label": "is_tensor",
        "importPath": "tensorflow",
        "description": "tensorflow",
        "isExtraImport": true,
        "detail": "tensorflow",
        "documentation": {}
    },
    {
        "label": "convert_to_tensor",
        "importPath": "tensorflow",
        "description": "tensorflow",
        "isExtraImport": true,
        "detail": "tensorflow",
        "documentation": {}
    },
    {
        "label": "float32",
        "importPath": "tensorflow",
        "description": "tensorflow",
        "isExtraImport": true,
        "detail": "tensorflow",
        "documentation": {}
    },
    {
        "label": "Tensor",
        "importPath": "tensorflow",
        "description": "tensorflow",
        "isExtraImport": true,
        "detail": "tensorflow",
        "documentation": {}
    },
    {
        "label": "cast",
        "importPath": "tensorflow",
        "description": "tensorflow",
        "isExtraImport": true,
        "detail": "tensorflow",
        "documentation": {}
    },
    {
        "label": "float32",
        "importPath": "tensorflow",
        "description": "tensorflow",
        "isExtraImport": true,
        "detail": "tensorflow",
        "documentation": {}
    },
    {
        "label": "newaxis",
        "importPath": "tensorflow",
        "description": "tensorflow",
        "isExtraImport": true,
        "detail": "tensorflow",
        "documentation": {}
    },
    {
        "label": "is_tensor",
        "importPath": "tensorflow",
        "description": "tensorflow",
        "isExtraImport": true,
        "detail": "tensorflow",
        "documentation": {}
    },
    {
        "label": "convert_to_tensor",
        "importPath": "tensorflow",
        "description": "tensorflow",
        "isExtraImport": true,
        "detail": "tensorflow",
        "documentation": {}
    },
    {
        "label": "float32",
        "importPath": "tensorflow",
        "description": "tensorflow",
        "isExtraImport": true,
        "detail": "tensorflow",
        "documentation": {}
    },
    {
        "label": "Tensor",
        "importPath": "tensorflow",
        "description": "tensorflow",
        "isExtraImport": true,
        "detail": "tensorflow",
        "documentation": {}
    },
    {
        "label": "keras",
        "importPath": "tensorflow",
        "description": "tensorflow",
        "isExtraImport": true,
        "detail": "tensorflow",
        "documentation": {}
    },
    {
        "label": "Tensor",
        "importPath": "tensorflow",
        "description": "tensorflow",
        "isExtraImport": true,
        "detail": "tensorflow",
        "documentation": {}
    },
    {
        "label": "keras",
        "importPath": "tensorflow",
        "description": "tensorflow",
        "isExtraImport": true,
        "detail": "tensorflow",
        "documentation": {}
    },
    {
        "label": "Tensor",
        "importPath": "tensorflow",
        "description": "tensorflow",
        "isExtraImport": true,
        "detail": "tensorflow",
        "documentation": {}
    },
    {
        "label": "numpy",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "numpy",
        "description": "numpy",
        "detail": "numpy",
        "documentation": {}
    },
    {
        "label": "ndarray",
        "importPath": "numpy",
        "description": "numpy",
        "isExtraImport": true,
        "detail": "numpy",
        "documentation": {}
    },
    {
        "label": "ndarray",
        "importPath": "numpy",
        "description": "numpy",
        "isExtraImport": true,
        "detail": "numpy",
        "documentation": {}
    },
    {
        "label": "ndarray",
        "importPath": "numpy",
        "description": "numpy",
        "isExtraImport": true,
        "detail": "numpy",
        "documentation": {}
    },
    {
        "label": "ndarray",
        "importPath": "numpy",
        "description": "numpy",
        "isExtraImport": true,
        "detail": "numpy",
        "documentation": {}
    },
    {
        "label": "ndarray",
        "importPath": "numpy",
        "description": "numpy",
        "isExtraImport": true,
        "detail": "numpy",
        "documentation": {}
    },
    {
        "label": "ndarray",
        "importPath": "numpy",
        "description": "numpy",
        "isExtraImport": true,
        "detail": "numpy",
        "documentation": {}
    },
    {
        "label": "Union",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "List",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Union",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "List",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Union",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Tuple",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Union",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "List",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Union",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "List",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Union",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "List",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Sequential",
        "importPath": "tensorflow.keras",
        "description": "tensorflow.keras",
        "isExtraImport": true,
        "detail": "tensorflow.keras",
        "documentation": {}
    },
    {
        "label": "tqdm",
        "importPath": "tqdm",
        "description": "tqdm",
        "isExtraImport": true,
        "detail": "tqdm",
        "documentation": {}
    },
    {
        "label": "tqdm",
        "importPath": "tqdm",
        "description": "tqdm",
        "isExtraImport": true,
        "detail": "tqdm",
        "documentation": {}
    },
    {
        "label": "SinuSoidal",
        "importPath": "posEncoders",
        "description": "posEncoders",
        "isExtraImport": true,
        "detail": "posEncoders",
        "documentation": {}
    },
    {
        "label": "*",
        "importPath": "baseT",
        "description": "baseT",
        "isExtraImport": true,
        "detail": "baseT",
        "documentation": {}
    },
    {
        "label": "BTConfig",
        "importPath": "baseT",
        "description": "baseT",
        "isExtraImport": true,
        "detail": "baseT",
        "documentation": {}
    },
    {
        "label": "BTransformer",
        "importPath": "baseT",
        "description": "baseT",
        "isExtraImport": true,
        "detail": "baseT",
        "documentation": {}
    },
    {
        "label": "dataclass",
        "importPath": "dataclasses",
        "description": "dataclasses",
        "isExtraImport": true,
        "detail": "dataclasses",
        "documentation": {}
    },
    {
        "label": "SinuSoidal",
        "importPath": "utils.posEncoders",
        "description": "utils.posEncoders",
        "isExtraImport": true,
        "detail": "utils.posEncoders",
        "documentation": {}
    },
    {
        "label": "EncoderLayer",
        "importPath": "utils.encoder",
        "description": "utils.encoder",
        "isExtraImport": true,
        "detail": "utils.encoder",
        "documentation": {}
    },
    {
        "label": "DecoderLayer",
        "kind": 6,
        "importPath": "task3.utils.decoder",
        "description": "task3.utils.decoder",
        "peekOfCode": "class DecoderLayer(Layer):\n    '''\n    the decoder structure from the paper. consist of SelfAttention & CrossAttention & FeedForward\n    '''\n    def __init__(self, key_dim:int , num_heads: int, output_shape: int, dropout=0.1):\n        '''\n        num_heads:     int.    Number of attention heads. \n        key_dim:       int.    Size of each attention head for query and key.\n        dropout=0.1:   float.  Dropout probability.\n        output_shape:  int.    The expected shape of an output tensor, besides the batch and sequence dims. ",
        "detail": "task3.utils.decoder",
        "documentation": {}
    },
    {
        "label": "os.environ['TF_CPP_MIN_LOG_LEVEL']",
        "kind": 5,
        "importPath": "task3.utils.decoder",
        "description": "task3.utils.decoder",
        "peekOfCode": "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\ntry:\n    from selfAttention import Former\n    from feedForward import FForward\nexcept ImportError:\n    from .selfAttention import Former\n    from .feedForward import FForward\nfrom tensorflow.keras.layers import Layer\nfrom tensorflow import Tensor\nfrom numpy import ndarray",
        "detail": "task3.utils.decoder",
        "documentation": {}
    },
    {
        "label": "FEncoderLayer",
        "kind": 6,
        "importPath": "task3.utils.encoder",
        "description": "task3.utils.encoder",
        "peekOfCode": "class FEncoderLayer(Layer):\n    '''\n    the encoder structure from the paper. consist of SelfAttention & FeedForward\n    '''\n    def __init__(self, key_dim: int, num_heads: int, output_shape: int=None, dropout=0.1):\n        '''\n        num_heads:     int.    Number of attention heads. \n        key_dim:       int.    Size of each attention head for query and key.\n        dropout=0.1:   float.  Dropout probability.\n        output_shape:  int.    The expected shape of an output tensor, besides the batch and sequence dims. ",
        "detail": "task3.utils.encoder",
        "documentation": {}
    },
    {
        "label": "os.environ['TF_CPP_MIN_LOG_LEVEL']",
        "kind": 5,
        "importPath": "task3.utils.encoder",
        "description": "task3.utils.encoder",
        "peekOfCode": "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\ntry:\n    from selfAttention import Former\n    from feedForward import FForward\nexcept ImportError:\n    from .selfAttention import Former\n    from .feedForward import FForward\nfrom tensorflow.keras.layers import Layer\nfrom tensorflow import Tensor\nfrom numpy import ndarray",
        "detail": "task3.utils.encoder",
        "documentation": {}
    },
    {
        "label": "FForward",
        "kind": 6,
        "importPath": "task3.utils.feedForward",
        "description": "task3.utils.feedForward",
        "peekOfCode": "class FForward(Layer):\n    '''\n    FeedForward part in the encoder from the paper.\n    '''\n    def __init__(self, units: Tuple, dropout=0.1):\n        '''\n        @params:\n                units:   Tuple.  Number of units at each dense layer, make sure that the number of\n                                      units at the last layer same as the inputs to the layer.\n                dropout: float. dropout ratio before Add&Normlize layer",
        "detail": "task3.utils.feedForward",
        "documentation": {}
    },
    {
        "label": "os.environ['TF_CPP_MIN_LOG_LEVEL']",
        "kind": 5,
        "importPath": "task3.utils.feedForward",
        "description": "task3.utils.feedForward",
        "peekOfCode": "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\nfrom tensorflow.keras import Sequential\nfrom tensorflow.keras.layers import (Layer, LayerNormalization, Add, Dropout, Dense)\nfrom tensorflow import is_tensor, convert_to_tensor, float32, Tensor\nfrom numpy import ndarray\nfrom typing import Union, Tuple\nclass FForward(Layer):\n    '''\n    FeedForward part in the encoder from the paper.\n    '''",
        "detail": "task3.utils.feedForward",
        "documentation": {}
    },
    {
        "label": "SinuSoidal",
        "kind": 6,
        "importPath": "task3.utils.posEncoders",
        "description": "task3.utils.posEncoders",
        "peekOfCode": "class SinuSoidal(Layer):\n    '''add position empedding vector to empeding vector and normlize'''\n    def __init__(self, n: int=10000):\n        super().__init__()\n        self.n = n\n        self.normalizer = LayerNormalization()\n    def __call__(self, x_emp):\n        assert len(x_emp.shape) > 1 and len(x_emp.shape) < 4, f\"expected 2D tensor for unbatched tensor or 3d for batched but get {len(x_emp.shape)}D tensor\"\n        if len(x_emp.shape) == 2:\n            T, E = x_emp.shape",
        "detail": "task3.utils.posEncoders",
        "documentation": {}
    },
    {
        "label": "sin_soidal",
        "kind": 2,
        "importPath": "task3.utils.posEncoders",
        "description": "task3.utils.posEncoders",
        "peekOfCode": "def sin_soidal(length: int, depth: int, n: int=10000):\n    '''create positionalemppeding matrix\n    @params:\n            length:  Max number of tokens in as sentence that the model will deal with it during inference.\n            depth:   Empeddingdim\n            n:       Hyper-parameter from the paper \n    '''\n    positions = np.arange(length)[:, np.newaxis]     # (seq, 1)  [0, 1, 2, 3 ... length-1]\n    depths = np.arange(depth)[np.newaxis, :]/depth   # (1, depth) [0 / depth, 1 / depth, 2/depth, 3/depth ... length-1/depth]\n    angle_rates = 1 / (n**depths)             # (1, depth)",
        "detail": "task3.utils.posEncoders",
        "documentation": {}
    },
    {
        "label": "os.environ['TF_CPP_MIN_LOG_LEVEL']",
        "kind": 5,
        "importPath": "task3.utils.posEncoders",
        "description": "task3.utils.posEncoders",
        "peekOfCode": "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n# Don't worry of warning under Tensorflow it's ok.\nfrom tensorflow.keras.layers import Layer, LayerNormalization\nfrom tensorflow import cast, float32, newaxis\nimport numpy as np\nclass SinuSoidal(Layer):\n    '''add position empedding vector to empeding vector and normlize'''\n    def __init__(self, n: int=10000):\n        super().__init__()\n        self.n = n",
        "detail": "task3.utils.posEncoders",
        "documentation": {}
    },
    {
        "label": "BaseAttention",
        "kind": 6,
        "importPath": "task3.utils.selfAttention",
        "description": "task3.utils.selfAttention",
        "peekOfCode": "class BaseAttention(Layer):\n    '''\n    the structure of self attention from the paper.\n    consist of MultiHeadAttention and Add&Normlize block \n    '''\n    def __init__(self, num_heads: int, key_dim: int, **kwargs):\n        '''\n        @params:\n                num_heads:                          int.    Number of attention heads. \n                key_dim:                            int.    Size of each attention head for query and key.",
        "detail": "task3.utils.selfAttention",
        "documentation": {}
    },
    {
        "label": "Former",
        "kind": 6,
        "importPath": "task3.utils.selfAttention",
        "description": "task3.utils.selfAttention",
        "peekOfCode": "class Former(BaseAttention):\n    '''\n    self attention part in the encoder from the paper. consist of MultiHeadAttention and Add&Normlize block \n    '''\n    def call(self, query: Union[Tensor, ndarray, List], key: Union[Tensor, ndarray, List],\n             value: Union[Tensor, ndarray, List], causal_mask: bool=False, \n             return_score=False, training=False, **kwargs): \n        '''\n        @params\n            query:          3D matrix.  Query Tensor of shape (B, T, dim).",
        "detail": "task3.utils.selfAttention",
        "documentation": {}
    },
    {
        "label": "Conformer",
        "kind": 6,
        "importPath": "task3.utils.selfAttention",
        "description": "task3.utils.selfAttention",
        "peekOfCode": "class Conformer(BaseAttention):\n    pass\nclass Branchformer(BaseAttention):\n    pass",
        "detail": "task3.utils.selfAttention",
        "documentation": {}
    },
    {
        "label": "Branchformer",
        "kind": 6,
        "importPath": "task3.utils.selfAttention",
        "description": "task3.utils.selfAttention",
        "peekOfCode": "class Branchformer(BaseAttention):\n    pass",
        "detail": "task3.utils.selfAttention",
        "documentation": {}
    },
    {
        "label": "os.environ['TF_CPP_MIN_LOG_LEVEL']",
        "kind": 5,
        "importPath": "task3.utils.selfAttention",
        "description": "task3.utils.selfAttention",
        "peekOfCode": "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\nfrom tensorflow.keras.layers import (Layer, MultiHeadAttention, LayerNormalization, \n                                     Add, serialize, deserialize)\nfrom tensorflow import is_tensor, convert_to_tensor, float32, Tensor\nfrom numpy import ndarray\nfrom typing import Union, List\nclass BaseAttention(Layer):\n    '''\n    the structure of self attention from the paper.\n    consist of MultiHeadAttention and Add&Normlize block ",
        "detail": "task3.utils.selfAttention",
        "documentation": {}
    },
    {
        "label": "create_dummy",
        "kind": 2,
        "importPath": "task3.utils.unit_test",
        "description": "task3.utils.unit_test",
        "peekOfCode": "def create_dummy(voc_size, batch, time_step, feature=None):\n    if feature is None:\n        return np.random.randint(0, voc_size, (batch, time_step))\n    return np.random.randn(batch, time_step, feature)\ndef test_SinuSoidal(): #voc_size: int, output_dim: int, max_sent_lenght: int, mask_zero\n    for v in tqdm(np.arange(1_000, 70_000, 1_000)):          #voc_size: int,\n        for o in np.arange(100, 700, 100):  #output_dim: int,\n            for m in np.arange(0, 2_000, 200):      #max_sent_lenght: int,\n                dummy = create_dummy(v, 128, m)\n                # for ms in mask_zero:",
        "detail": "task3.utils.unit_test",
        "documentation": {}
    },
    {
        "label": "test_SinuSoidal",
        "kind": 2,
        "importPath": "task3.utils.unit_test",
        "description": "task3.utils.unit_test",
        "peekOfCode": "def test_SinuSoidal(): #voc_size: int, output_dim: int, max_sent_lenght: int, mask_zero\n    for v in tqdm(np.arange(1_000, 70_000, 1_000)):          #voc_size: int,\n        for o in np.arange(100, 700, 100):  #output_dim: int,\n            for m in np.arange(0, 2_000, 200):      #max_sent_lenght: int,\n                dummy = create_dummy(v, 128, m)\n                # for ms in mask_zero:\n                opj = SinuSoidal(v, o, m)\n                try:\n                    opj(dummy)\n                    # opj.compute_mask(dummy)",
        "detail": "task3.utils.unit_test",
        "documentation": {}
    },
    {
        "label": "test_self_attention",
        "kind": 2,
        "importPath": "task3.utils.unit_test",
        "description": "task3.utils.unit_test",
        "peekOfCode": "def test_self_attention():\n    pass\ndef test_feed_forward():\n    pass\ndef test_enc_layer():\n    pass\ndef test_dec_layer():\n    pass\ndef test_enc():\n    pass",
        "detail": "task3.utils.unit_test",
        "documentation": {}
    },
    {
        "label": "test_feed_forward",
        "kind": 2,
        "importPath": "task3.utils.unit_test",
        "description": "task3.utils.unit_test",
        "peekOfCode": "def test_feed_forward():\n    pass\ndef test_enc_layer():\n    pass\ndef test_dec_layer():\n    pass\ndef test_enc():\n    pass\ndef test_dec():\n    pass",
        "detail": "task3.utils.unit_test",
        "documentation": {}
    },
    {
        "label": "test_enc_layer",
        "kind": 2,
        "importPath": "task3.utils.unit_test",
        "description": "task3.utils.unit_test",
        "peekOfCode": "def test_enc_layer():\n    pass\ndef test_dec_layer():\n    pass\ndef test_enc():\n    pass\ndef test_dec():\n    pass\ndef test_model():\n    dummy = create_dummy(10000, 128, 10)",
        "detail": "task3.utils.unit_test",
        "documentation": {}
    },
    {
        "label": "test_dec_layer",
        "kind": 2,
        "importPath": "task3.utils.unit_test",
        "description": "task3.utils.unit_test",
        "peekOfCode": "def test_dec_layer():\n    pass\ndef test_enc():\n    pass\ndef test_dec():\n    pass\ndef test_model():\n    dummy = create_dummy(10000, 128, 10)\n    # #, vocab_size, emp_dim, max_sent_lenght, key_dim, n_heads, n_layers=1\n    # #, vocab_size, emp_dim, max_sent_lenght, key_dim, n_heads, n_layers=1",
        "detail": "task3.utils.unit_test",
        "documentation": {}
    },
    {
        "label": "test_enc",
        "kind": 2,
        "importPath": "task3.utils.unit_test",
        "description": "task3.utils.unit_test",
        "peekOfCode": "def test_enc():\n    pass\ndef test_dec():\n    pass\ndef test_model():\n    dummy = create_dummy(10000, 128, 10)\n    # #, vocab_size, emp_dim, max_sent_lenght, key_dim, n_heads, n_layers=1\n    # #, vocab_size, emp_dim, max_sent_lenght, key_dim, n_heads, n_layers=1\n    # # e_config: vocab_size, emp_dim, max_sent_lenght, num_heads, key_dim, n_layers\n    # # d_config: vocab_size, emp_dim, max_sent_lenght, num_heads, key_dim, n_layers",
        "detail": "task3.utils.unit_test",
        "documentation": {}
    },
    {
        "label": "test_dec",
        "kind": 2,
        "importPath": "task3.utils.unit_test",
        "description": "task3.utils.unit_test",
        "peekOfCode": "def test_dec():\n    pass\ndef test_model():\n    dummy = create_dummy(10000, 128, 10)\n    # #, vocab_size, emp_dim, max_sent_lenght, key_dim, n_heads, n_layers=1\n    # #, vocab_size, emp_dim, max_sent_lenght, key_dim, n_heads, n_layers=1\n    # # e_config: vocab_size, emp_dim, max_sent_lenght, num_heads, key_dim, n_layers\n    # # d_config: vocab_size, emp_dim, max_sent_lenght, num_heads, key_dim, n_layers\n    config_e = (10000, 512, 50, 8, 256, 3)\n    config_d = (2000, 512, 40, 8, 256, 3)",
        "detail": "task3.utils.unit_test",
        "documentation": {}
    },
    {
        "label": "test_model",
        "kind": 2,
        "importPath": "task3.utils.unit_test",
        "description": "task3.utils.unit_test",
        "peekOfCode": "def test_model():\n    dummy = create_dummy(10000, 128, 10)\n    # #, vocab_size, emp_dim, max_sent_lenght, key_dim, n_heads, n_layers=1\n    # #, vocab_size, emp_dim, max_sent_lenght, key_dim, n_heads, n_layers=1\n    # # e_config: vocab_size, emp_dim, max_sent_lenght, num_heads, key_dim, n_layers\n    # # d_config: vocab_size, emp_dim, max_sent_lenght, num_heads, key_dim, n_layers\n    config_e = (10000, 512, 50, 8, 256, 3)\n    config_d = (2000, 512, 40, 8, 256, 3)\n    m = BTransformer(config_e, config_d)\n    try:",
        "detail": "task3.utils.unit_test",
        "documentation": {}
    },
    {
        "label": "BTConfig",
        "kind": 6,
        "importPath": "task3.baseT",
        "description": "task3.baseT",
        "peekOfCode": "class BTConfig:\n    '''\n        @params:\n                - out_heads: Dict.  Size of the soruce lang vocabulary, i.e. maximum integer index + 1.\n                - max_seq_size:    int.                Max number of tokens in as sentence that the model will \n                                                        deal with it during inference.\n                - enc_heads:    int.                Number of attention heads at each encoder. \n                - enc_key:      int.                Size of each attention head for query and key for each encoder.\n                - dec_heads:    int.                Number of attention heads at each decoder. \n                - dec_key:      int.                Size of each attention head for query and key for each decoder.",
        "detail": "task3.baseT",
        "documentation": {}
    },
    {
        "label": "OutputHead",
        "kind": 6,
        "importPath": "task3.baseT",
        "description": "task3.baseT",
        "peekOfCode": "class OutputHead(keras.layers.Layer):\n    def __init__(self, projection,  units=[], name=None):\n        super().__init__()\n        if name is None:\n            self.name = 'untitled_Head'\n        else:\n            self.name = name\n        layers = [keras.layers.Dense(u) for u in units]\n        layers.append(keras.layers.Dense(projection))\n        self.head = keras.Sequential(layers)",
        "detail": "task3.baseT",
        "documentation": {}
    },
    {
        "label": "BTransformer",
        "kind": 6,
        "importPath": "task3.baseT",
        "description": "task3.baseT",
        "peekOfCode": "class BTransformer(keras.Model):\n    '''\n    the Transformer Module from the paper. consist of Encoder Module & Decoder Module. This class also Implement\n    Greedy and Beam search decoding.\n    '''\n    def __init__(self, config: BTConfig, task='multi'):\n        super().__init__()\n        self.config: BTConfig = config\n        self.pos = SinuSoidal()\n        self.enc = keras.Sequential([FEncoderLayer(self.config.enc_key, self.config.enc_heads) ",
        "detail": "task3.baseT",
        "documentation": {}
    },
    {
        "label": "os.environ['TF_CPP_MIN_LOG_LEVEL']",
        "kind": 5,
        "importPath": "task3.baseT",
        "description": "task3.baseT",
        "peekOfCode": "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\nfrom tensorflow import keras\nfrom tensorflow import Tensor\nimport numpy as np\nfrom numpy import ndarray\nfrom typing import Union, List\nfrom dataclasses import dataclass\ntry:\n    from utils.posEncoders import SinuSoidal\n    from utils.encoder import FEncoderLayer",
        "detail": "task3.baseT",
        "documentation": {}
    },
    {
        "label": "Bert",
        "kind": 6,
        "importPath": "task3.bert1.0",
        "description": "task3.bert1.0",
        "peekOfCode": "class Bert(keras.Model):\n    def __init__(self, vocab_size: int, emp_dim: int, max_sent_lenght: int,\n                 key_dim: int, n_heads: int, n_layers: int=1):\n        '''\n        @params:\n                vocab_size:        int.  Size of the vocabulary, i.e. maximum integer index + 1.\n                emp_dim:         int.  Dimension of the dense embedding.\n                max_sent_lenght: int.  Max number of tokens in as sentence that the model will \n                                          deal with it during inference.\n                num_heads:       int.  Number of attention heads. ",
        "detail": "task3.bert1.0",
        "documentation": {}
    },
    {
        "label": "os.environ['TF_CPP_MIN_LOG_LEVEL']",
        "kind": 5,
        "importPath": "task3.bert1.0",
        "description": "task3.bert1.0",
        "peekOfCode": "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\nfrom tensorflow import keras\nfrom tensorflow import Tensor\nimport numpy as np\nfrom numpy import ndarray\nfrom typing import Union, List\nfrom utils.posEncoders import SinuSoidal\nfrom utils.encoder import EncoderLayer\nclass Bert(keras.Model):\n    def __init__(self, vocab_size: int, emp_dim: int, max_sent_lenght: int,",
        "detail": "task3.bert1.0",
        "documentation": {}
    },
    {
        "label": "Custom_loss",
        "kind": 6,
        "importPath": "task3.train_base",
        "description": "task3.train_base",
        "peekOfCode": "class Custom_loss(tf.losses.Loss):\n    def __init__(self, reduction=tf.losses_utils.ReductionV2.AUTO, name=None):\n        super().__init__(reduction, name)\n        self.reduction = reduction\n    def call(self, y_true, y_predict):\n        outputs = tf.losses.sparse_categorical_crossentropy(y_true, y_predict)\n        return outputs * self.reduction\nclass Train:\n    def __init__(self, model, loss, optimizer, metric=[]):\n        self.model  = model",
        "detail": "task3.train_base",
        "documentation": {}
    },
    {
        "label": "Train",
        "kind": 6,
        "importPath": "task3.train_base",
        "description": "task3.train_base",
        "peekOfCode": "class Train:\n    def __init__(self, model, loss, optimizer, metric=[]):\n        self.model  = model\n        self._loss_fun   = loss\n        self._opt    = optimizer\n        self._metric = metric\n        self.train_loss = 0\n        self.val_loss   = 0\n    def __call__(self, train_set, val_set, epochs):\n        for i in tqdm(range(epochs)):",
        "detail": "task3.train_base",
        "documentation": {}
    },
    {
        "label": "Dog",
        "kind": 6,
        "importPath": "tricks",
        "description": "tricks",
        "peekOfCode": "class Dog:\n    def __init__(self, name) -> None:\n        self.name = name\n    @property\n    def name(self):\n        return self._name\n    @name.setter\n    def name(self, name):\n        assert name.isalpha()\n        self._name = name",
        "detail": "tricks",
        "documentation": {}
    }
]