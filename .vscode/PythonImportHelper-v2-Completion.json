[
    {
        "label": "os",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "os",
        "description": "os",
        "detail": "os",
        "documentation": {}
    },
    {
        "label": "keras",
        "importPath": "tensorflow",
        "description": "tensorflow",
        "isExtraImport": true,
        "detail": "tensorflow",
        "documentation": {}
    },
    {
        "label": "float32",
        "importPath": "tensorflow",
        "description": "tensorflow",
        "isExtraImport": true,
        "detail": "tensorflow",
        "documentation": {}
    },
    {
        "label": "Tensor",
        "importPath": "tensorflow",
        "description": "tensorflow",
        "isExtraImport": true,
        "detail": "tensorflow",
        "documentation": {}
    },
    {
        "label": "is_tensor",
        "importPath": "tensorflow",
        "description": "tensorflow",
        "isExtraImport": true,
        "detail": "tensorflow",
        "documentation": {}
    },
    {
        "label": "convert_to_tensor",
        "importPath": "tensorflow",
        "description": "tensorflow",
        "isExtraImport": true,
        "detail": "tensorflow",
        "documentation": {}
    },
    {
        "label": "cast",
        "importPath": "tensorflow",
        "description": "tensorflow",
        "isExtraImport": true,
        "detail": "tensorflow",
        "documentation": {}
    },
    {
        "label": "float32",
        "importPath": "tensorflow",
        "description": "tensorflow",
        "isExtraImport": true,
        "detail": "tensorflow",
        "documentation": {}
    },
    {
        "label": "newaxis",
        "importPath": "tensorflow",
        "description": "tensorflow",
        "isExtraImport": true,
        "detail": "tensorflow",
        "documentation": {}
    },
    {
        "label": "Tensor",
        "importPath": "tensorflow",
        "description": "tensorflow",
        "isExtraImport": true,
        "detail": "tensorflow",
        "documentation": {}
    },
    {
        "label": "is_tensor",
        "importPath": "tensorflow",
        "description": "tensorflow",
        "isExtraImport": true,
        "detail": "tensorflow",
        "documentation": {}
    },
    {
        "label": "convert_to_tensor",
        "importPath": "tensorflow",
        "description": "tensorflow",
        "isExtraImport": true,
        "detail": "tensorflow",
        "documentation": {}
    },
    {
        "label": "float32",
        "importPath": "tensorflow",
        "description": "tensorflow",
        "isExtraImport": true,
        "detail": "tensorflow",
        "documentation": {}
    },
    {
        "label": "Tensor",
        "importPath": "tensorflow",
        "description": "tensorflow",
        "isExtraImport": true,
        "detail": "tensorflow",
        "documentation": {}
    },
    {
        "label": "is_tensor",
        "importPath": "tensorflow",
        "description": "tensorflow",
        "isExtraImport": true,
        "detail": "tensorflow",
        "documentation": {}
    },
    {
        "label": "convert_to_tensor",
        "importPath": "tensorflow",
        "description": "tensorflow",
        "isExtraImport": true,
        "detail": "tensorflow",
        "documentation": {}
    },
    {
        "label": "is_tensor",
        "importPath": "tensorflow",
        "description": "tensorflow",
        "isExtraImport": true,
        "detail": "tensorflow",
        "documentation": {}
    },
    {
        "label": "convert_to_tensor",
        "importPath": "tensorflow",
        "description": "tensorflow",
        "isExtraImport": true,
        "detail": "tensorflow",
        "documentation": {}
    },
    {
        "label": "float32",
        "importPath": "tensorflow",
        "description": "tensorflow",
        "isExtraImport": true,
        "detail": "tensorflow",
        "documentation": {}
    },
    {
        "label": "Tensor",
        "importPath": "tensorflow",
        "description": "tensorflow",
        "isExtraImport": true,
        "detail": "tensorflow",
        "documentation": {}
    },
    {
        "label": "numpy",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "numpy",
        "description": "numpy",
        "detail": "numpy",
        "documentation": {}
    },
    {
        "label": "ndarray",
        "importPath": "numpy",
        "description": "numpy",
        "isExtraImport": true,
        "detail": "numpy",
        "documentation": {}
    },
    {
        "label": "ndarray",
        "importPath": "numpy",
        "description": "numpy",
        "isExtraImport": true,
        "detail": "numpy",
        "documentation": {}
    },
    {
        "label": "ndarray",
        "importPath": "numpy",
        "description": "numpy",
        "isExtraImport": true,
        "detail": "numpy",
        "documentation": {}
    },
    {
        "label": "Union",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "List",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Union",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "List",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Union",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "List",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "SinuSoidal",
        "importPath": "posEncoders",
        "description": "posEncoders",
        "isExtraImport": true,
        "detail": "posEncoders",
        "documentation": {}
    },
    {
        "label": "SinuSoidal",
        "importPath": "posEncoders",
        "description": "posEncoders",
        "isExtraImport": true,
        "detail": "posEncoders",
        "documentation": {}
    },
    {
        "label": "EncoderLayer",
        "importPath": "utils",
        "description": "utils",
        "isExtraImport": true,
        "detail": "utils",
        "documentation": {}
    },
    {
        "label": "DecoderLayer",
        "importPath": "utils",
        "description": "utils",
        "isExtraImport": true,
        "detail": "utils",
        "documentation": {}
    },
    {
        "label": "*",
        "importPath": "utils",
        "description": "utils",
        "isExtraImport": true,
        "detail": "utils",
        "documentation": {}
    },
    {
        "label": "layers",
        "importPath": "tensorflow.keras",
        "description": "tensorflow.keras",
        "isExtraImport": true,
        "detail": "tensorflow.keras",
        "documentation": {}
    },
    {
        "label": "Sequential",
        "importPath": "tensorflow.keras",
        "description": "tensorflow.keras",
        "isExtraImport": true,
        "detail": "tensorflow.keras",
        "documentation": {}
    },
    {
        "label": "Layer",
        "importPath": "tensorflow.keras.layers",
        "description": "tensorflow.keras.layers",
        "isExtraImport": true,
        "detail": "tensorflow.keras.layers",
        "documentation": {}
    },
    {
        "label": "Embedding",
        "importPath": "tensorflow.keras.layers",
        "description": "tensorflow.keras.layers",
        "isExtraImport": true,
        "detail": "tensorflow.keras.layers",
        "documentation": {}
    },
    {
        "label": "Layer",
        "importPath": "tensorflow.keras.layers",
        "description": "tensorflow.keras.layers",
        "isExtraImport": true,
        "detail": "tensorflow.keras.layers",
        "documentation": {}
    },
    {
        "label": "MultiHeadAttention",
        "importPath": "tensorflow.keras.layers",
        "description": "tensorflow.keras.layers",
        "isExtraImport": true,
        "detail": "tensorflow.keras.layers",
        "documentation": {}
    },
    {
        "label": "LayerNormalization",
        "importPath": "tensorflow.keras.layers",
        "description": "tensorflow.keras.layers",
        "isExtraImport": true,
        "detail": "tensorflow.keras.layers",
        "documentation": {}
    },
    {
        "label": "Add",
        "importPath": "tensorflow.keras.layers",
        "description": "tensorflow.keras.layers",
        "isExtraImport": true,
        "detail": "tensorflow.keras.layers",
        "documentation": {}
    },
    {
        "label": "serialize",
        "importPath": "tensorflow.keras.layers",
        "description": "tensorflow.keras.layers",
        "isExtraImport": true,
        "detail": "tensorflow.keras.layers",
        "documentation": {}
    },
    {
        "label": "deserialize",
        "importPath": "tensorflow.keras.layers",
        "description": "tensorflow.keras.layers",
        "isExtraImport": true,
        "detail": "tensorflow.keras.layers",
        "documentation": {}
    },
    {
        "label": "Dropout",
        "importPath": "tensorflow.keras.layers",
        "description": "tensorflow.keras.layers",
        "isExtraImport": true,
        "detail": "tensorflow.keras.layers",
        "documentation": {}
    },
    {
        "label": "Dense",
        "importPath": "tensorflow.keras.layers",
        "description": "tensorflow.keras.layers",
        "isExtraImport": true,
        "detail": "tensorflow.keras.layers",
        "documentation": {}
    },
    {
        "label": "sqrt",
        "importPath": "tensorflow.math",
        "description": "tensorflow.math",
        "isExtraImport": true,
        "detail": "tensorflow.math",
        "documentation": {}
    },
    {
        "label": "tqdm",
        "importPath": "tqdm",
        "description": "tqdm",
        "isExtraImport": true,
        "detail": "tqdm",
        "documentation": {}
    },
    {
        "label": "Encoder",
        "kind": 6,
        "importPath": "transformer.baseT",
        "description": "transformer.baseT",
        "peekOfCode": "class Encoder(keras.Model):\n    #TODO -> docstring\n    def __init__(self, vocab_size, emp_dim, max_sent_lenght, key_dim, n_heads, n_layers=1) -> None:\n        #TODO -> docstring\n        super(Encoder, self).__init__()\n        self.pos_encoder = SinuSoidal(vocab_size, emp_dim, max_sent_lenght)\n        self.enc = keras.Sequential([EncoderLayer(key_dim, n_heads, emp_dim) \n                                     for _ in range(n_layers)])\n    def call(self, x, training=False, **kwargs):\n        #TODO -> docstring",
        "detail": "transformer.baseT",
        "documentation": {}
    },
    {
        "label": "Decoder",
        "kind": 6,
        "importPath": "transformer.baseT",
        "description": "transformer.baseT",
        "peekOfCode": "class Decoder(keras.Model):\n    #TODO -> docstring\n    def __init__(self, vocab_size, emp_dim, max_sent_lenght, key_dim, n_heads, n_layers=1) -> None:\n        #TODO -> docstring\n        super(Decoder, self).__init__()\n        self.n_layers = n_layers\n        self.pos_encoder = SinuSoidal(vocab_size, emp_dim, max_sent_lenght)\n        self.dec = [DecoderLayer(key_dim, n_heads, emp_dim) for _ in range(n_layers)]\n    def call(self, x, context, training=False, **kwargs):\n        #TODO -> docstring",
        "detail": "transformer.baseT",
        "documentation": {}
    },
    {
        "label": "BTransformer",
        "kind": 6,
        "importPath": "transformer.baseT",
        "description": "transformer.baseT",
        "peekOfCode": "class BTransformer(keras.Model):\n    def __init__(self, e_config: tuple, d_config: tuple) -> None:\n        super().__init__()\n        self.max_len = e_config[2]\n        self.enc = Encoder(*e_config)\n        self.dec = Decoder(*d_config)\n        self.final_layer = keras.layers.Dense(d_config[0])\n    def call(self, src, training=False):\n        # To use a Keras model with `.fit` you must pass all your inputs in the\n        # first argument.",
        "detail": "transformer.baseT",
        "documentation": {}
    },
    {
        "label": "os.environ['TF_CPP_MIN_LOG_LEVEL']",
        "kind": 5,
        "importPath": "transformer.baseT",
        "description": "transformer.baseT",
        "peekOfCode": "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\nfrom tensorflow import keras\nfrom tensorflow import float32, Tensor, is_tensor, convert_to_tensor\nfrom numpy import ndarray\nfrom typing import Union, List\nfrom posEncoders import SinuSoidal\nfrom utils import EncoderLayer, DecoderLayer\nclass Encoder(keras.Model):\n    #TODO -> docstring\n    def __init__(self, vocab_size, emp_dim, max_sent_lenght, key_dim, n_heads, n_layers=1) -> None:",
        "detail": "transformer.baseT",
        "documentation": {}
    },
    {
        "label": "SinuSoidal",
        "kind": 6,
        "importPath": "transformer.posEncoders",
        "description": "transformer.posEncoders",
        "peekOfCode": "class SinuSoidal(Layer):\n    '''\n    Custom layer that get the emppeding of words with positons info\n    as propoased on Attention is all you need paper -> https://arxiv.org/abs/1706.03762 \n    '''\n    def __init__(self, voc_size: int, output_dim: int, max_sent_lenght: int, mask_zero: bool=False, **kwargs) ->None:\n        '''instantiate Empedding layer and static positional encoding matrix\n        @params:\n                voc_size:       Integer. Size of the vocabulary, i.e. maximum integer index + 1.\n                output_dim:      Integer. Dimension of the dense embedding.",
        "detail": "transformer.posEncoders",
        "documentation": {}
    },
    {
        "label": "os.environ['TF_CPP_MIN_LOG_LEVEL']",
        "kind": 5,
        "importPath": "transformer.posEncoders",
        "description": "transformer.posEncoders",
        "peekOfCode": "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\nfrom tensorflow.keras import layers\nfrom tensorflow.keras.layers import Layer, Embedding\nfrom tensorflow import cast, float32, newaxis, Tensor, is_tensor, convert_to_tensor\nfrom tensorflow.math import sqrt\nfrom tensorflow import float32, Tensor, is_tensor, convert_to_tensor\nfrom numpy import ndarray\nfrom typing import Union, List\nimport numpy as np\nclass SinuSoidal(Layer):",
        "detail": "transformer.posEncoders",
        "documentation": {}
    },
    {
        "label": "create_dummy",
        "kind": 2,
        "importPath": "transformer.unit_test",
        "description": "transformer.unit_test",
        "peekOfCode": "def create_dummy(voc_size, batch, time_step, feature=None):\n    if feature is None:\n        return np.random.randint(0, voc_size, (batch, time_step))\n    return np.random.randn(batch, time_step, feature)\ndef test_SinuSoidal(): #voc_size: int, output_dim: int, max_sent_lenght: int, mask_zero\n    for v in tqdm(np.arange(1_000, 70_000, 1_000)):          #voc_size: int,\n        for o in np.arange(100, 700, 100):  #output_dim: int,\n            for m in np.arange(0, 2_000, 200):      #max_sent_lenght: int,\n                dummy = create_dummy(v, 128, m)\n                # for ms in mask_zero:",
        "detail": "transformer.unit_test",
        "documentation": {}
    },
    {
        "label": "test_SinuSoidal",
        "kind": 2,
        "importPath": "transformer.unit_test",
        "description": "transformer.unit_test",
        "peekOfCode": "def test_SinuSoidal(): #voc_size: int, output_dim: int, max_sent_lenght: int, mask_zero\n    for v in tqdm(np.arange(1_000, 70_000, 1_000)):          #voc_size: int,\n        for o in np.arange(100, 700, 100):  #output_dim: int,\n            for m in np.arange(0, 2_000, 200):      #max_sent_lenght: int,\n                dummy = create_dummy(v, 128, m)\n                # for ms in mask_zero:\n                opj = SinuSoidal(v, o, m)\n                try:\n                    opj(dummy)\n                    # opj.compute_mask(dummy)",
        "detail": "transformer.unit_test",
        "documentation": {}
    },
    {
        "label": "test_self_attention",
        "kind": 2,
        "importPath": "transformer.unit_test",
        "description": "transformer.unit_test",
        "peekOfCode": "def test_self_attention():\n    pass\ndef test_feed_forward():\n    pass\ndef test_enc_layer():\n    pass\ndef test_dec_layer():\n    pass\ndef test_enc():\n    pass",
        "detail": "transformer.unit_test",
        "documentation": {}
    },
    {
        "label": "test_feed_forward",
        "kind": 2,
        "importPath": "transformer.unit_test",
        "description": "transformer.unit_test",
        "peekOfCode": "def test_feed_forward():\n    pass\ndef test_enc_layer():\n    pass\ndef test_dec_layer():\n    pass\ndef test_enc():\n    pass\ndef test_dec():\n    pass",
        "detail": "transformer.unit_test",
        "documentation": {}
    },
    {
        "label": "test_enc_layer",
        "kind": 2,
        "importPath": "transformer.unit_test",
        "description": "transformer.unit_test",
        "peekOfCode": "def test_enc_layer():\n    pass\ndef test_dec_layer():\n    pass\ndef test_enc():\n    pass\ndef test_dec():\n    pass\ndef test_model():\n    pass",
        "detail": "transformer.unit_test",
        "documentation": {}
    },
    {
        "label": "test_dec_layer",
        "kind": 2,
        "importPath": "transformer.unit_test",
        "description": "transformer.unit_test",
        "peekOfCode": "def test_dec_layer():\n    pass\ndef test_enc():\n    pass\ndef test_dec():\n    pass\ndef test_model():\n    pass\nif __name__ == '__main__':\n    test_SinuSoidal()",
        "detail": "transformer.unit_test",
        "documentation": {}
    },
    {
        "label": "test_enc",
        "kind": 2,
        "importPath": "transformer.unit_test",
        "description": "transformer.unit_test",
        "peekOfCode": "def test_enc():\n    pass\ndef test_dec():\n    pass\ndef test_model():\n    pass\nif __name__ == '__main__':\n    test_SinuSoidal()",
        "detail": "transformer.unit_test",
        "documentation": {}
    },
    {
        "label": "test_dec",
        "kind": 2,
        "importPath": "transformer.unit_test",
        "description": "transformer.unit_test",
        "peekOfCode": "def test_dec():\n    pass\ndef test_model():\n    pass\nif __name__ == '__main__':\n    test_SinuSoidal()",
        "detail": "transformer.unit_test",
        "documentation": {}
    },
    {
        "label": "test_model",
        "kind": 2,
        "importPath": "transformer.unit_test",
        "description": "transformer.unit_test",
        "peekOfCode": "def test_model():\n    pass\nif __name__ == '__main__':\n    test_SinuSoidal()",
        "detail": "transformer.unit_test",
        "documentation": {}
    },
    {
        "label": "BaseAttention",
        "kind": 6,
        "importPath": "transformer.utils",
        "description": "transformer.utils",
        "peekOfCode": "class BaseAttention(Layer):\n    def __init__(self, num_heads:int, key_dim:int, *args, **kwargs):\n        '''\n        @params:\n                num_heads:                          int.    Number of attention heads. \n                key_dim:                            int.    Size of each attention head for query and key.\n                value_dim=None:                     int.    Size of each attention head for value.\n                dropout=0.0:                        float.  Dropout probability.\n                use_bias=True:                      bool.   whether the dense layers use bias vectors/matrices.\n                output_shape=None:                  int.  The expected shape of an output tensor, besides the ",
        "detail": "transformer.utils",
        "documentation": {}
    },
    {
        "label": "SelfAttention",
        "kind": 6,
        "importPath": "transformer.utils",
        "description": "transformer.utils",
        "peekOfCode": "class SelfAttention(BaseAttention):\n    def call(self, query: Union[Tensor, ndarray, List], key: Union[Tensor, ndarray, List],\n             value: Union[Tensor, ndarray, List], causal_mask: bool=False, \n             return_score=False, training=False, **kwargs)->Tensor: #attention_mask: Union[Tensor, ndarray, List]=None, \n        '''\n        @params\n            query:          3D matrix.  Query Tensor of shape (B, T, dim).\n            value:          3D matrix.  Value Tensor of shape (B, S, dim).\n            key:            3D matrix.  Optional key Tensor of shape (B, S, dim). If not given, \n                                            will use value for both key and value, which is the most ",
        "detail": "transformer.utils",
        "documentation": {}
    },
    {
        "label": "FeedForward",
        "kind": 6,
        "importPath": "transformer.utils",
        "description": "transformer.utils",
        "peekOfCode": "class FeedForward(Layer):\n    def __init__(self, units: list, dropout=0.1):\n        '''\n        @params:\n                units:   list.  Number of units at each dense layer, make sure that the number of\n                                      units at the last layer same as the inputs to the layer.\n                dropout: float. dropout ratio before Add&Normlize layer\n        '''\n        super(FeedForward, self).__init__()\n        self.seq = Sequential([Dense(u) for u in units])",
        "detail": "transformer.utils",
        "documentation": {}
    },
    {
        "label": "EncoderLayer",
        "kind": 6,
        "importPath": "transformer.utils",
        "description": "transformer.utils",
        "peekOfCode": "class EncoderLayer(Layer):\n    def __init__(self, key_dim, num_heads, output_shape, dropout=0.1):\n        #TODO -> docstring\n        super().__init__()\n        self.self_attention = SelfAttention(num_heads=num_heads,\n                                            key_dim=key_dim,\n                                            dropout=dropout,\n                                            output_shape=output_shape)\n        self.ffn = FeedForward([key_dim, output_shape])\n    def call(self, x, training=False, **kwargs):",
        "detail": "transformer.utils",
        "documentation": {}
    },
    {
        "label": "DecoderLayer",
        "kind": 6,
        "importPath": "transformer.utils",
        "description": "transformer.utils",
        "peekOfCode": "class DecoderLayer(Layer):\n    #TODO -> docstring\n    def __init__(self, key_dim, num_heads, output_shape: int, dropout=0.1):\n        #TODO -> docstring\n        super().__init__()\n        self.self_attention  = SelfAttention(num_heads=num_heads,\n                                             key_dim=key_dim,\n                                             dropout=dropout,\n                                             output_shape=output_shape)\n        self.cross_attention = SelfAttention(num_heads=num_heads,",
        "detail": "transformer.utils",
        "documentation": {}
    },
    {
        "label": "os.environ['TF_CPP_MIN_LOG_LEVEL']",
        "kind": 5,
        "importPath": "transformer.utils",
        "description": "transformer.utils",
        "peekOfCode": "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\nfrom tensorflow.keras import Sequential\nfrom tensorflow.keras.layers import (Layer, MultiHeadAttention, LayerNormalization, \n                              Add, serialize, deserialize, Dropout, Dense)\nfrom tensorflow import is_tensor, convert_to_tensor, float32, Tensor\nfrom numpy import ndarray\nfrom typing import Union, List\nclass BaseAttention(Layer):\n    def __init__(self, num_heads:int, key_dim:int, *args, **kwargs):\n        '''",
        "detail": "transformer.utils",
        "documentation": {}
    },
    {
        "label": "Dog",
        "kind": 6,
        "importPath": "tricks",
        "description": "tricks",
        "peekOfCode": "class Dog:\n    def __init__(self, name) -> None:\n        self.name = name\n    @property\n    def name(self):\n        return self._name\n    @name.setter\n    def name(self, name):\n        assert name.isalpha()\n        self._name = name",
        "detail": "tricks",
        "documentation": {}
    }
]