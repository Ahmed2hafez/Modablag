[
    {
        "label": "librosa.util",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "librosa.util",
        "description": "librosa.util",
        "detail": "librosa.util",
        "documentation": {}
    },
    {
        "label": "pad_center",
        "importPath": "librosa.util",
        "description": "librosa.util",
        "isExtraImport": true,
        "detail": "librosa.util",
        "documentation": {}
    },
    {
        "label": "tiny",
        "importPath": "librosa.util",
        "description": "librosa.util",
        "isExtraImport": true,
        "detail": "librosa.util",
        "documentation": {}
    },
    {
        "label": "numpy",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "numpy",
        "description": "numpy",
        "detail": "numpy",
        "documentation": {}
    },
    {
        "label": "torch",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "torch",
        "description": "torch",
        "detail": "torch",
        "documentation": {}
    },
    {
        "label": "nn",
        "importPath": "torch",
        "description": "torch",
        "isExtraImport": true,
        "detail": "torch",
        "documentation": {}
    },
    {
        "label": "nn",
        "importPath": "torch",
        "description": "torch",
        "isExtraImport": true,
        "detail": "torch",
        "documentation": {}
    },
    {
        "label": "nn",
        "importPath": "torch",
        "description": "torch",
        "isExtraImport": true,
        "detail": "torch",
        "documentation": {}
    },
    {
        "label": "nn",
        "importPath": "torch",
        "description": "torch",
        "isExtraImport": true,
        "detail": "torch",
        "documentation": {}
    },
    {
        "label": "get_window",
        "importPath": "scipy.signal",
        "description": "scipy.signal",
        "isExtraImport": true,
        "detail": "scipy.signal",
        "documentation": {}
    },
    {
        "label": "get_window",
        "importPath": "scipy.signal",
        "description": "scipy.signal",
        "isExtraImport": true,
        "detail": "scipy.signal",
        "documentation": {}
    },
    {
        "label": "torch.nn.functional",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "torch.nn.functional",
        "description": "torch.nn.functional",
        "detail": "torch.nn.functional",
        "documentation": {}
    },
    {
        "label": "dynamic_range_compression",
        "importPath": "audio.audio_processing",
        "description": "audio.audio_processing",
        "isExtraImport": true,
        "detail": "audio.audio_processing",
        "documentation": {}
    },
    {
        "label": "dynamic_range_decompression",
        "importPath": "audio.audio_processing",
        "description": "audio.audio_processing",
        "isExtraImport": true,
        "detail": "audio.audio_processing",
        "documentation": {}
    },
    {
        "label": "window_sumsquare",
        "importPath": "audio.audio_processing",
        "description": "audio.audio_processing",
        "isExtraImport": true,
        "detail": "audio.audio_processing",
        "documentation": {}
    },
    {
        "label": "griffin_lim",
        "importPath": "audio.audio_processing",
        "description": "audio.audio_processing",
        "isExtraImport": true,
        "detail": "audio.audio_processing",
        "documentation": {}
    },
    {
        "label": "mel",
        "importPath": "librosa.filters",
        "description": "librosa.filters",
        "isExtraImport": true,
        "detail": "librosa.filters",
        "documentation": {}
    },
    {
        "label": "write",
        "importPath": "scipy.io.wavfile",
        "description": "scipy.io.wavfile",
        "isExtraImport": true,
        "detail": "scipy.io.wavfile",
        "documentation": {}
    },
    {
        "label": "read",
        "importPath": "scipy.io.wavfile",
        "description": "scipy.io.wavfile",
        "isExtraImport": true,
        "detail": "scipy.io.wavfile",
        "documentation": {}
    },
    {
        "label": "torch.nn",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "torch.nn",
        "description": "torch.nn",
        "detail": "torch.nn",
        "documentation": {}
    },
    {
        "label": "Conv1d",
        "importPath": "torch.nn",
        "description": "torch.nn",
        "isExtraImport": true,
        "detail": "torch.nn",
        "documentation": {}
    },
    {
        "label": "ConvTranspose1d",
        "importPath": "torch.nn",
        "description": "torch.nn",
        "isExtraImport": true,
        "detail": "torch.nn",
        "documentation": {}
    },
    {
        "label": "functional",
        "importPath": "torch.nn",
        "description": "torch.nn",
        "isExtraImport": true,
        "detail": "torch.nn",
        "documentation": {}
    },
    {
        "label": "remove_weight_norm",
        "importPath": "torch.nn.utils",
        "description": "torch.nn.utils",
        "isExtraImport": true,
        "detail": "torch.nn.utils",
        "documentation": {}
    },
    {
        "label": "weight_norm",
        "importPath": "torch.nn.utils",
        "description": "torch.nn.utils",
        "isExtraImport": true,
        "detail": "torch.nn.utils",
        "documentation": {}
    },
    {
        "label": "json",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "json",
        "description": "json",
        "detail": "json",
        "documentation": {}
    },
    {
        "label": "os",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "os",
        "description": "os",
        "detail": "os",
        "documentation": {}
    },
    {
        "label": "collections",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "collections",
        "description": "collections",
        "detail": "collections",
        "documentation": {}
    },
    {
        "label": "OrderedDict",
        "importPath": "collections",
        "description": "collections",
        "isExtraImport": true,
        "detail": "collections",
        "documentation": {}
    },
    {
        "label": "codecs",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "codecs",
        "description": "codecs",
        "detail": "codecs",
        "documentation": {}
    },
    {
        "label": "re",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "re",
        "description": "re",
        "detail": "re",
        "documentation": {}
    },
    {
        "label": "sys",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "sys",
        "description": "sys",
        "detail": "sys",
        "documentation": {}
    },
    {
        "label": "librosa",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "librosa",
        "description": "librosa",
        "detail": "librosa",
        "documentation": {}
    },
    {
        "label": "wavfile",
        "importPath": "scipy.io",
        "description": "scipy.io",
        "isExtraImport": true,
        "detail": "scipy.io",
        "documentation": {}
    },
    {
        "label": "wavfile",
        "importPath": "scipy.io",
        "description": "scipy.io",
        "isExtraImport": true,
        "detail": "scipy.io",
        "documentation": {}
    },
    {
        "label": "wavfile",
        "importPath": "scipy.io",
        "description": "scipy.io",
        "isExtraImport": true,
        "detail": "scipy.io",
        "documentation": {}
    },
    {
        "label": "wavfile",
        "importPath": "scipy.io",
        "description": "scipy.io",
        "isExtraImport": true,
        "detail": "scipy.io",
        "documentation": {}
    },
    {
        "label": "wavfile",
        "importPath": "scipy.io",
        "description": "scipy.io",
        "isExtraImport": true,
        "detail": "scipy.io",
        "documentation": {}
    },
    {
        "label": "tqdm",
        "importPath": "tqdm",
        "description": "tqdm",
        "isExtraImport": true,
        "detail": "tqdm",
        "documentation": {}
    },
    {
        "label": "tqdm",
        "importPath": "tqdm",
        "description": "tqdm",
        "isExtraImport": true,
        "detail": "tqdm",
        "documentation": {}
    },
    {
        "label": "tqdm",
        "importPath": "tqdm",
        "description": "tqdm",
        "isExtraImport": true,
        "detail": "tqdm",
        "documentation": {}
    },
    {
        "label": "tqdm",
        "importPath": "tqdm",
        "description": "tqdm",
        "isExtraImport": true,
        "detail": "tqdm",
        "documentation": {}
    },
    {
        "label": "tqdm",
        "importPath": "tqdm",
        "description": "tqdm",
        "isExtraImport": true,
        "detail": "tqdm",
        "documentation": {}
    },
    {
        "label": "tqdm",
        "importPath": "tqdm",
        "description": "tqdm",
        "isExtraImport": true,
        "detail": "tqdm",
        "documentation": {}
    },
    {
        "label": "_clean_text",
        "importPath": "text",
        "description": "text",
        "isExtraImport": true,
        "detail": "text",
        "documentation": {}
    },
    {
        "label": "_clean_text",
        "importPath": "text",
        "description": "text",
        "isExtraImport": true,
        "detail": "text",
        "documentation": {}
    },
    {
        "label": "_clean_text",
        "importPath": "text",
        "description": "text",
        "isExtraImport": true,
        "detail": "text",
        "documentation": {}
    },
    {
        "label": "text_to_sequence",
        "importPath": "text",
        "description": "text",
        "isExtraImport": true,
        "detail": "text",
        "documentation": {}
    },
    {
        "label": "random",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "random",
        "description": "random",
        "detail": "random",
        "documentation": {}
    },
    {
        "label": "audio",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "audio",
        "description": "audio",
        "detail": "audio",
        "documentation": {}
    },
    {
        "label": "pyworld",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "pyworld",
        "description": "pyworld",
        "detail": "pyworld",
        "documentation": {}
    },
    {
        "label": "tgt",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "tgt",
        "description": "tgt",
        "detail": "tgt",
        "documentation": {}
    },
    {
        "label": "interp1d",
        "importPath": "scipy.interpolate",
        "description": "scipy.interpolate",
        "isExtraImport": true,
        "detail": "scipy.interpolate",
        "documentation": {}
    },
    {
        "label": "StandardScaler",
        "importPath": "sklearn.preprocessing",
        "description": "sklearn.preprocessing",
        "isExtraImport": true,
        "detail": "sklearn.preprocessing",
        "documentation": {}
    },
    {
        "label": "unidecode",
        "importPath": "unidecode",
        "description": "unidecode",
        "isExtraImport": true,
        "detail": "unidecode",
        "documentation": {}
    },
    {
        "label": "inflect",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "inflect",
        "description": "inflect",
        "detail": "inflect",
        "documentation": {}
    },
    {
        "label": "gdown",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "gdown",
        "description": "gdown",
        "detail": "gdown",
        "documentation": {}
    },
    {
        "label": "matplotlib",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "matplotlib",
        "description": "matplotlib",
        "detail": "matplotlib",
        "documentation": {}
    },
    {
        "label": "pyplot",
        "importPath": "matplotlib",
        "description": "matplotlib",
        "isExtraImport": true,
        "detail": "matplotlib",
        "documentation": {}
    },
    {
        "label": "Dataset",
        "importPath": "torch.utils.data",
        "description": "torch.utils.data",
        "isExtraImport": true,
        "detail": "torch.utils.data",
        "documentation": {}
    },
    {
        "label": "DataLoader",
        "importPath": "torch.utils.data",
        "description": "torch.utils.data",
        "isExtraImport": true,
        "detail": "torch.utils.data",
        "documentation": {}
    },
    {
        "label": "DataLoader",
        "importPath": "torch.utils.data",
        "description": "torch.utils.data",
        "isExtraImport": true,
        "detail": "torch.utils.data",
        "documentation": {}
    },
    {
        "label": "DataLoader",
        "importPath": "torch.utils.data",
        "description": "torch.utils.data",
        "isExtraImport": true,
        "detail": "torch.utils.data",
        "documentation": {}
    },
    {
        "label": "argparse",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "argparse",
        "description": "argparse",
        "detail": "argparse",
        "documentation": {}
    },
    {
        "label": "yaml",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "yaml",
        "description": "yaml",
        "detail": "yaml",
        "documentation": {}
    },
    {
        "label": "Dataset",
        "importPath": "dataset",
        "description": "dataset",
        "isExtraImport": true,
        "detail": "dataset",
        "documentation": {}
    },
    {
        "label": "TextDataset",
        "importPath": "dataset",
        "description": "dataset",
        "isExtraImport": true,
        "detail": "dataset",
        "documentation": {}
    },
    {
        "label": "Dataset",
        "importPath": "dataset",
        "description": "dataset",
        "isExtraImport": true,
        "detail": "dataset",
        "documentation": {}
    },
    {
        "label": "FastSpeech2Loss",
        "importPath": "model",
        "description": "model",
        "isExtraImport": true,
        "detail": "model",
        "documentation": {}
    },
    {
        "label": "FastSpeech2Loss",
        "importPath": "model",
        "description": "model",
        "isExtraImport": true,
        "detail": "model",
        "documentation": {}
    },
    {
        "label": "get_model",
        "importPath": "utils.model",
        "description": "utils.model",
        "isExtraImport": true,
        "detail": "utils.model",
        "documentation": {}
    },
    {
        "label": "get_model",
        "importPath": "utils.model",
        "description": "utils.model",
        "isExtraImport": true,
        "detail": "utils.model",
        "documentation": {}
    },
    {
        "label": "get_vocoder",
        "importPath": "utils.model",
        "description": "utils.model",
        "isExtraImport": true,
        "detail": "utils.model",
        "documentation": {}
    },
    {
        "label": "get_model",
        "importPath": "utils.model",
        "description": "utils.model",
        "isExtraImport": true,
        "detail": "utils.model",
        "documentation": {}
    },
    {
        "label": "get_param_num",
        "importPath": "utils.model",
        "description": "utils.model",
        "isExtraImport": true,
        "detail": "utils.model",
        "documentation": {}
    },
    {
        "label": "get_vocoder",
        "importPath": "utils.model",
        "description": "utils.model",
        "isExtraImport": true,
        "detail": "utils.model",
        "documentation": {}
    },
    {
        "label": "log",
        "importPath": "utils.tools",
        "description": "utils.tools",
        "isExtraImport": true,
        "detail": "utils.tools",
        "documentation": {}
    },
    {
        "label": "synth_one_sample",
        "importPath": "utils.tools",
        "description": "utils.tools",
        "isExtraImport": true,
        "detail": "utils.tools",
        "documentation": {}
    },
    {
        "label": "to_device",
        "importPath": "utils.tools",
        "description": "utils.tools",
        "isExtraImport": true,
        "detail": "utils.tools",
        "documentation": {}
    },
    {
        "label": "synth_samples",
        "importPath": "utils.tools",
        "description": "utils.tools",
        "isExtraImport": true,
        "detail": "utils.tools",
        "documentation": {}
    },
    {
        "label": "to_device",
        "importPath": "utils.tools",
        "description": "utils.tools",
        "isExtraImport": true,
        "detail": "utils.tools",
        "documentation": {}
    },
    {
        "label": "log",
        "importPath": "utils.tools",
        "description": "utils.tools",
        "isExtraImport": true,
        "detail": "utils.tools",
        "documentation": {}
    },
    {
        "label": "synth_one_sample",
        "importPath": "utils.tools",
        "description": "utils.tools",
        "isExtraImport": true,
        "detail": "utils.tools",
        "documentation": {}
    },
    {
        "label": "to_device",
        "importPath": "utils.tools",
        "description": "utils.tools",
        "isExtraImport": true,
        "detail": "utils.tools",
        "documentation": {}
    },
    {
        "label": "mishkal.tashkeel",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "mishkal.tashkeel",
        "description": "mishkal.tashkeel",
        "detail": "mishkal.tashkeel",
        "documentation": {}
    },
    {
        "label": "bw2ar",
        "importPath": "klaam.external.FastSpeech2.buckwalter",
        "description": "klaam.external.FastSpeech2.buckwalter",
        "isExtraImport": true,
        "detail": "klaam.external.FastSpeech2.buckwalter",
        "documentation": {}
    },
    {
        "label": "bw2ar",
        "importPath": "klaam.external.FastSpeech2.buckwalter",
        "description": "klaam.external.FastSpeech2.buckwalter",
        "isExtraImport": true,
        "detail": "klaam.external.FastSpeech2.buckwalter",
        "documentation": {}
    },
    {
        "label": "phonetise",
        "importPath": "klaam.external.FastSpeech2.phonetise.phonetise_arabic",
        "description": "klaam.external.FastSpeech2.phonetise.phonetise_arabic",
        "isExtraImport": true,
        "detail": "klaam.external.FastSpeech2.phonetise.phonetise_arabic",
        "documentation": {}
    },
    {
        "label": "text_to_sequence",
        "importPath": "klaam.external.FastSpeech2.text",
        "description": "klaam.external.FastSpeech2.text",
        "isExtraImport": true,
        "detail": "klaam.external.FastSpeech2.text",
        "documentation": {}
    },
    {
        "label": "get_model_inference",
        "importPath": "klaam.external.FastSpeech2.utils.model",
        "description": "klaam.external.FastSpeech2.utils.model",
        "isExtraImport": true,
        "detail": "klaam.external.FastSpeech2.utils.model",
        "documentation": {}
    },
    {
        "label": "get_vocoder",
        "importPath": "klaam.external.FastSpeech2.utils.model",
        "description": "klaam.external.FastSpeech2.utils.model",
        "isExtraImport": true,
        "detail": "klaam.external.FastSpeech2.utils.model",
        "documentation": {}
    },
    {
        "label": "synth_samples",
        "importPath": "klaam.external.FastSpeech2.utils.tools",
        "description": "klaam.external.FastSpeech2.utils.tools",
        "isExtraImport": true,
        "detail": "klaam.external.FastSpeech2.utils.tools",
        "documentation": {}
    },
    {
        "label": "to_device",
        "importPath": "klaam.external.FastSpeech2.utils.tools",
        "description": "klaam.external.FastSpeech2.utils.tools",
        "isExtraImport": true,
        "detail": "klaam.external.FastSpeech2.utils.tools",
        "documentation": {}
    },
    {
        "label": "aishell3",
        "importPath": "preprocessor",
        "description": "preprocessor",
        "isExtraImport": true,
        "detail": "preprocessor",
        "documentation": {}
    },
    {
        "label": "arabic",
        "importPath": "preprocessor",
        "description": "preprocessor",
        "isExtraImport": true,
        "detail": "preprocessor",
        "documentation": {}
    },
    {
        "label": "libritts",
        "importPath": "preprocessor",
        "description": "preprocessor",
        "isExtraImport": true,
        "detail": "preprocessor",
        "documentation": {}
    },
    {
        "label": "ljspeech",
        "importPath": "preprocessor",
        "description": "preprocessor",
        "isExtraImport": true,
        "detail": "preprocessor",
        "documentation": {}
    },
    {
        "label": "Preprocessor",
        "importPath": "preprocessor.preprocessor",
        "description": "preprocessor.preprocessor",
        "isExtraImport": true,
        "detail": "preprocessor.preprocessor",
        "documentation": {}
    },
    {
        "label": "punctuation",
        "importPath": "string",
        "description": "string",
        "isExtraImport": true,
        "detail": "string",
        "documentation": {}
    },
    {
        "label": "phonetise",
        "importPath": "arabic_pronounce",
        "description": "arabic_pronounce",
        "isExtraImport": true,
        "detail": "arabic_pronounce",
        "documentation": {}
    },
    {
        "label": "Style",
        "importPath": "pypinyin",
        "description": "pypinyin",
        "isExtraImport": true,
        "detail": "pypinyin",
        "documentation": {}
    },
    {
        "label": "pinyin",
        "importPath": "pypinyin",
        "description": "pypinyin",
        "isExtraImport": true,
        "detail": "pypinyin",
        "documentation": {}
    },
    {
        "label": "evaluate",
        "importPath": "evaluate",
        "description": "evaluate",
        "isExtraImport": true,
        "detail": "evaluate",
        "documentation": {}
    },
    {
        "label": "SummaryWriter",
        "importPath": "torch.utils.tensorboard",
        "description": "torch.utils.tensorboard",
        "isExtraImport": true,
        "detail": "torch.utils.tensorboard",
        "documentation": {}
    },
    {
        "label": "transformers",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "transformers",
        "description": "transformers",
        "detail": "transformers",
        "documentation": {}
    },
    {
        "label": "Wav2Vec2Model",
        "importPath": "transformers",
        "description": "transformers",
        "isExtraImport": true,
        "detail": "transformers",
        "documentation": {}
    },
    {
        "label": "Wav2Vec2PreTrainedModel",
        "importPath": "transformers",
        "description": "transformers",
        "isExtraImport": true,
        "detail": "transformers",
        "documentation": {}
    },
    {
        "label": "Wav2Vec2FeatureExtractor",
        "importPath": "transformers",
        "description": "transformers",
        "isExtraImport": true,
        "detail": "transformers",
        "documentation": {}
    },
    {
        "label": "Wav2Vec2ForCTC",
        "importPath": "transformers",
        "description": "transformers",
        "isExtraImport": true,
        "detail": "transformers",
        "documentation": {}
    },
    {
        "label": "Wav2Vec2Processor",
        "importPath": "transformers",
        "description": "transformers",
        "isExtraImport": true,
        "detail": "transformers",
        "documentation": {}
    },
    {
        "label": "HfArgumentParser",
        "importPath": "transformers",
        "description": "transformers",
        "isExtraImport": true,
        "detail": "transformers",
        "documentation": {}
    },
    {
        "label": "Trainer",
        "importPath": "transformers",
        "description": "transformers",
        "isExtraImport": true,
        "detail": "transformers",
        "documentation": {}
    },
    {
        "label": "TrainingArguments",
        "importPath": "transformers",
        "description": "transformers",
        "isExtraImport": true,
        "detail": "transformers",
        "documentation": {}
    },
    {
        "label": "Wav2Vec2FeatureExtractor",
        "importPath": "transformers",
        "description": "transformers",
        "isExtraImport": true,
        "detail": "transformers",
        "documentation": {}
    },
    {
        "label": "is_apex_available",
        "importPath": "transformers",
        "description": "transformers",
        "isExtraImport": true,
        "detail": "transformers",
        "documentation": {}
    },
    {
        "label": "set_seed",
        "importPath": "transformers",
        "description": "transformers",
        "isExtraImport": true,
        "detail": "transformers",
        "documentation": {}
    },
    {
        "label": "HfArgumentParser",
        "importPath": "transformers",
        "description": "transformers",
        "isExtraImport": true,
        "detail": "transformers",
        "documentation": {}
    },
    {
        "label": "Trainer",
        "importPath": "transformers",
        "description": "transformers",
        "isExtraImport": true,
        "detail": "transformers",
        "documentation": {}
    },
    {
        "label": "TrainingArguments",
        "importPath": "transformers",
        "description": "transformers",
        "isExtraImport": true,
        "detail": "transformers",
        "documentation": {}
    },
    {
        "label": "Wav2Vec2CTCTokenizer",
        "importPath": "transformers",
        "description": "transformers",
        "isExtraImport": true,
        "detail": "transformers",
        "documentation": {}
    },
    {
        "label": "Wav2Vec2FeatureExtractor",
        "importPath": "transformers",
        "description": "transformers",
        "isExtraImport": true,
        "detail": "transformers",
        "documentation": {}
    },
    {
        "label": "Wav2Vec2ForCTC",
        "importPath": "transformers",
        "description": "transformers",
        "isExtraImport": true,
        "detail": "transformers",
        "documentation": {}
    },
    {
        "label": "Wav2Vec2Processor",
        "importPath": "transformers",
        "description": "transformers",
        "isExtraImport": true,
        "detail": "transformers",
        "documentation": {}
    },
    {
        "label": "is_apex_available",
        "importPath": "transformers",
        "description": "transformers",
        "isExtraImport": true,
        "detail": "transformers",
        "documentation": {}
    },
    {
        "label": "set_seed",
        "importPath": "transformers",
        "description": "transformers",
        "isExtraImport": true,
        "detail": "transformers",
        "documentation": {}
    },
    {
        "label": "HfArgumentParser",
        "importPath": "transformers",
        "description": "transformers",
        "isExtraImport": true,
        "detail": "transformers",
        "documentation": {}
    },
    {
        "label": "Trainer",
        "importPath": "transformers",
        "description": "transformers",
        "isExtraImport": true,
        "detail": "transformers",
        "documentation": {}
    },
    {
        "label": "TrainingArguments",
        "importPath": "transformers",
        "description": "transformers",
        "isExtraImport": true,
        "detail": "transformers",
        "documentation": {}
    },
    {
        "label": "Wav2Vec2CTCTokenizer",
        "importPath": "transformers",
        "description": "transformers",
        "isExtraImport": true,
        "detail": "transformers",
        "documentation": {}
    },
    {
        "label": "Wav2Vec2FeatureExtractor",
        "importPath": "transformers",
        "description": "transformers",
        "isExtraImport": true,
        "detail": "transformers",
        "documentation": {}
    },
    {
        "label": "Wav2Vec2ForCTC",
        "importPath": "transformers",
        "description": "transformers",
        "isExtraImport": true,
        "detail": "transformers",
        "documentation": {}
    },
    {
        "label": "Wav2Vec2Processor",
        "importPath": "transformers",
        "description": "transformers",
        "isExtraImport": true,
        "detail": "transformers",
        "documentation": {}
    },
    {
        "label": "is_apex_available",
        "importPath": "transformers",
        "description": "transformers",
        "isExtraImport": true,
        "detail": "transformers",
        "documentation": {}
    },
    {
        "label": "set_seed",
        "importPath": "transformers",
        "description": "transformers",
        "isExtraImport": true,
        "detail": "transformers",
        "documentation": {}
    },
    {
        "label": "HfArgumentParser",
        "importPath": "transformers",
        "description": "transformers",
        "isExtraImport": true,
        "detail": "transformers",
        "documentation": {}
    },
    {
        "label": "Trainer",
        "importPath": "transformers",
        "description": "transformers",
        "isExtraImport": true,
        "detail": "transformers",
        "documentation": {}
    },
    {
        "label": "TrainingArguments",
        "importPath": "transformers",
        "description": "transformers",
        "isExtraImport": true,
        "detail": "transformers",
        "documentation": {}
    },
    {
        "label": "Wav2Vec2CTCTokenizer",
        "importPath": "transformers",
        "description": "transformers",
        "isExtraImport": true,
        "detail": "transformers",
        "documentation": {}
    },
    {
        "label": "Wav2Vec2FeatureExtractor",
        "importPath": "transformers",
        "description": "transformers",
        "isExtraImport": true,
        "detail": "transformers",
        "documentation": {}
    },
    {
        "label": "Wav2Vec2ForCTC",
        "importPath": "transformers",
        "description": "transformers",
        "isExtraImport": true,
        "detail": "transformers",
        "documentation": {}
    },
    {
        "label": "Wav2Vec2Processor",
        "importPath": "transformers",
        "description": "transformers",
        "isExtraImport": true,
        "detail": "transformers",
        "documentation": {}
    },
    {
        "label": "is_apex_available",
        "importPath": "transformers",
        "description": "transformers",
        "isExtraImport": true,
        "detail": "transformers",
        "documentation": {}
    },
    {
        "label": "set_seed",
        "importPath": "transformers",
        "description": "transformers",
        "isExtraImport": true,
        "detail": "transformers",
        "documentation": {}
    },
    {
        "label": "absolute_import",
        "importPath": "__future__",
        "description": "__future__",
        "isExtraImport": true,
        "detail": "__future__",
        "documentation": {}
    },
    {
        "label": "division",
        "importPath": "__future__",
        "description": "__future__",
        "isExtraImport": true,
        "detail": "__future__",
        "documentation": {}
    },
    {
        "label": "print_function",
        "importPath": "__future__",
        "description": "__future__",
        "isExtraImport": true,
        "detail": "__future__",
        "documentation": {}
    },
    {
        "label": "absolute_import",
        "importPath": "__future__",
        "description": "__future__",
        "isExtraImport": true,
        "detail": "__future__",
        "documentation": {}
    },
    {
        "label": "division",
        "importPath": "__future__",
        "description": "__future__",
        "isExtraImport": true,
        "detail": "__future__",
        "documentation": {}
    },
    {
        "label": "print_function",
        "importPath": "__future__",
        "description": "__future__",
        "isExtraImport": true,
        "detail": "__future__",
        "documentation": {}
    },
    {
        "label": "absolute_import",
        "importPath": "__future__",
        "description": "__future__",
        "isExtraImport": true,
        "detail": "__future__",
        "documentation": {}
    },
    {
        "label": "division",
        "importPath": "__future__",
        "description": "__future__",
        "isExtraImport": true,
        "detail": "__future__",
        "documentation": {}
    },
    {
        "label": "print_function",
        "importPath": "__future__",
        "description": "__future__",
        "isExtraImport": true,
        "detail": "__future__",
        "documentation": {}
    },
    {
        "label": "absolute_import",
        "importPath": "__future__",
        "description": "__future__",
        "isExtraImport": true,
        "detail": "__future__",
        "documentation": {}
    },
    {
        "label": "division",
        "importPath": "__future__",
        "description": "__future__",
        "isExtraImport": true,
        "detail": "__future__",
        "documentation": {}
    },
    {
        "label": "print_function",
        "importPath": "__future__",
        "description": "__future__",
        "isExtraImport": true,
        "detail": "__future__",
        "documentation": {}
    },
    {
        "label": "unicode_literals",
        "importPath": "__future__",
        "description": "__future__",
        "isExtraImport": true,
        "detail": "__future__",
        "documentation": {}
    },
    {
        "label": "absolute_import",
        "importPath": "__future__",
        "description": "__future__",
        "isExtraImport": true,
        "detail": "__future__",
        "documentation": {}
    },
    {
        "label": "division",
        "importPath": "__future__",
        "description": "__future__",
        "isExtraImport": true,
        "detail": "__future__",
        "documentation": {}
    },
    {
        "label": "print_function",
        "importPath": "__future__",
        "description": "__future__",
        "isExtraImport": true,
        "detail": "__future__",
        "documentation": {}
    },
    {
        "label": "absolute_import",
        "importPath": "__future__",
        "description": "__future__",
        "isExtraImport": true,
        "detail": "__future__",
        "documentation": {}
    },
    {
        "label": "division",
        "importPath": "__future__",
        "description": "__future__",
        "isExtraImport": true,
        "detail": "__future__",
        "documentation": {}
    },
    {
        "label": "print_function",
        "importPath": "__future__",
        "description": "__future__",
        "isExtraImport": true,
        "detail": "__future__",
        "documentation": {}
    },
    {
        "label": "print_function",
        "importPath": "__future__",
        "description": "__future__",
        "isExtraImport": true,
        "detail": "__future__",
        "documentation": {}
    },
    {
        "label": "absolute_import",
        "importPath": "__future__",
        "description": "__future__",
        "isExtraImport": true,
        "detail": "__future__",
        "documentation": {}
    },
    {
        "label": "division",
        "importPath": "__future__",
        "description": "__future__",
        "isExtraImport": true,
        "detail": "__future__",
        "documentation": {}
    },
    {
        "label": "print_function",
        "importPath": "__future__",
        "description": "__future__",
        "isExtraImport": true,
        "detail": "__future__",
        "documentation": {}
    },
    {
        "label": "print_function",
        "importPath": "__future__",
        "description": "__future__",
        "isExtraImport": true,
        "detail": "__future__",
        "documentation": {}
    },
    {
        "label": "absolute_import",
        "importPath": "__future__",
        "description": "__future__",
        "isExtraImport": true,
        "detail": "__future__",
        "documentation": {}
    },
    {
        "label": "division",
        "importPath": "__future__",
        "description": "__future__",
        "isExtraImport": true,
        "detail": "__future__",
        "documentation": {}
    },
    {
        "label": "print_function",
        "importPath": "__future__",
        "description": "__future__",
        "isExtraImport": true,
        "detail": "__future__",
        "documentation": {}
    },
    {
        "label": "print_function",
        "importPath": "__future__",
        "description": "__future__",
        "isExtraImport": true,
        "detail": "__future__",
        "documentation": {}
    },
    {
        "label": "absolute_import",
        "importPath": "__future__",
        "description": "__future__",
        "isExtraImport": true,
        "detail": "__future__",
        "documentation": {}
    },
    {
        "label": "division",
        "importPath": "__future__",
        "description": "__future__",
        "isExtraImport": true,
        "detail": "__future__",
        "documentation": {}
    },
    {
        "label": "print_function",
        "importPath": "__future__",
        "description": "__future__",
        "isExtraImport": true,
        "detail": "__future__",
        "documentation": {}
    },
    {
        "label": "absolute_import",
        "importPath": "__future__",
        "description": "__future__",
        "isExtraImport": true,
        "detail": "__future__",
        "documentation": {}
    },
    {
        "label": "division",
        "importPath": "__future__",
        "description": "__future__",
        "isExtraImport": true,
        "detail": "__future__",
        "documentation": {}
    },
    {
        "label": "print_function",
        "importPath": "__future__",
        "description": "__future__",
        "isExtraImport": true,
        "detail": "__future__",
        "documentation": {}
    },
    {
        "label": "absolute_import",
        "importPath": "__future__",
        "description": "__future__",
        "isExtraImport": true,
        "detail": "__future__",
        "documentation": {}
    },
    {
        "label": "division",
        "importPath": "__future__",
        "description": "__future__",
        "isExtraImport": true,
        "detail": "__future__",
        "documentation": {}
    },
    {
        "label": "print_function",
        "importPath": "__future__",
        "description": "__future__",
        "isExtraImport": true,
        "detail": "__future__",
        "documentation": {}
    },
    {
        "label": "absolute_import",
        "importPath": "__future__",
        "description": "__future__",
        "isExtraImport": true,
        "detail": "__future__",
        "documentation": {}
    },
    {
        "label": "division",
        "importPath": "__future__",
        "description": "__future__",
        "isExtraImport": true,
        "detail": "__future__",
        "documentation": {}
    },
    {
        "label": "print_function",
        "importPath": "__future__",
        "description": "__future__",
        "isExtraImport": true,
        "detail": "__future__",
        "documentation": {}
    },
    {
        "label": "absolute_import",
        "importPath": "__future__",
        "description": "__future__",
        "isExtraImport": true,
        "detail": "__future__",
        "documentation": {}
    },
    {
        "label": "division",
        "importPath": "__future__",
        "description": "__future__",
        "isExtraImport": true,
        "detail": "__future__",
        "documentation": {}
    },
    {
        "label": "print_function",
        "importPath": "__future__",
        "description": "__future__",
        "isExtraImport": true,
        "detail": "__future__",
        "documentation": {}
    },
    {
        "label": "print_function",
        "importPath": "__future__",
        "description": "__future__",
        "isExtraImport": true,
        "detail": "__future__",
        "documentation": {}
    },
    {
        "label": "absolute_import",
        "importPath": "__future__",
        "description": "__future__",
        "isExtraImport": true,
        "detail": "__future__",
        "documentation": {}
    },
    {
        "label": "division",
        "importPath": "__future__",
        "description": "__future__",
        "isExtraImport": true,
        "detail": "__future__",
        "documentation": {}
    },
    {
        "label": "print_function",
        "importPath": "__future__",
        "description": "__future__",
        "isExtraImport": true,
        "detail": "__future__",
        "documentation": {}
    },
    {
        "label": "absolute_import",
        "importPath": "__future__",
        "description": "__future__",
        "isExtraImport": true,
        "detail": "__future__",
        "documentation": {}
    },
    {
        "label": "division",
        "importPath": "__future__",
        "description": "__future__",
        "isExtraImport": true,
        "detail": "__future__",
        "documentation": {}
    },
    {
        "label": "print_function",
        "importPath": "__future__",
        "description": "__future__",
        "isExtraImport": true,
        "detail": "__future__",
        "documentation": {}
    },
    {
        "label": "print_function",
        "importPath": "__future__",
        "description": "__future__",
        "isExtraImport": true,
        "detail": "__future__",
        "documentation": {}
    },
    {
        "label": "absolute_import",
        "importPath": "__future__",
        "description": "__future__",
        "isExtraImport": true,
        "detail": "__future__",
        "documentation": {}
    },
    {
        "label": "division",
        "importPath": "__future__",
        "description": "__future__",
        "isExtraImport": true,
        "detail": "__future__",
        "documentation": {}
    },
    {
        "label": "print_function",
        "importPath": "__future__",
        "description": "__future__",
        "isExtraImport": true,
        "detail": "__future__",
        "documentation": {}
    },
    {
        "label": "print_function",
        "importPath": "__future__",
        "description": "__future__",
        "isExtraImport": true,
        "detail": "__future__",
        "documentation": {}
    },
    {
        "label": "absolute_import",
        "importPath": "__future__",
        "description": "__future__",
        "isExtraImport": true,
        "detail": "__future__",
        "documentation": {}
    },
    {
        "label": "division",
        "importPath": "__future__",
        "description": "__future__",
        "isExtraImport": true,
        "detail": "__future__",
        "documentation": {}
    },
    {
        "label": "print_function",
        "importPath": "__future__",
        "description": "__future__",
        "isExtraImport": true,
        "detail": "__future__",
        "documentation": {}
    },
    {
        "label": "print_function",
        "importPath": "__future__",
        "description": "__future__",
        "isExtraImport": true,
        "detail": "__future__",
        "documentation": {}
    },
    {
        "label": "datasets",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "datasets",
        "description": "datasets",
        "detail": "datasets",
        "documentation": {}
    },
    {
        "label": "soundfile",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "soundfile",
        "description": "soundfile",
        "detail": "soundfile",
        "documentation": {}
    },
    {
        "label": "io",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "io",
        "description": "io",
        "detail": "io",
        "documentation": {}
    },
    {
        "label": "wave",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "wave",
        "description": "wave",
        "detail": "wave",
        "documentation": {}
    },
    {
        "label": "b64decode",
        "importPath": "base64",
        "description": "base64",
        "isExtraImport": true,
        "detail": "base64",
        "documentation": {}
    },
    {
        "label": "ffmpeg",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "ffmpeg",
        "description": "ffmpeg",
        "detail": "ffmpeg",
        "documentation": {}
    },
    {
        "label": "eval_js",
        "importPath": "google.colab.output",
        "description": "google.colab.output",
        "isExtraImport": true,
        "detail": "google.colab.output",
        "documentation": {}
    },
    {
        "label": "HTML",
        "importPath": "IPython.display",
        "description": "IPython.display",
        "isExtraImport": true,
        "detail": "IPython.display",
        "documentation": {}
    },
    {
        "label": "infer_tts",
        "importPath": "klaam.external.FastSpeech2.inference",
        "description": "klaam.external.FastSpeech2.inference",
        "isExtraImport": true,
        "detail": "klaam.external.FastSpeech2.inference",
        "documentation": {}
    },
    {
        "label": "prepare_tts_model",
        "importPath": "klaam.external.FastSpeech2.inference",
        "description": "klaam.external.FastSpeech2.inference",
        "isExtraImport": true,
        "detail": "klaam.external.FastSpeech2.inference",
        "documentation": {}
    },
    {
        "label": "Wav2Vec2ClassificationModel",
        "importPath": "klaam.models.wav2vec",
        "description": "klaam.models.wav2vec",
        "isExtraImport": true,
        "detail": "klaam.models.wav2vec",
        "documentation": {}
    },
    {
        "label": "Wav2Vec2ClassificationModel",
        "importPath": "klaam.models.wav2vec",
        "description": "klaam.models.wav2vec",
        "isExtraImport": true,
        "detail": "klaam.models.wav2vec",
        "documentation": {}
    },
    {
        "label": "CustomWav2Vec2Processor",
        "importPath": "klaam.processors.wav2vec",
        "description": "klaam.processors.wav2vec",
        "isExtraImport": true,
        "detail": "klaam.processors.wav2vec",
        "documentation": {}
    },
    {
        "label": "CustomWav2Vec2Processor",
        "importPath": "klaam.processors.wav2vec",
        "description": "klaam.processors.wav2vec",
        "isExtraImport": true,
        "detail": "klaam.processors.wav2vec",
        "documentation": {}
    },
    {
        "label": "load_file_to_data",
        "importPath": "klaam.utils.utils",
        "description": "klaam.utils.utils",
        "isExtraImport": true,
        "detail": "klaam.utils.utils",
        "documentation": {}
    },
    {
        "label": "predict",
        "importPath": "klaam.utils.utils",
        "description": "klaam.utils.utils",
        "isExtraImport": true,
        "detail": "klaam.utils.utils",
        "documentation": {}
    },
    {
        "label": "logging",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "logging",
        "description": "logging",
        "detail": "logging",
        "documentation": {}
    },
    {
        "label": "dataclass",
        "importPath": "dataclasses",
        "description": "dataclasses",
        "isExtraImport": true,
        "detail": "dataclasses",
        "documentation": {}
    },
    {
        "label": "field",
        "importPath": "dataclasses",
        "description": "dataclasses",
        "isExtraImport": true,
        "detail": "dataclasses",
        "documentation": {}
    },
    {
        "label": "dataclass",
        "importPath": "dataclasses",
        "description": "dataclasses",
        "isExtraImport": true,
        "detail": "dataclasses",
        "documentation": {}
    },
    {
        "label": "field",
        "importPath": "dataclasses",
        "description": "dataclasses",
        "isExtraImport": true,
        "detail": "dataclasses",
        "documentation": {}
    },
    {
        "label": "dataclass",
        "importPath": "dataclasses",
        "description": "dataclasses",
        "isExtraImport": true,
        "detail": "dataclasses",
        "documentation": {}
    },
    {
        "label": "field",
        "importPath": "dataclasses",
        "description": "dataclasses",
        "isExtraImport": true,
        "detail": "dataclasses",
        "documentation": {}
    },
    {
        "label": "dataclass",
        "importPath": "dataclasses",
        "description": "dataclasses",
        "isExtraImport": true,
        "detail": "dataclasses",
        "documentation": {}
    },
    {
        "label": "field",
        "importPath": "dataclasses",
        "description": "dataclasses",
        "isExtraImport": true,
        "detail": "dataclasses",
        "documentation": {}
    },
    {
        "label": "Any",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Dict",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "List",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Optional",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Union",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Any",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Dict",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "List",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Optional",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Union",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Any",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Dict",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "List",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Optional",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Union",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Any",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Dict",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "List",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Optional",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Union",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "version",
        "importPath": "packaging",
        "description": "packaging",
        "isExtraImport": true,
        "detail": "packaging",
        "documentation": {}
    },
    {
        "label": "version",
        "importPath": "packaging",
        "description": "packaging",
        "isExtraImport": true,
        "detail": "packaging",
        "documentation": {}
    },
    {
        "label": "version",
        "importPath": "packaging",
        "description": "packaging",
        "isExtraImport": true,
        "detail": "packaging",
        "documentation": {}
    },
    {
        "label": "version",
        "importPath": "packaging",
        "description": "packaging",
        "isExtraImport": true,
        "detail": "packaging",
        "documentation": {}
    },
    {
        "label": "accuracy_score",
        "importPath": "sklearn.metrics",
        "description": "sklearn.metrics",
        "isExtraImport": true,
        "detail": "sklearn.metrics",
        "documentation": {}
    },
    {
        "label": "get_last_checkpoint",
        "importPath": "transformers.trainer_utils",
        "description": "transformers.trainer_utils",
        "isExtraImport": true,
        "detail": "transformers.trainer_utils",
        "documentation": {}
    },
    {
        "label": "is_main_process",
        "importPath": "transformers.trainer_utils",
        "description": "transformers.trainer_utils",
        "isExtraImport": true,
        "detail": "transformers.trainer_utils",
        "documentation": {}
    },
    {
        "label": "get_last_checkpoint",
        "importPath": "transformers.trainer_utils",
        "description": "transformers.trainer_utils",
        "isExtraImport": true,
        "detail": "transformers.trainer_utils",
        "documentation": {}
    },
    {
        "label": "is_main_process",
        "importPath": "transformers.trainer_utils",
        "description": "transformers.trainer_utils",
        "isExtraImport": true,
        "detail": "transformers.trainer_utils",
        "documentation": {}
    },
    {
        "label": "get_last_checkpoint",
        "importPath": "transformers.trainer_utils",
        "description": "transformers.trainer_utils",
        "isExtraImport": true,
        "detail": "transformers.trainer_utils",
        "documentation": {}
    },
    {
        "label": "is_main_process",
        "importPath": "transformers.trainer_utils",
        "description": "transformers.trainer_utils",
        "isExtraImport": true,
        "detail": "transformers.trainer_utils",
        "documentation": {}
    },
    {
        "label": "get_last_checkpoint",
        "importPath": "transformers.trainer_utils",
        "description": "transformers.trainer_utils",
        "isExtraImport": true,
        "detail": "transformers.trainer_utils",
        "documentation": {}
    },
    {
        "label": "is_main_process",
        "importPath": "transformers.trainer_utils",
        "description": "transformers.trainer_utils",
        "isExtraImport": true,
        "detail": "transformers.trainer_utils",
        "documentation": {}
    },
    {
        "label": "torchaudio",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "torchaudio",
        "description": "torchaudio",
        "detail": "torchaudio",
        "documentation": {}
    },
    {
        "label": "pytest",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "pytest",
        "description": "pytest",
        "detail": "pytest",
        "documentation": {}
    },
    {
        "label": "math",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "math",
        "description": "math",
        "detail": "math",
        "documentation": {}
    },
    {
        "label": "itertools",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "itertools",
        "description": "itertools",
        "detail": "itertools",
        "documentation": {}
    },
    {
        "label": "tensorflow",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "tensorflow",
        "description": "tensorflow",
        "detail": "tensorflow",
        "documentation": {}
    },
    {
        "label": "lookup_ops",
        "importPath": "tensorflow.python.ops",
        "description": "tensorflow.python.ops",
        "isExtraImport": true,
        "detail": "tensorflow.python.ops",
        "documentation": {}
    },
    {
        "label": "lookup_ops",
        "importPath": "tensorflow.python.ops",
        "description": "tensorflow.python.ops",
        "isExtraImport": true,
        "detail": "tensorflow.python.ops",
        "documentation": {}
    },
    {
        "label": "lookup_ops",
        "importPath": "tensorflow.python.ops",
        "description": "tensorflow.python.ops",
        "isExtraImport": true,
        "detail": "tensorflow.python.ops",
        "documentation": {}
    },
    {
        "label": "lookup_ops",
        "importPath": "tensorflow.python.ops",
        "description": "tensorflow.python.ops",
        "isExtraImport": true,
        "detail": "tensorflow.python.ops",
        "documentation": {}
    },
    {
        "label": "subprocess",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "subprocess",
        "description": "subprocess",
        "detail": "subprocess",
        "documentation": {}
    },
    {
        "label": "time",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "time",
        "description": "time",
        "detail": "time",
        "documentation": {}
    },
    {
        "label": "version",
        "importPath": "distutils",
        "description": "distutils",
        "isExtraImport": true,
        "detail": "distutils",
        "documentation": {}
    },
    {
        "label": "six",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "six",
        "description": "six",
        "detail": "six",
        "documentation": {}
    },
    {
        "label": "abc",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "abc",
        "description": "abc",
        "detail": "abc",
        "documentation": {}
    },
    {
        "label": "pprint",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "pprint",
        "description": "pprint",
        "detail": "pprint",
        "documentation": {}
    },
    {
        "label": "window_sumsquare",
        "kind": 2,
        "importPath": "klaam.klaam.external.FastSpeech2.audio.audio_processing",
        "description": "klaam.klaam.external.FastSpeech2.audio.audio_processing",
        "peekOfCode": "def window_sumsquare(\n    window,\n    n_frames,\n    hop_length,\n    win_length,\n    n_fft,\n    dtype=np.float32,\n    norm=None,\n):\n    \"\"\"",
        "detail": "klaam.klaam.external.FastSpeech2.audio.audio_processing",
        "documentation": {}
    },
    {
        "label": "griffin_lim",
        "kind": 2,
        "importPath": "klaam.klaam.external.FastSpeech2.audio.audio_processing",
        "description": "klaam.klaam.external.FastSpeech2.audio.audio_processing",
        "peekOfCode": "def griffin_lim(magnitudes, stft_fn, n_iters=30):\n    \"\"\"\n    PARAMS\n    ------\n    magnitudes: spectrogram magnitudes\n    stft_fn: STFT class with transform (STFT) and inverse (ISTFT) methods\n    \"\"\"\n    angles = np.angle(np.exp(2j * np.pi * np.random.rand(*magnitudes.size())))\n    angles = angles.astype(np.float32)\n    angles = torch.autograd.Variable(torch.from_numpy(angles))",
        "detail": "klaam.klaam.external.FastSpeech2.audio.audio_processing",
        "documentation": {}
    },
    {
        "label": "dynamic_range_compression",
        "kind": 2,
        "importPath": "klaam.klaam.external.FastSpeech2.audio.audio_processing",
        "description": "klaam.klaam.external.FastSpeech2.audio.audio_processing",
        "peekOfCode": "def dynamic_range_compression(x, C=1, clip_val=1e-5):\n    \"\"\"\n    PARAMS\n    ------\n    C: compression factor\n    \"\"\"\n    return torch.log(torch.clamp(x, min=clip_val) * C)\ndef dynamic_range_decompression(x, C=1):\n    \"\"\"\n    PARAMS",
        "detail": "klaam.klaam.external.FastSpeech2.audio.audio_processing",
        "documentation": {}
    },
    {
        "label": "dynamic_range_decompression",
        "kind": 2,
        "importPath": "klaam.klaam.external.FastSpeech2.audio.audio_processing",
        "description": "klaam.klaam.external.FastSpeech2.audio.audio_processing",
        "peekOfCode": "def dynamic_range_decompression(x, C=1):\n    \"\"\"\n    PARAMS\n    ------\n    C: compression factor used to compress\n    \"\"\"\n    return torch.exp(x) / C",
        "detail": "klaam.klaam.external.FastSpeech2.audio.audio_processing",
        "documentation": {}
    },
    {
        "label": "STFT",
        "kind": 6,
        "importPath": "klaam.klaam.external.FastSpeech2.audio.stft",
        "description": "klaam.klaam.external.FastSpeech2.audio.stft",
        "peekOfCode": "class STFT(torch.nn.Module):\n    \"\"\"adapted from Prem Seetharaman's https://github.com/pseeth/pytorch-stft\"\"\"\n    def __init__(self, filter_length, hop_length, win_length, window=\"hann\"):\n        super(STFT, self).__init__()\n        self.filter_length = filter_length\n        self.hop_length = hop_length\n        self.win_length = win_length\n        self.window = window\n        self.forward_transform = None\n        scale = self.filter_length / self.hop_length",
        "detail": "klaam.klaam.external.FastSpeech2.audio.stft",
        "documentation": {}
    },
    {
        "label": "TacotronSTFT",
        "kind": 6,
        "importPath": "klaam.klaam.external.FastSpeech2.audio.stft",
        "description": "klaam.klaam.external.FastSpeech2.audio.stft",
        "peekOfCode": "class TacotronSTFT(torch.nn.Module):\n    def __init__(\n        self,\n        filter_length,\n        hop_length,\n        win_length,\n        n_mel_channels,\n        sampling_rate,\n        mel_fmin,\n        mel_fmax,",
        "detail": "klaam.klaam.external.FastSpeech2.audio.stft",
        "documentation": {}
    },
    {
        "label": "get_mel_from_wav",
        "kind": 2,
        "importPath": "klaam.klaam.external.FastSpeech2.audio.tools",
        "description": "klaam.klaam.external.FastSpeech2.audio.tools",
        "peekOfCode": "def get_mel_from_wav(audio, _stft):\n    audio = torch.clip(torch.FloatTensor(audio).unsqueeze(0), -1, 1)\n    audio = torch.autograd.Variable(audio, requires_grad=False)\n    melspec, energy = _stft.mel_spectrogram(audio)\n    melspec = torch.squeeze(melspec, 0).numpy().astype(np.float32)\n    energy = torch.squeeze(energy, 0).numpy().astype(np.float32)\n    return melspec, energy\ndef inv_mel_spec(mel, out_filename, _stft, griffin_iters=60):\n    mel = torch.stack([mel])\n    mel_decompress = _stft.spectral_de_normalize(mel)",
        "detail": "klaam.klaam.external.FastSpeech2.audio.tools",
        "documentation": {}
    },
    {
        "label": "inv_mel_spec",
        "kind": 2,
        "importPath": "klaam.klaam.external.FastSpeech2.audio.tools",
        "description": "klaam.klaam.external.FastSpeech2.audio.tools",
        "peekOfCode": "def inv_mel_spec(mel, out_filename, _stft, griffin_iters=60):\n    mel = torch.stack([mel])\n    mel_decompress = _stft.spectral_de_normalize(mel)\n    mel_decompress = mel_decompress.transpose(1, 2).data.cpu()\n    spec_from_mel_scaling = 1000\n    spec_from_mel = torch.mm(mel_decompress[0], _stft.mel_basis)\n    spec_from_mel = spec_from_mel.transpose(0, 1).unsqueeze(0)\n    spec_from_mel = spec_from_mel * spec_from_mel_scaling\n    audio = griffin_lim(torch.autograd.Variable(spec_from_mel[:, :, :-1]), _stft._stft_fn, griffin_iters)\n    audio = audio.squeeze()",
        "detail": "klaam.klaam.external.FastSpeech2.audio.tools",
        "documentation": {}
    },
    {
        "label": "ResBlock",
        "kind": 6,
        "importPath": "klaam.klaam.external.FastSpeech2.hifigan.models",
        "description": "klaam.klaam.external.FastSpeech2.hifigan.models",
        "peekOfCode": "class ResBlock(torch.nn.Module):\n    def __init__(self, h, channels, kernel_size=3, dilation=(1, 3, 5)):\n        super(ResBlock, self).__init__()\n        self.h = h\n        self.convs1 = nn.ModuleList(\n            [\n                weight_norm(\n                    Conv1d(\n                        channels,\n                        channels,",
        "detail": "klaam.klaam.external.FastSpeech2.hifigan.models",
        "documentation": {}
    },
    {
        "label": "Generator",
        "kind": 6,
        "importPath": "klaam.klaam.external.FastSpeech2.hifigan.models",
        "description": "klaam.klaam.external.FastSpeech2.hifigan.models",
        "peekOfCode": "class Generator(torch.nn.Module):\n    def __init__(self, h):\n        super(Generator, self).__init__()\n        self.h = h\n        self.num_kernels = len(h.resblock_kernel_sizes)\n        self.num_upsamples = len(h.upsample_rates)\n        self.conv_pre = weight_norm(Conv1d(80, h.upsample_initial_channel, 7, 1, padding=3))\n        resblock = ResBlock\n        self.ups = nn.ModuleList()\n        for i, (u, k) in enumerate(zip(h.upsample_rates, h.upsample_kernel_sizes)):",
        "detail": "klaam.klaam.external.FastSpeech2.hifigan.models",
        "documentation": {}
    },
    {
        "label": "init_weights",
        "kind": 2,
        "importPath": "klaam.klaam.external.FastSpeech2.hifigan.models",
        "description": "klaam.klaam.external.FastSpeech2.hifigan.models",
        "peekOfCode": "def init_weights(m, mean=0.0, std=0.01):\n    classname = m.__class__.__name__\n    if classname.find(\"Conv\") != -1:\n        m.weight.data.normal_(mean, std)\ndef get_padding(kernel_size, dilation=1):\n    return int((kernel_size * dilation - dilation) / 2)\nclass ResBlock(torch.nn.Module):\n    def __init__(self, h, channels, kernel_size=3, dilation=(1, 3, 5)):\n        super(ResBlock, self).__init__()\n        self.h = h",
        "detail": "klaam.klaam.external.FastSpeech2.hifigan.models",
        "documentation": {}
    },
    {
        "label": "get_padding",
        "kind": 2,
        "importPath": "klaam.klaam.external.FastSpeech2.hifigan.models",
        "description": "klaam.klaam.external.FastSpeech2.hifigan.models",
        "peekOfCode": "def get_padding(kernel_size, dilation=1):\n    return int((kernel_size * dilation - dilation) / 2)\nclass ResBlock(torch.nn.Module):\n    def __init__(self, h, channels, kernel_size=3, dilation=(1, 3, 5)):\n        super(ResBlock, self).__init__()\n        self.h = h\n        self.convs1 = nn.ModuleList(\n            [\n                weight_norm(\n                    Conv1d(",
        "detail": "klaam.klaam.external.FastSpeech2.hifigan.models",
        "documentation": {}
    },
    {
        "label": "LRELU_SLOPE",
        "kind": 5,
        "importPath": "klaam.klaam.external.FastSpeech2.hifigan.models",
        "description": "klaam.klaam.external.FastSpeech2.hifigan.models",
        "peekOfCode": "LRELU_SLOPE = 0.1\ndef init_weights(m, mean=0.0, std=0.01):\n    classname = m.__class__.__name__\n    if classname.find(\"Conv\") != -1:\n        m.weight.data.normal_(mean, std)\ndef get_padding(kernel_size, dilation=1):\n    return int((kernel_size * dilation - dilation) / 2)\nclass ResBlock(torch.nn.Module):\n    def __init__(self, h, channels, kernel_size=3, dilation=(1, 3, 5)):\n        super(ResBlock, self).__init__()",
        "detail": "klaam.klaam.external.FastSpeech2.hifigan.models",
        "documentation": {}
    },
    {
        "label": "FastSpeech2",
        "kind": 6,
        "importPath": "klaam.klaam.external.FastSpeech2.model.fastspeech2",
        "description": "klaam.klaam.external.FastSpeech2.model.fastspeech2",
        "peekOfCode": "class FastSpeech2(nn.Module):\n    \"\"\"FastSpeech2\"\"\"\n    def __init__(self, preprocess_config, model_config):\n        super(FastSpeech2, self).__init__()\n        self.model_config = model_config\n        self.encoder = Encoder(model_config)\n        self.variance_adaptor = VarianceAdaptor(preprocess_config, model_config)\n        self.decoder = Decoder(model_config)\n        self.mel_linear = nn.Linear(\n            model_config[\"transformer\"][\"decoder_hidden\"],",
        "detail": "klaam.klaam.external.FastSpeech2.model.fastspeech2",
        "documentation": {}
    },
    {
        "label": "FastSpeech2Loss",
        "kind": 6,
        "importPath": "klaam.klaam.external.FastSpeech2.model.loss",
        "description": "klaam.klaam.external.FastSpeech2.model.loss",
        "peekOfCode": "class FastSpeech2Loss(nn.Module):\n    \"\"\"FastSpeech2 Loss\"\"\"\n    def __init__(self, preprocess_config, model_config):\n        super(FastSpeech2Loss, self).__init__()\n        self.pitch_feature_level = preprocess_config[\"preprocessing\"][\"pitch\"][\"feature\"]\n        self.energy_feature_level = preprocess_config[\"preprocessing\"][\"energy\"][\"feature\"]\n        self.mse_loss = nn.MSELoss()\n        self.mae_loss = nn.L1Loss()\n    def forward(self, inputs, predictions):\n        (",
        "detail": "klaam.klaam.external.FastSpeech2.model.loss",
        "documentation": {}
    },
    {
        "label": "VarianceAdaptor",
        "kind": 6,
        "importPath": "klaam.klaam.external.FastSpeech2.model.modules",
        "description": "klaam.klaam.external.FastSpeech2.model.modules",
        "peekOfCode": "class VarianceAdaptor(nn.Module):\n    \"\"\"Variance Adaptor\"\"\"\n    def __init__(self, preprocess_config, model_config):\n        super(VarianceAdaptor, self).__init__()\n        self.duration_predictor = VariancePredictor(model_config)\n        self.length_regulator = LengthRegulator()\n        self.pitch_predictor = VariancePredictor(model_config)\n        self.energy_predictor = VariancePredictor(model_config)\n        self.pitch_feature_level = preprocess_config[\"preprocessing\"][\"pitch\"][\"feature\"]\n        self.energy_feature_level = preprocess_config[\"preprocessing\"][\"energy\"][\"feature\"]",
        "detail": "klaam.klaam.external.FastSpeech2.model.modules",
        "documentation": {}
    },
    {
        "label": "LengthRegulator",
        "kind": 6,
        "importPath": "klaam.klaam.external.FastSpeech2.model.modules",
        "description": "klaam.klaam.external.FastSpeech2.model.modules",
        "peekOfCode": "class LengthRegulator(nn.Module):\n    \"\"\"Length Regulator\"\"\"\n    def __init__(self):\n        super(LengthRegulator, self).__init__()\n    def LR(self, x, duration, max_len):\n        output = []\n        mel_len = []\n        for batch, expand_target in zip(x, duration):\n            expanded = self.expand(batch, expand_target)\n            output.append(expanded)",
        "detail": "klaam.klaam.external.FastSpeech2.model.modules",
        "documentation": {}
    },
    {
        "label": "VariancePredictor",
        "kind": 6,
        "importPath": "klaam.klaam.external.FastSpeech2.model.modules",
        "description": "klaam.klaam.external.FastSpeech2.model.modules",
        "peekOfCode": "class VariancePredictor(nn.Module):\n    \"\"\"Duration, Pitch and Energy Predictor\"\"\"\n    def __init__(self, model_config):\n        super(VariancePredictor, self).__init__()\n        self.input_size = model_config[\"transformer\"][\"encoder_hidden\"]\n        self.filter_size = model_config[\"variance_predictor\"][\"filter_size\"]\n        self.kernel = model_config[\"variance_predictor\"][\"kernel_size\"]\n        self.conv_output_size = model_config[\"variance_predictor\"][\"filter_size\"]\n        self.dropout = model_config[\"variance_predictor\"][\"dropout\"]\n        self.conv_layer = nn.Sequential(",
        "detail": "klaam.klaam.external.FastSpeech2.model.modules",
        "documentation": {}
    },
    {
        "label": "Conv",
        "kind": 6,
        "importPath": "klaam.klaam.external.FastSpeech2.model.modules",
        "description": "klaam.klaam.external.FastSpeech2.model.modules",
        "peekOfCode": "class Conv(nn.Module):\n    \"\"\"\n    Convolution Module\n    \"\"\"\n    def __init__(\n        self,\n        in_channels,\n        out_channels,\n        kernel_size=1,\n        stride=1,",
        "detail": "klaam.klaam.external.FastSpeech2.model.modules",
        "documentation": {}
    },
    {
        "label": "device",
        "kind": 5,
        "importPath": "klaam.klaam.external.FastSpeech2.model.modules",
        "description": "klaam.klaam.external.FastSpeech2.model.modules",
        "peekOfCode": "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nclass VarianceAdaptor(nn.Module):\n    \"\"\"Variance Adaptor\"\"\"\n    def __init__(self, preprocess_config, model_config):\n        super(VarianceAdaptor, self).__init__()\n        self.duration_predictor = VariancePredictor(model_config)\n        self.length_regulator = LengthRegulator()\n        self.pitch_predictor = VariancePredictor(model_config)\n        self.energy_predictor = VariancePredictor(model_config)\n        self.pitch_feature_level = preprocess_config[\"preprocessing\"][\"pitch\"][\"feature\"]",
        "detail": "klaam.klaam.external.FastSpeech2.model.modules",
        "documentation": {}
    },
    {
        "label": "ScheduledOptim",
        "kind": 6,
        "importPath": "klaam.klaam.external.FastSpeech2.model.optimizer",
        "description": "klaam.klaam.external.FastSpeech2.model.optimizer",
        "peekOfCode": "class ScheduledOptim:\n    \"\"\"A simple wrapper class for learning rate scheduling\"\"\"\n    def __init__(self, model, train_config, model_config, current_step):\n        self._optimizer = torch.optim.Adam(\n            model.parameters(),\n            betas=train_config[\"optimizer\"][\"betas\"],\n            eps=train_config[\"optimizer\"][\"eps\"],\n            weight_decay=train_config[\"optimizer\"][\"weight_decay\"],\n        )\n        self.n_warmup_steps = train_config[\"optimizer\"][\"warm_up_step\"]",
        "detail": "klaam.klaam.external.FastSpeech2.model.optimizer",
        "documentation": {}
    },
    {
        "label": "findStressIndex",
        "kind": 2,
        "importPath": "klaam.klaam.external.FastSpeech2.phonetise.find_stress",
        "description": "klaam.klaam.external.FastSpeech2.phonetise.find_stress",
        "peekOfCode": "def findStressIndex(sequence):  # Find stress syllable in word starting from \"start\"\n    if sequence == \"\" or len(sequence) == 0:\n        return \"\"\n    # print(sequence)\n    consonants = [\n        \"r\",\n        \"g\",\n        \"y\",\n        \"G\",\n        \"b\",",
        "detail": "klaam.klaam.external.FastSpeech2.phonetise.find_stress",
        "documentation": {}
    },
    {
        "label": "arabicToBuckwalter",
        "kind": 2,
        "importPath": "klaam.klaam.external.FastSpeech2.phonetise.phonetise_arabic",
        "description": "klaam.klaam.external.FastSpeech2.phonetise.phonetise_arabic",
        "peekOfCode": "def arabicToBuckwalter(word):  # Convert input string to Buckwalter\n    res = \"\"\n    for letter in word:\n        if letter in buckwalter:\n            res += buckwalter[letter]\n        else:\n            res += letter\n    return res\ndef buckwalterToArabic(word):  # Convert input string to Arabic\n    res = \"\"",
        "detail": "klaam.klaam.external.FastSpeech2.phonetise.phonetise_arabic",
        "documentation": {}
    },
    {
        "label": "buckwalterToArabic",
        "kind": 2,
        "importPath": "klaam.klaam.external.FastSpeech2.phonetise.phonetise_arabic",
        "description": "klaam.klaam.external.FastSpeech2.phonetise.phonetise_arabic",
        "peekOfCode": "def buckwalterToArabic(word):  # Convert input string to Arabic\n    res = \"\"\n    for letter in word:\n        if letter in ArabicScript:\n            res += ArabicScript[letter]\n        else:\n            res += letter\n    return res\n# ----------------------------------------------------------------------------\n# Grapheme to Phoneme mappings------------------------------------------------",
        "detail": "klaam.klaam.external.FastSpeech2.phonetise.phonetise_arabic",
        "documentation": {}
    },
    {
        "label": "isFixedWord",
        "kind": 2,
        "importPath": "klaam.klaam.external.FastSpeech2.phonetise.phonetise_arabic",
        "description": "klaam.klaam.external.FastSpeech2.phonetise.phonetise_arabic",
        "peekOfCode": "def isFixedWord(word, results, orthography, pronunciations):\n    lastLetter = \"\"\n    if len(word) > 0:\n        lastLetter = word[-1]\n    if lastLetter == \"a\":\n        lastLetter = [\"a\", \"A\"]\n    elif lastLetter == \"A\":\n        lastLetter = [\"aa\"]\n    elif lastLetter == \"u\":\n        lastLetter = [\"u0\"]",
        "detail": "klaam.klaam.external.FastSpeech2.phonetise.phonetise_arabic",
        "documentation": {}
    },
    {
        "label": "phonetise",
        "kind": 2,
        "importPath": "klaam.klaam.external.FastSpeech2.phonetise.phonetise_arabic",
        "description": "klaam.klaam.external.FastSpeech2.phonetise.phonetise_arabic",
        "peekOfCode": "def phonetise(text):\n    utterances = text.splitlines()\n    result = \"\"  # Pronunciations Dictionary\n    utterancesPronuncations = []  # Most likely pronunciation for all utterances\n    utterancesPronuncationsWithBoundaries = []  # Most likely pronunciation for all utterances\n    # -----------------------------------------------------------------------------------------------------\n    # Loop through utterances------------------------------------------------------------------------------\n    # -----------------------------------------------------------------------------------------------------\n    utteranceNumber = 1\n    for utterance in utterances:",
        "detail": "klaam.klaam.external.FastSpeech2.phonetise.phonetise_arabic",
        "documentation": {}
    },
    {
        "label": "buckwalter",
        "kind": 5,
        "importPath": "klaam.klaam.external.FastSpeech2.phonetise.phonetise_arabic",
        "description": "klaam.klaam.external.FastSpeech2.phonetise.phonetise_arabic",
        "peekOfCode": "buckwalter = {  # mapping from Arabic script to Buckwalter\n    \"\\u0628\": \"b\",\n    \"\\u0630\": \"*\",\n    \"\\u0637\": \"T\",\n    \"\\u0645\": \"m\",\n    \"\\u062a\": \"t\",\n    \"\\u0631\": \"r\",\n    \"\\u0638\": \"Z\",\n    \"\\u0646\": \"n\",\n    \"\\u062b\": \"^\",",
        "detail": "klaam.klaam.external.FastSpeech2.phonetise.phonetise_arabic",
        "documentation": {}
    },
    {
        "label": "ArabicScript",
        "kind": 5,
        "importPath": "klaam.klaam.external.FastSpeech2.phonetise.phonetise_arabic",
        "description": "klaam.klaam.external.FastSpeech2.phonetise.phonetise_arabic",
        "peekOfCode": "ArabicScript = {  # mapping from Buckwalter to Arabic script\n    \"b\": \"\\u0628\",\n    \"*\": \"\\u0630\",\n    \"T\": \"\\u0637\",\n    \"m\": \"\\u0645\",\n    \"t\": \"\\u062a\",\n    \"r\": \"\\u0631\",\n    \"Z\": \"\\u0638\",\n    \"n\": \"\\u0646\",\n    \"^\": \"\\u062b\",",
        "detail": "klaam.klaam.external.FastSpeech2.phonetise.phonetise_arabic",
        "documentation": {}
    },
    {
        "label": "unambiguousConsonantMap",
        "kind": 5,
        "importPath": "klaam.klaam.external.FastSpeech2.phonetise.phonetise_arabic",
        "description": "klaam.klaam.external.FastSpeech2.phonetise.phonetise_arabic",
        "peekOfCode": "unambiguousConsonantMap = {\n    \"b\": \"b\",\n    \"*\": \"*\",\n    \"T\": \"T\",\n    \"m\": \"m\",\n    \"t\": \"t\",\n    \"r\": \"r\",\n    \"Z\": \"Z\",\n    \"n\": \"n\",\n    \"^\": \"^\",",
        "detail": "klaam.klaam.external.FastSpeech2.phonetise.phonetise_arabic",
        "documentation": {}
    },
    {
        "label": "ambiguousConsonantMap",
        "kind": 5,
        "importPath": "klaam.klaam.external.FastSpeech2.phonetise.phonetise_arabic",
        "description": "klaam.klaam.external.FastSpeech2.phonetise.phonetise_arabic",
        "peekOfCode": "ambiguousConsonantMap = {\n    \"l\": [\"l\", \"\"],\n    \"w\": \"w\",\n    \"y\": \"y\",\n    \"p\": [\"t\", \"\"],  # These consonants are only unambiguous in certain contexts\n}\nmaddaMap = {\"|\": [[\"<\", \"aa\"], [\"<\", \"AA\"]]}\nvowelMap = {\n    \"A\": [[\"aa\", \"\"], [\"AA\", \"\"]],\n    \"Y\": [[\"aa\", \"\"], [\"AA\", \"\"]],",
        "detail": "klaam.klaam.external.FastSpeech2.phonetise.phonetise_arabic",
        "documentation": {}
    },
    {
        "label": "maddaMap",
        "kind": 5,
        "importPath": "klaam.klaam.external.FastSpeech2.phonetise.phonetise_arabic",
        "description": "klaam.klaam.external.FastSpeech2.phonetise.phonetise_arabic",
        "peekOfCode": "maddaMap = {\"|\": [[\"<\", \"aa\"], [\"<\", \"AA\"]]}\nvowelMap = {\n    \"A\": [[\"aa\", \"\"], [\"AA\", \"\"]],\n    \"Y\": [[\"aa\", \"\"], [\"AA\", \"\"]],\n    \"w\": [[\"uu0\", \"uu1\"], [\"UU0\", \"UU1\"]],\n    \"y\": [[\"ii0\", \"ii1\"], [\"II0\", \"II1\"]],\n    \"a\": [\"a\", \"A\"],\n    \"u\": [[\"u0\", \"u1\"], [\"U0\", \"U1\"]],\n    \"i\": [[\"i0\", \"i1\"], [\"I0\", \"I1\"]],\n}",
        "detail": "klaam.klaam.external.FastSpeech2.phonetise.phonetise_arabic",
        "documentation": {}
    },
    {
        "label": "vowelMap",
        "kind": 5,
        "importPath": "klaam.klaam.external.FastSpeech2.phonetise.phonetise_arabic",
        "description": "klaam.klaam.external.FastSpeech2.phonetise.phonetise_arabic",
        "peekOfCode": "vowelMap = {\n    \"A\": [[\"aa\", \"\"], [\"AA\", \"\"]],\n    \"Y\": [[\"aa\", \"\"], [\"AA\", \"\"]],\n    \"w\": [[\"uu0\", \"uu1\"], [\"UU0\", \"UU1\"]],\n    \"y\": [[\"ii0\", \"ii1\"], [\"II0\", \"II1\"]],\n    \"a\": [\"a\", \"A\"],\n    \"u\": [[\"u0\", \"u1\"], [\"U0\", \"U1\"]],\n    \"i\": [[\"i0\", \"i1\"], [\"I0\", \"I1\"]],\n}\nnunationMap = {\"F\": [[\"a\", \"n\"], [\"A\", \"n\"]], \"N\": [[\"u1\", \"n\"], [\"U1\", \"n\"]], \"K\": [[\"i1\", \"n\"], [\"I1\", \"n\"]]}",
        "detail": "klaam.klaam.external.FastSpeech2.phonetise.phonetise_arabic",
        "documentation": {}
    },
    {
        "label": "nunationMap",
        "kind": 5,
        "importPath": "klaam.klaam.external.FastSpeech2.phonetise.phonetise_arabic",
        "description": "klaam.klaam.external.FastSpeech2.phonetise.phonetise_arabic",
        "peekOfCode": "nunationMap = {\"F\": [[\"a\", \"n\"], [\"A\", \"n\"]], \"N\": [[\"u1\", \"n\"], [\"U1\", \"n\"]], \"K\": [[\"i1\", \"n\"], [\"I1\", \"n\"]]}\ndiacritics = [\"o\", \"a\", \"u\", \"i\", \"F\", \"N\", \"K\", \"~\"]\ndiacriticsWithoutShadda = [\"o\", \"a\", \"u\", \"i\", \"F\", \"N\", \"K\"]\nemphatics = [\"D\", \"S\", \"T\", \"Z\", \"g\", \"x\", \"q\"]\nforwardEmphatics = [\"g\", \"x\"]\nconsonants = [\n    \">\",\n    \"<\",\n    \"}\",\n    \"&\",",
        "detail": "klaam.klaam.external.FastSpeech2.phonetise.phonetise_arabic",
        "documentation": {}
    },
    {
        "label": "diacritics",
        "kind": 5,
        "importPath": "klaam.klaam.external.FastSpeech2.phonetise.phonetise_arabic",
        "description": "klaam.klaam.external.FastSpeech2.phonetise.phonetise_arabic",
        "peekOfCode": "diacritics = [\"o\", \"a\", \"u\", \"i\", \"F\", \"N\", \"K\", \"~\"]\ndiacriticsWithoutShadda = [\"o\", \"a\", \"u\", \"i\", \"F\", \"N\", \"K\"]\nemphatics = [\"D\", \"S\", \"T\", \"Z\", \"g\", \"x\", \"q\"]\nforwardEmphatics = [\"g\", \"x\"]\nconsonants = [\n    \">\",\n    \"<\",\n    \"}\",\n    \"&\",\n    \"'\",",
        "detail": "klaam.klaam.external.FastSpeech2.phonetise.phonetise_arabic",
        "documentation": {}
    },
    {
        "label": "diacriticsWithoutShadda",
        "kind": 5,
        "importPath": "klaam.klaam.external.FastSpeech2.phonetise.phonetise_arabic",
        "description": "klaam.klaam.external.FastSpeech2.phonetise.phonetise_arabic",
        "peekOfCode": "diacriticsWithoutShadda = [\"o\", \"a\", \"u\", \"i\", \"F\", \"N\", \"K\"]\nemphatics = [\"D\", \"S\", \"T\", \"Z\", \"g\", \"x\", \"q\"]\nforwardEmphatics = [\"g\", \"x\"]\nconsonants = [\n    \">\",\n    \"<\",\n    \"}\",\n    \"&\",\n    \"'\",\n    \"b\",",
        "detail": "klaam.klaam.external.FastSpeech2.phonetise.phonetise_arabic",
        "documentation": {}
    },
    {
        "label": "emphatics",
        "kind": 5,
        "importPath": "klaam.klaam.external.FastSpeech2.phonetise.phonetise_arabic",
        "description": "klaam.klaam.external.FastSpeech2.phonetise.phonetise_arabic",
        "peekOfCode": "emphatics = [\"D\", \"S\", \"T\", \"Z\", \"g\", \"x\", \"q\"]\nforwardEmphatics = [\"g\", \"x\"]\nconsonants = [\n    \">\",\n    \"<\",\n    \"}\",\n    \"&\",\n    \"'\",\n    \"b\",\n    \"t\",",
        "detail": "klaam.klaam.external.FastSpeech2.phonetise.phonetise_arabic",
        "documentation": {}
    },
    {
        "label": "forwardEmphatics",
        "kind": 5,
        "importPath": "klaam.klaam.external.FastSpeech2.phonetise.phonetise_arabic",
        "description": "klaam.klaam.external.FastSpeech2.phonetise.phonetise_arabic",
        "peekOfCode": "forwardEmphatics = [\"g\", \"x\"]\nconsonants = [\n    \">\",\n    \"<\",\n    \"}\",\n    \"&\",\n    \"'\",\n    \"b\",\n    \"t\",\n    \"^\",",
        "detail": "klaam.klaam.external.FastSpeech2.phonetise.phonetise_arabic",
        "documentation": {}
    },
    {
        "label": "consonants",
        "kind": 5,
        "importPath": "klaam.klaam.external.FastSpeech2.phonetise.phonetise_arabic",
        "description": "klaam.klaam.external.FastSpeech2.phonetise.phonetise_arabic",
        "peekOfCode": "consonants = [\n    \">\",\n    \"<\",\n    \"}\",\n    \"&\",\n    \"'\",\n    \"b\",\n    \"t\",\n    \"^\",\n    \"j\",",
        "detail": "klaam.klaam.external.FastSpeech2.phonetise.phonetise_arabic",
        "documentation": {}
    },
    {
        "label": "fixedWords",
        "kind": 5,
        "importPath": "klaam.klaam.external.FastSpeech2.phonetise.phonetise_arabic",
        "description": "klaam.klaam.external.FastSpeech2.phonetise.phonetise_arabic",
        "peekOfCode": "fixedWords = {\n    \"h*A\": [\n        \"h aa * aa\",\n        \"h aa * a\",\n    ],\n    \"bh*A\": [\n        \"b i0 h aa * aa\",\n        \"b i0 h aa * a\",\n    ],\n    \"kh*A\": [",
        "detail": "klaam.klaam.external.FastSpeech2.phonetise.phonetise_arabic",
        "documentation": {}
    },
    {
        "label": "prepare_align",
        "kind": 2,
        "importPath": "klaam.klaam.external.FastSpeech2.preprocessor.aishell3",
        "description": "klaam.klaam.external.FastSpeech2.preprocessor.aishell3",
        "peekOfCode": "def prepare_align(config):\n    in_dir = config[\"path\"][\"corpus_path\"]\n    out_dir = config[\"path\"][\"raw_path\"]\n    sampling_rate = config[\"preprocessing\"][\"audio\"][\"sampling_rate\"]\n    max_wav_value = config[\"preprocessing\"][\"audio\"][\"max_wav_value\"]\n    for dataset in [\"train\", \"test\"]:\n        print(\"Processing {}ing set...\".format(dataset))\n        with open(os.path.join(in_dir, dataset, \"content.txt\"), encoding=\"utf-8\") as f:\n            for line in tqdm(f):\n                wav_name, text = line.strip(\"\\n\").split(\"\\t\")",
        "detail": "klaam.klaam.external.FastSpeech2.preprocessor.aishell3",
        "documentation": {}
    },
    {
        "label": "prepare_align",
        "kind": 2,
        "importPath": "klaam.klaam.external.FastSpeech2.preprocessor.arabic",
        "description": "klaam.klaam.external.FastSpeech2.preprocessor.arabic",
        "peekOfCode": "def prepare_align(config):\n    in_dir = config[\"path\"][\"corpus_path\"]\n    out_dir = config[\"path\"][\"raw_path\"]\n    sampling_rate = config[\"preprocessing\"][\"audio\"][\"sampling_rate\"]\n    max_wav_value = config[\"preprocessing\"][\"audio\"][\"max_wav_value\"]\n    cleaners = config[\"preprocessing\"][\"text\"][\"text_cleaners\"]\n    speaker = \"Arabic\"\n    with open(os.path.join(in_dir, \"metadata.csv\"), encoding=\"utf-8\") as f:\n        for line in tqdm(f):\n            parts = line.strip().split(\"|\")",
        "detail": "klaam.klaam.external.FastSpeech2.preprocessor.arabic",
        "documentation": {}
    },
    {
        "label": "prepare_align",
        "kind": 2,
        "importPath": "klaam.klaam.external.FastSpeech2.preprocessor.libritts",
        "description": "klaam.klaam.external.FastSpeech2.preprocessor.libritts",
        "peekOfCode": "def prepare_align(config):\n    in_dir = config[\"path\"][\"corpus_path\"]\n    out_dir = config[\"path\"][\"raw_path\"]\n    sampling_rate = config[\"preprocessing\"][\"audio\"][\"sampling_rate\"]\n    max_wav_value = config[\"preprocessing\"][\"audio\"][\"max_wav_value\"]\n    cleaners = config[\"preprocessing\"][\"text\"][\"text_cleaners\"]\n    for speaker in tqdm(os.listdir(in_dir)):\n        for chapter in os.listdir(os.path.join(in_dir, speaker)):\n            for file_name in os.listdir(os.path.join(in_dir, speaker, chapter)):\n                if file_name[-4:] != \".wav\":",
        "detail": "klaam.klaam.external.FastSpeech2.preprocessor.libritts",
        "documentation": {}
    },
    {
        "label": "prepare_align",
        "kind": 2,
        "importPath": "klaam.klaam.external.FastSpeech2.preprocessor.ljspeech",
        "description": "klaam.klaam.external.FastSpeech2.preprocessor.ljspeech",
        "peekOfCode": "def prepare_align(config):\n    in_dir = config[\"path\"][\"corpus_path\"]\n    out_dir = config[\"path\"][\"raw_path\"]\n    sampling_rate = config[\"preprocessing\"][\"audio\"][\"sampling_rate\"]\n    max_wav_value = config[\"preprocessing\"][\"audio\"][\"max_wav_value\"]\n    cleaners = config[\"preprocessing\"][\"text\"][\"text_cleaners\"]\n    speaker = \"LJSpeech\"\n    with open(os.path.join(in_dir, \"metadata.csv\"), encoding=\"utf-8\") as f:\n        for line in tqdm(f):\n            parts = line.strip().split(\"|\")",
        "detail": "klaam.klaam.external.FastSpeech2.preprocessor.ljspeech",
        "documentation": {}
    },
    {
        "label": "Preprocessor",
        "kind": 6,
        "importPath": "klaam.klaam.external.FastSpeech2.preprocessor.preprocessor",
        "description": "klaam.klaam.external.FastSpeech2.preprocessor.preprocessor",
        "peekOfCode": "class Preprocessor:\n    def __init__(self, config):\n        self.config = config\n        self.in_dir = config[\"path\"][\"raw_path\"]\n        self.out_dir = config[\"path\"][\"preprocessed_path\"]\n        self.out_dir = config[\"path\"][\"stats_path\"]\n        self.val_size = config[\"preprocessing\"][\"val_size\"]\n        self.sampling_rate = config[\"preprocessing\"][\"audio\"][\"sampling_rate\"]\n        self.hop_length = config[\"preprocessing\"][\"stft\"][\"hop_length\"]\n        assert config[\"preprocessing\"][\"pitch\"][\"feature\"] in [",
        "detail": "klaam.klaam.external.FastSpeech2.preprocessor.preprocessor",
        "documentation": {}
    },
    {
        "label": "expand_abbreviations",
        "kind": 2,
        "importPath": "klaam.klaam.external.FastSpeech2.text.cleaners",
        "description": "klaam.klaam.external.FastSpeech2.text.cleaners",
        "peekOfCode": "def expand_abbreviations(text):\n    for regex, replacement in _abbreviations:\n        text = re.sub(regex, replacement, text)\n    return text\ndef expand_numbers(text):\n    return normalize_numbers(text)\ndef lowercase(text):\n    return text.lower()\ndef collapse_whitespace(text):\n    return re.sub(_whitespace_re, \" \", text)",
        "detail": "klaam.klaam.external.FastSpeech2.text.cleaners",
        "documentation": {}
    },
    {
        "label": "expand_numbers",
        "kind": 2,
        "importPath": "klaam.klaam.external.FastSpeech2.text.cleaners",
        "description": "klaam.klaam.external.FastSpeech2.text.cleaners",
        "peekOfCode": "def expand_numbers(text):\n    return normalize_numbers(text)\ndef lowercase(text):\n    return text.lower()\ndef collapse_whitespace(text):\n    return re.sub(_whitespace_re, \" \", text)\ndef convert_to_ascii(text):\n    return unidecode(text)\ndef basic_cleaners(text):\n    \"\"\"Basic pipeline that lowercases and collapses whitespace without transliteration.\"\"\"",
        "detail": "klaam.klaam.external.FastSpeech2.text.cleaners",
        "documentation": {}
    },
    {
        "label": "lowercase",
        "kind": 2,
        "importPath": "klaam.klaam.external.FastSpeech2.text.cleaners",
        "description": "klaam.klaam.external.FastSpeech2.text.cleaners",
        "peekOfCode": "def lowercase(text):\n    return text.lower()\ndef collapse_whitespace(text):\n    return re.sub(_whitespace_re, \" \", text)\ndef convert_to_ascii(text):\n    return unidecode(text)\ndef basic_cleaners(text):\n    \"\"\"Basic pipeline that lowercases and collapses whitespace without transliteration.\"\"\"\n    text = lowercase(text)\n    text = collapse_whitespace(text)",
        "detail": "klaam.klaam.external.FastSpeech2.text.cleaners",
        "documentation": {}
    },
    {
        "label": "collapse_whitespace",
        "kind": 2,
        "importPath": "klaam.klaam.external.FastSpeech2.text.cleaners",
        "description": "klaam.klaam.external.FastSpeech2.text.cleaners",
        "peekOfCode": "def collapse_whitespace(text):\n    return re.sub(_whitespace_re, \" \", text)\ndef convert_to_ascii(text):\n    return unidecode(text)\ndef basic_cleaners(text):\n    \"\"\"Basic pipeline that lowercases and collapses whitespace without transliteration.\"\"\"\n    text = lowercase(text)\n    text = collapse_whitespace(text)\n    return text\ndef transliteration_cleaners(text):",
        "detail": "klaam.klaam.external.FastSpeech2.text.cleaners",
        "documentation": {}
    },
    {
        "label": "convert_to_ascii",
        "kind": 2,
        "importPath": "klaam.klaam.external.FastSpeech2.text.cleaners",
        "description": "klaam.klaam.external.FastSpeech2.text.cleaners",
        "peekOfCode": "def convert_to_ascii(text):\n    return unidecode(text)\ndef basic_cleaners(text):\n    \"\"\"Basic pipeline that lowercases and collapses whitespace without transliteration.\"\"\"\n    text = lowercase(text)\n    text = collapse_whitespace(text)\n    return text\ndef transliteration_cleaners(text):\n    \"\"\"Pipeline for non-English text that transliterates to ASCII.\"\"\"\n    text = convert_to_ascii(text)",
        "detail": "klaam.klaam.external.FastSpeech2.text.cleaners",
        "documentation": {}
    },
    {
        "label": "basic_cleaners",
        "kind": 2,
        "importPath": "klaam.klaam.external.FastSpeech2.text.cleaners",
        "description": "klaam.klaam.external.FastSpeech2.text.cleaners",
        "peekOfCode": "def basic_cleaners(text):\n    \"\"\"Basic pipeline that lowercases and collapses whitespace without transliteration.\"\"\"\n    text = lowercase(text)\n    text = collapse_whitespace(text)\n    return text\ndef transliteration_cleaners(text):\n    \"\"\"Pipeline for non-English text that transliterates to ASCII.\"\"\"\n    text = convert_to_ascii(text)\n    text = lowercase(text)\n    text = collapse_whitespace(text)",
        "detail": "klaam.klaam.external.FastSpeech2.text.cleaners",
        "documentation": {}
    },
    {
        "label": "transliteration_cleaners",
        "kind": 2,
        "importPath": "klaam.klaam.external.FastSpeech2.text.cleaners",
        "description": "klaam.klaam.external.FastSpeech2.text.cleaners",
        "peekOfCode": "def transliteration_cleaners(text):\n    \"\"\"Pipeline for non-English text that transliterates to ASCII.\"\"\"\n    text = convert_to_ascii(text)\n    text = lowercase(text)\n    text = collapse_whitespace(text)\n    return text\ndef english_cleaners(text):\n    \"\"\"Pipeline for English text, including number and abbreviation expansion.\"\"\"\n    text = convert_to_ascii(text)\n    text = lowercase(text)",
        "detail": "klaam.klaam.external.FastSpeech2.text.cleaners",
        "documentation": {}
    },
    {
        "label": "english_cleaners",
        "kind": 2,
        "importPath": "klaam.klaam.external.FastSpeech2.text.cleaners",
        "description": "klaam.klaam.external.FastSpeech2.text.cleaners",
        "peekOfCode": "def english_cleaners(text):\n    \"\"\"Pipeline for English text, including number and abbreviation expansion.\"\"\"\n    text = convert_to_ascii(text)\n    text = lowercase(text)\n    text = expand_numbers(text)\n    text = expand_abbreviations(text)\n    text = collapse_whitespace(text)\n    return text",
        "detail": "klaam.klaam.external.FastSpeech2.text.cleaners",
        "documentation": {}
    },
    {
        "label": "_whitespace_re",
        "kind": 5,
        "importPath": "klaam.klaam.external.FastSpeech2.text.cleaners",
        "description": "klaam.klaam.external.FastSpeech2.text.cleaners",
        "peekOfCode": "_whitespace_re = re.compile(r\"\\s+\")\n# List of (regular expression, replacement) pairs for abbreviations:\n_abbreviations = [\n    (re.compile(\"\\\\b%s\\\\.\" % x[0], re.IGNORECASE), x[1])\n    for x in [\n        (\"mrs\", \"misess\"),\n        (\"mr\", \"mister\"),\n        (\"dr\", \"doctor\"),\n        (\"st\", \"saint\"),\n        (\"co\", \"company\"),",
        "detail": "klaam.klaam.external.FastSpeech2.text.cleaners",
        "documentation": {}
    },
    {
        "label": "_abbreviations",
        "kind": 5,
        "importPath": "klaam.klaam.external.FastSpeech2.text.cleaners",
        "description": "klaam.klaam.external.FastSpeech2.text.cleaners",
        "peekOfCode": "_abbreviations = [\n    (re.compile(\"\\\\b%s\\\\.\" % x[0], re.IGNORECASE), x[1])\n    for x in [\n        (\"mrs\", \"misess\"),\n        (\"mr\", \"mister\"),\n        (\"dr\", \"doctor\"),\n        (\"st\", \"saint\"),\n        (\"co\", \"company\"),\n        (\"jr\", \"junior\"),\n        (\"maj\", \"major\"),",
        "detail": "klaam.klaam.external.FastSpeech2.text.cleaners",
        "documentation": {}
    },
    {
        "label": "CMUDict",
        "kind": 6,
        "importPath": "klaam.klaam.external.FastSpeech2.text.cmudict",
        "description": "klaam.klaam.external.FastSpeech2.text.cmudict",
        "peekOfCode": "class CMUDict:\n    \"\"\"Thin wrapper around CMUDict data. http://www.speech.cs.cmu.edu/cgi-bin/cmudict\"\"\"\n    def __init__(self, file_or_path, keep_ambiguous=True):\n        if isinstance(file_or_path, str):\n            with open(file_or_path, encoding=\"latin-1\") as f:\n                entries = _parse_cmudict(f)\n        else:\n            entries = _parse_cmudict(file_or_path)\n        if not keep_ambiguous:\n            entries = {word: pron for word, pron in entries.items() if len(pron) == 1}",
        "detail": "klaam.klaam.external.FastSpeech2.text.cmudict",
        "documentation": {}
    },
    {
        "label": "valid_symbols",
        "kind": 5,
        "importPath": "klaam.klaam.external.FastSpeech2.text.cmudict",
        "description": "klaam.klaam.external.FastSpeech2.text.cmudict",
        "peekOfCode": "valid_symbols = [\n    \"AA\",\n    \"AA0\",\n    \"AA1\",\n    \"AA2\",\n    \"AE\",\n    \"AE0\",\n    \"AE1\",\n    \"AE2\",\n    \"AH\",",
        "detail": "klaam.klaam.external.FastSpeech2.text.cmudict",
        "documentation": {}
    },
    {
        "label": "_valid_symbol_set",
        "kind": 5,
        "importPath": "klaam.klaam.external.FastSpeech2.text.cmudict",
        "description": "klaam.klaam.external.FastSpeech2.text.cmudict",
        "peekOfCode": "_valid_symbol_set = set(valid_symbols)\nclass CMUDict:\n    \"\"\"Thin wrapper around CMUDict data. http://www.speech.cs.cmu.edu/cgi-bin/cmudict\"\"\"\n    def __init__(self, file_or_path, keep_ambiguous=True):\n        if isinstance(file_or_path, str):\n            with open(file_or_path, encoding=\"latin-1\") as f:\n                entries = _parse_cmudict(f)\n        else:\n            entries = _parse_cmudict(file_or_path)\n        if not keep_ambiguous:",
        "detail": "klaam.klaam.external.FastSpeech2.text.cmudict",
        "documentation": {}
    },
    {
        "label": "_alt_re",
        "kind": 5,
        "importPath": "klaam.klaam.external.FastSpeech2.text.cmudict",
        "description": "klaam.klaam.external.FastSpeech2.text.cmudict",
        "peekOfCode": "_alt_re = re.compile(r\"\\([0-9]+\\)\")\ndef _parse_cmudict(file):\n    cmudict = {}\n    for line in file:\n        if len(line) and (line[0] >= \"A\" and line[0] <= \"Z\" or line[0] == \"'\"):\n            parts = line.split(\"  \")\n            word = re.sub(_alt_re, \"\", parts[0])\n            pronunciation = _get_pronunciation(parts[1])\n            if pronunciation:\n                if word in cmudict:",
        "detail": "klaam.klaam.external.FastSpeech2.text.cmudict",
        "documentation": {}
    },
    {
        "label": "normalize_numbers",
        "kind": 2,
        "importPath": "klaam.klaam.external.FastSpeech2.text.numbers",
        "description": "klaam.klaam.external.FastSpeech2.text.numbers",
        "peekOfCode": "def normalize_numbers(text):\n    text = re.sub(_comma_number_re, _remove_commas, text)\n    text = re.sub(_pounds_re, r\"\\1 pounds\", text)\n    text = re.sub(_dollars_re, _expand_dollars, text)\n    text = re.sub(_decimal_number_re, _expand_decimal_point, text)\n    text = re.sub(_ordinal_re, _expand_ordinal, text)\n    text = re.sub(_number_re, _expand_number, text)\n    return text",
        "detail": "klaam.klaam.external.FastSpeech2.text.numbers",
        "documentation": {}
    },
    {
        "label": "_inflect",
        "kind": 5,
        "importPath": "klaam.klaam.external.FastSpeech2.text.numbers",
        "description": "klaam.klaam.external.FastSpeech2.text.numbers",
        "peekOfCode": "_inflect = inflect.engine()\n_comma_number_re = re.compile(r\"([0-9][0-9\\,]+[0-9])\")\n_decimal_number_re = re.compile(r\"([0-9]+\\.[0-9]+)\")\n_pounds_re = re.compile(r\"£([0-9\\,]*[0-9]+)\")\n_dollars_re = re.compile(r\"\\$([0-9\\.\\,]*[0-9]+)\")\n_ordinal_re = re.compile(r\"[0-9]+(st|nd|rd|th)\")\n_number_re = re.compile(r\"[0-9]+\")\ndef _remove_commas(m):\n    return m.group(1).replace(\",\", \"\")\ndef _expand_decimal_point(m):",
        "detail": "klaam.klaam.external.FastSpeech2.text.numbers",
        "documentation": {}
    },
    {
        "label": "_comma_number_re",
        "kind": 5,
        "importPath": "klaam.klaam.external.FastSpeech2.text.numbers",
        "description": "klaam.klaam.external.FastSpeech2.text.numbers",
        "peekOfCode": "_comma_number_re = re.compile(r\"([0-9][0-9\\,]+[0-9])\")\n_decimal_number_re = re.compile(r\"([0-9]+\\.[0-9]+)\")\n_pounds_re = re.compile(r\"£([0-9\\,]*[0-9]+)\")\n_dollars_re = re.compile(r\"\\$([0-9\\.\\,]*[0-9]+)\")\n_ordinal_re = re.compile(r\"[0-9]+(st|nd|rd|th)\")\n_number_re = re.compile(r\"[0-9]+\")\ndef _remove_commas(m):\n    return m.group(1).replace(\",\", \"\")\ndef _expand_decimal_point(m):\n    return m.group(1).replace(\".\", \" point \")",
        "detail": "klaam.klaam.external.FastSpeech2.text.numbers",
        "documentation": {}
    },
    {
        "label": "_decimal_number_re",
        "kind": 5,
        "importPath": "klaam.klaam.external.FastSpeech2.text.numbers",
        "description": "klaam.klaam.external.FastSpeech2.text.numbers",
        "peekOfCode": "_decimal_number_re = re.compile(r\"([0-9]+\\.[0-9]+)\")\n_pounds_re = re.compile(r\"£([0-9\\,]*[0-9]+)\")\n_dollars_re = re.compile(r\"\\$([0-9\\.\\,]*[0-9]+)\")\n_ordinal_re = re.compile(r\"[0-9]+(st|nd|rd|th)\")\n_number_re = re.compile(r\"[0-9]+\")\ndef _remove_commas(m):\n    return m.group(1).replace(\",\", \"\")\ndef _expand_decimal_point(m):\n    return m.group(1).replace(\".\", \" point \")\ndef _expand_dollars(m):",
        "detail": "klaam.klaam.external.FastSpeech2.text.numbers",
        "documentation": {}
    },
    {
        "label": "_pounds_re",
        "kind": 5,
        "importPath": "klaam.klaam.external.FastSpeech2.text.numbers",
        "description": "klaam.klaam.external.FastSpeech2.text.numbers",
        "peekOfCode": "_pounds_re = re.compile(r\"£([0-9\\,]*[0-9]+)\")\n_dollars_re = re.compile(r\"\\$([0-9\\.\\,]*[0-9]+)\")\n_ordinal_re = re.compile(r\"[0-9]+(st|nd|rd|th)\")\n_number_re = re.compile(r\"[0-9]+\")\ndef _remove_commas(m):\n    return m.group(1).replace(\",\", \"\")\ndef _expand_decimal_point(m):\n    return m.group(1).replace(\".\", \" point \")\ndef _expand_dollars(m):\n    match = m.group(1)",
        "detail": "klaam.klaam.external.FastSpeech2.text.numbers",
        "documentation": {}
    },
    {
        "label": "_dollars_re",
        "kind": 5,
        "importPath": "klaam.klaam.external.FastSpeech2.text.numbers",
        "description": "klaam.klaam.external.FastSpeech2.text.numbers",
        "peekOfCode": "_dollars_re = re.compile(r\"\\$([0-9\\.\\,]*[0-9]+)\")\n_ordinal_re = re.compile(r\"[0-9]+(st|nd|rd|th)\")\n_number_re = re.compile(r\"[0-9]+\")\ndef _remove_commas(m):\n    return m.group(1).replace(\",\", \"\")\ndef _expand_decimal_point(m):\n    return m.group(1).replace(\".\", \" point \")\ndef _expand_dollars(m):\n    match = m.group(1)\n    parts = match.split(\".\")",
        "detail": "klaam.klaam.external.FastSpeech2.text.numbers",
        "documentation": {}
    },
    {
        "label": "_ordinal_re",
        "kind": 5,
        "importPath": "klaam.klaam.external.FastSpeech2.text.numbers",
        "description": "klaam.klaam.external.FastSpeech2.text.numbers",
        "peekOfCode": "_ordinal_re = re.compile(r\"[0-9]+(st|nd|rd|th)\")\n_number_re = re.compile(r\"[0-9]+\")\ndef _remove_commas(m):\n    return m.group(1).replace(\",\", \"\")\ndef _expand_decimal_point(m):\n    return m.group(1).replace(\".\", \" point \")\ndef _expand_dollars(m):\n    match = m.group(1)\n    parts = match.split(\".\")\n    if len(parts) > 2:",
        "detail": "klaam.klaam.external.FastSpeech2.text.numbers",
        "documentation": {}
    },
    {
        "label": "_number_re",
        "kind": 5,
        "importPath": "klaam.klaam.external.FastSpeech2.text.numbers",
        "description": "klaam.klaam.external.FastSpeech2.text.numbers",
        "peekOfCode": "_number_re = re.compile(r\"[0-9]+\")\ndef _remove_commas(m):\n    return m.group(1).replace(\",\", \"\")\ndef _expand_decimal_point(m):\n    return m.group(1).replace(\".\", \" point \")\ndef _expand_dollars(m):\n    match = m.group(1)\n    parts = match.split(\".\")\n    if len(parts) > 2:\n        return match + \" dollars\"  # Unexpected format",
        "detail": "klaam.klaam.external.FastSpeech2.text.numbers",
        "documentation": {}
    },
    {
        "label": "initials",
        "kind": 5,
        "importPath": "klaam.klaam.external.FastSpeech2.text.pinyin",
        "description": "klaam.klaam.external.FastSpeech2.text.pinyin",
        "peekOfCode": "initials = [\n    \"b\",\n    \"c\",\n    \"ch\",\n    \"d\",\n    \"f\",\n    \"g\",\n    \"h\",\n    \"j\",\n    \"k\",",
        "detail": "klaam.klaam.external.FastSpeech2.text.pinyin",
        "documentation": {}
    },
    {
        "label": "finals",
        "kind": 5,
        "importPath": "klaam.klaam.external.FastSpeech2.text.pinyin",
        "description": "klaam.klaam.external.FastSpeech2.text.pinyin",
        "peekOfCode": "finals = [\n    \"a1\",\n    \"a2\",\n    \"a3\",\n    \"a4\",\n    \"a5\",\n    \"ai1\",\n    \"ai2\",\n    \"ai3\",\n    \"ai4\",",
        "detail": "klaam.klaam.external.FastSpeech2.text.pinyin",
        "documentation": {}
    },
    {
        "label": "valid_symbols",
        "kind": 5,
        "importPath": "klaam.klaam.external.FastSpeech2.text.pinyin",
        "description": "klaam.klaam.external.FastSpeech2.text.pinyin",
        "peekOfCode": "valid_symbols = initials + finals + [\"rr\"]",
        "detail": "klaam.klaam.external.FastSpeech2.text.pinyin",
        "documentation": {}
    },
    {
        "label": "_pad",
        "kind": 5,
        "importPath": "klaam.klaam.external.FastSpeech2.text.symbols",
        "description": "klaam.klaam.external.FastSpeech2.text.symbols",
        "peekOfCode": "_pad = \"_\"\n_punctuation = \"!'(),.:;? \"\n_special = \"-\"\n_letters = \"ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz\"\n_silences = [\"@sp\", \"@spn\", \"@sil\"]\n# Prepend \"@\" to ARPAbet symbols to ensure uniqueness (some are the same as uppercase letters):\n_arpabet = [\"@\" + s for s in cmudict.valid_symbols]\n_pinyin = [\"@\" + s for s in pinyin.valid_symbols]\nara = [\n    \"yy\",",
        "detail": "klaam.klaam.external.FastSpeech2.text.symbols",
        "documentation": {}
    },
    {
        "label": "_punctuation",
        "kind": 5,
        "importPath": "klaam.klaam.external.FastSpeech2.text.symbols",
        "description": "klaam.klaam.external.FastSpeech2.text.symbols",
        "peekOfCode": "_punctuation = \"!'(),.:;? \"\n_special = \"-\"\n_letters = \"ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz\"\n_silences = [\"@sp\", \"@spn\", \"@sil\"]\n# Prepend \"@\" to ARPAbet symbols to ensure uniqueness (some are the same as uppercase letters):\n_arpabet = [\"@\" + s for s in cmudict.valid_symbols]\n_pinyin = [\"@\" + s for s in pinyin.valid_symbols]\nara = [\n    \"yy\",\n    \"ii1'\",",
        "detail": "klaam.klaam.external.FastSpeech2.text.symbols",
        "documentation": {}
    },
    {
        "label": "_special",
        "kind": 5,
        "importPath": "klaam.klaam.external.FastSpeech2.text.symbols",
        "description": "klaam.klaam.external.FastSpeech2.text.symbols",
        "peekOfCode": "_special = \"-\"\n_letters = \"ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz\"\n_silences = [\"@sp\", \"@spn\", \"@sil\"]\n# Prepend \"@\" to ARPAbet symbols to ensure uniqueness (some are the same as uppercase letters):\n_arpabet = [\"@\" + s for s in cmudict.valid_symbols]\n_pinyin = [\"@\" + s for s in pinyin.valid_symbols]\nara = [\n    \"yy\",\n    \"ii1'\",\n    \"S\",",
        "detail": "klaam.klaam.external.FastSpeech2.text.symbols",
        "documentation": {}
    },
    {
        "label": "_letters",
        "kind": 5,
        "importPath": "klaam.klaam.external.FastSpeech2.text.symbols",
        "description": "klaam.klaam.external.FastSpeech2.text.symbols",
        "peekOfCode": "_letters = \"ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz\"\n_silences = [\"@sp\", \"@spn\", \"@sil\"]\n# Prepend \"@\" to ARPAbet symbols to ensure uniqueness (some are the same as uppercase letters):\n_arpabet = [\"@\" + s for s in cmudict.valid_symbols]\n_pinyin = [\"@\" + s for s in pinyin.valid_symbols]\nara = [\n    \"yy\",\n    \"ii1'\",\n    \"S\",\n    \"Z\",",
        "detail": "klaam.klaam.external.FastSpeech2.text.symbols",
        "documentation": {}
    },
    {
        "label": "_silences",
        "kind": 5,
        "importPath": "klaam.klaam.external.FastSpeech2.text.symbols",
        "description": "klaam.klaam.external.FastSpeech2.text.symbols",
        "peekOfCode": "_silences = [\"@sp\", \"@spn\", \"@sil\"]\n# Prepend \"@\" to ARPAbet symbols to ensure uniqueness (some are the same as uppercase letters):\n_arpabet = [\"@\" + s for s in cmudict.valid_symbols]\n_pinyin = [\"@\" + s for s in pinyin.valid_symbols]\nara = [\n    \"yy\",\n    \"ii1'\",\n    \"S\",\n    \"Z\",\n    \"I0'\",",
        "detail": "klaam.klaam.external.FastSpeech2.text.symbols",
        "documentation": {}
    },
    {
        "label": "_arpabet",
        "kind": 5,
        "importPath": "klaam.klaam.external.FastSpeech2.text.symbols",
        "description": "klaam.klaam.external.FastSpeech2.text.symbols",
        "peekOfCode": "_arpabet = [\"@\" + s for s in cmudict.valid_symbols]\n_pinyin = [\"@\" + s for s in pinyin.valid_symbols]\nara = [\n    \"yy\",\n    \"ii1'\",\n    \"S\",\n    \"Z\",\n    \"I0'\",\n    \"ii0\",\n    \"p\",",
        "detail": "klaam.klaam.external.FastSpeech2.text.symbols",
        "documentation": {}
    },
    {
        "label": "_pinyin",
        "kind": 5,
        "importPath": "klaam.klaam.external.FastSpeech2.text.symbols",
        "description": "klaam.klaam.external.FastSpeech2.text.symbols",
        "peekOfCode": "_pinyin = [\"@\" + s for s in pinyin.valid_symbols]\nara = [\n    \"yy\",\n    \"ii1'\",\n    \"S\",\n    \"Z\",\n    \"I0'\",\n    \"ii0\",\n    \"p\",\n    \"i0\",",
        "detail": "klaam.klaam.external.FastSpeech2.text.symbols",
        "documentation": {}
    },
    {
        "label": "ara",
        "kind": 5,
        "importPath": "klaam.klaam.external.FastSpeech2.text.symbols",
        "description": "klaam.klaam.external.FastSpeech2.text.symbols",
        "peekOfCode": "ara = [\n    \"yy\",\n    \"ii1'\",\n    \"S\",\n    \"Z\",\n    \"I0'\",\n    \"ii0\",\n    \"p\",\n    \"i0\",\n    \"hh\",",
        "detail": "klaam.klaam.external.FastSpeech2.text.symbols",
        "documentation": {}
    },
    {
        "label": "_ara",
        "kind": 5,
        "importPath": "klaam.klaam.external.FastSpeech2.text.symbols",
        "description": "klaam.klaam.external.FastSpeech2.text.symbols",
        "peekOfCode": "_ara = [\"@\" + s for s in ara]\n# Export all symbols:\n# TO_DO\nsymbols = [_pad] + list(_special) + list(_punctuation) + list(_letters) + _arpabet + _pinyin + _silences + _ara",
        "detail": "klaam.klaam.external.FastSpeech2.text.symbols",
        "documentation": {}
    },
    {
        "label": "symbols",
        "kind": 5,
        "importPath": "klaam.klaam.external.FastSpeech2.text.symbols",
        "description": "klaam.klaam.external.FastSpeech2.text.symbols",
        "peekOfCode": "symbols = [_pad] + list(_special) + list(_punctuation) + list(_letters) + _arpabet + _pinyin + _silences + _ara",
        "detail": "klaam.klaam.external.FastSpeech2.text.symbols",
        "documentation": {}
    },
    {
        "label": "PAD",
        "kind": 5,
        "importPath": "klaam.klaam.external.FastSpeech2.transformer.Constants",
        "description": "klaam.klaam.external.FastSpeech2.transformer.Constants",
        "peekOfCode": "PAD = 0\nUNK = 1\nBOS = 2\nEOS = 3\nPAD_WORD = \"<blank>\"\nUNK_WORD = \"<unk>\"\nBOS_WORD = \"<s>\"\nEOS_WORD = \"</s>\"",
        "detail": "klaam.klaam.external.FastSpeech2.transformer.Constants",
        "documentation": {}
    },
    {
        "label": "UNK",
        "kind": 5,
        "importPath": "klaam.klaam.external.FastSpeech2.transformer.Constants",
        "description": "klaam.klaam.external.FastSpeech2.transformer.Constants",
        "peekOfCode": "UNK = 1\nBOS = 2\nEOS = 3\nPAD_WORD = \"<blank>\"\nUNK_WORD = \"<unk>\"\nBOS_WORD = \"<s>\"\nEOS_WORD = \"</s>\"",
        "detail": "klaam.klaam.external.FastSpeech2.transformer.Constants",
        "documentation": {}
    },
    {
        "label": "BOS",
        "kind": 5,
        "importPath": "klaam.klaam.external.FastSpeech2.transformer.Constants",
        "description": "klaam.klaam.external.FastSpeech2.transformer.Constants",
        "peekOfCode": "BOS = 2\nEOS = 3\nPAD_WORD = \"<blank>\"\nUNK_WORD = \"<unk>\"\nBOS_WORD = \"<s>\"\nEOS_WORD = \"</s>\"",
        "detail": "klaam.klaam.external.FastSpeech2.transformer.Constants",
        "documentation": {}
    },
    {
        "label": "EOS",
        "kind": 5,
        "importPath": "klaam.klaam.external.FastSpeech2.transformer.Constants",
        "description": "klaam.klaam.external.FastSpeech2.transformer.Constants",
        "peekOfCode": "EOS = 3\nPAD_WORD = \"<blank>\"\nUNK_WORD = \"<unk>\"\nBOS_WORD = \"<s>\"\nEOS_WORD = \"</s>\"",
        "detail": "klaam.klaam.external.FastSpeech2.transformer.Constants",
        "documentation": {}
    },
    {
        "label": "PAD_WORD",
        "kind": 5,
        "importPath": "klaam.klaam.external.FastSpeech2.transformer.Constants",
        "description": "klaam.klaam.external.FastSpeech2.transformer.Constants",
        "peekOfCode": "PAD_WORD = \"<blank>\"\nUNK_WORD = \"<unk>\"\nBOS_WORD = \"<s>\"\nEOS_WORD = \"</s>\"",
        "detail": "klaam.klaam.external.FastSpeech2.transformer.Constants",
        "documentation": {}
    },
    {
        "label": "UNK_WORD",
        "kind": 5,
        "importPath": "klaam.klaam.external.FastSpeech2.transformer.Constants",
        "description": "klaam.klaam.external.FastSpeech2.transformer.Constants",
        "peekOfCode": "UNK_WORD = \"<unk>\"\nBOS_WORD = \"<s>\"\nEOS_WORD = \"</s>\"",
        "detail": "klaam.klaam.external.FastSpeech2.transformer.Constants",
        "documentation": {}
    },
    {
        "label": "BOS_WORD",
        "kind": 5,
        "importPath": "klaam.klaam.external.FastSpeech2.transformer.Constants",
        "description": "klaam.klaam.external.FastSpeech2.transformer.Constants",
        "peekOfCode": "BOS_WORD = \"<s>\"\nEOS_WORD = \"</s>\"",
        "detail": "klaam.klaam.external.FastSpeech2.transformer.Constants",
        "documentation": {}
    },
    {
        "label": "EOS_WORD",
        "kind": 5,
        "importPath": "klaam.klaam.external.FastSpeech2.transformer.Constants",
        "description": "klaam.klaam.external.FastSpeech2.transformer.Constants",
        "peekOfCode": "EOS_WORD = \"</s>\"",
        "detail": "klaam.klaam.external.FastSpeech2.transformer.Constants",
        "documentation": {}
    },
    {
        "label": "FFTBlock",
        "kind": 6,
        "importPath": "klaam.klaam.external.FastSpeech2.transformer.Layers",
        "description": "klaam.klaam.external.FastSpeech2.transformer.Layers",
        "peekOfCode": "class FFTBlock(torch.nn.Module):\n    \"\"\"FFT Block\"\"\"\n    def __init__(self, d_model, n_head, d_k, d_v, d_inner, kernel_size, dropout=0.1):\n        super(FFTBlock, self).__init__()\n        self.slf_attn = MultiHeadAttention(n_head, d_model, d_k, d_v, dropout=dropout)\n        self.pos_ffn = PositionwiseFeedForward(d_model, d_inner, kernel_size, dropout=dropout)\n    def forward(self, enc_input, mask=None, slf_attn_mask=None):\n        enc_output, enc_slf_attn = self.slf_attn(enc_input, enc_input, enc_input, mask=slf_attn_mask)\n        enc_output = enc_output.masked_fill(mask.unsqueeze(-1), 0)\n        enc_output = self.pos_ffn(enc_output)",
        "detail": "klaam.klaam.external.FastSpeech2.transformer.Layers",
        "documentation": {}
    },
    {
        "label": "ConvNorm",
        "kind": 6,
        "importPath": "klaam.klaam.external.FastSpeech2.transformer.Layers",
        "description": "klaam.klaam.external.FastSpeech2.transformer.Layers",
        "peekOfCode": "class ConvNorm(torch.nn.Module):\n    def __init__(\n        self,\n        in_channels,\n        out_channels,\n        kernel_size=1,\n        stride=1,\n        padding=None,\n        dilation=1,\n        bias=True,",
        "detail": "klaam.klaam.external.FastSpeech2.transformer.Layers",
        "documentation": {}
    },
    {
        "label": "PostNet",
        "kind": 6,
        "importPath": "klaam.klaam.external.FastSpeech2.transformer.Layers",
        "description": "klaam.klaam.external.FastSpeech2.transformer.Layers",
        "peekOfCode": "class PostNet(nn.Module):\n    \"\"\"\n    PostNet: Five 1-d convolution with 512 channels and kernel size 5\n    \"\"\"\n    def __init__(\n        self,\n        n_mel_channels=80,\n        postnet_embedding_dim=512,\n        postnet_kernel_size=5,\n        postnet_n_convolutions=5,",
        "detail": "klaam.klaam.external.FastSpeech2.transformer.Layers",
        "documentation": {}
    },
    {
        "label": "Encoder",
        "kind": 6,
        "importPath": "klaam.klaam.external.FastSpeech2.transformer.Models",
        "description": "klaam.klaam.external.FastSpeech2.transformer.Models",
        "peekOfCode": "class Encoder(nn.Module):\n    \"\"\"Encoder\"\"\"\n    def __init__(self, config):\n        super(Encoder, self).__init__()\n        n_position = config[\"max_seq_len\"] + 1\n        n_src_vocab = len(symbols) + 1\n        d_word_vec = config[\"transformer\"][\"encoder_hidden\"]\n        n_layers = config[\"transformer\"][\"encoder_layer\"]\n        n_head = config[\"transformer\"][\"encoder_head\"]\n        d_k = d_v = config[\"transformer\"][\"encoder_hidden\"] // config[\"transformer\"][\"encoder_head\"]",
        "detail": "klaam.klaam.external.FastSpeech2.transformer.Models",
        "documentation": {}
    },
    {
        "label": "Decoder",
        "kind": 6,
        "importPath": "klaam.klaam.external.FastSpeech2.transformer.Models",
        "description": "klaam.klaam.external.FastSpeech2.transformer.Models",
        "peekOfCode": "class Decoder(nn.Module):\n    \"\"\"Decoder\"\"\"\n    def __init__(self, config):\n        super(Decoder, self).__init__()\n        n_position = config[\"max_seq_len\"] + 1\n        d_word_vec = config[\"transformer\"][\"decoder_hidden\"]\n        n_layers = config[\"transformer\"][\"decoder_layer\"]\n        n_head = config[\"transformer\"][\"decoder_head\"]\n        d_k = d_v = config[\"transformer\"][\"decoder_hidden\"] // config[\"transformer\"][\"decoder_head\"]\n        d_model = config[\"transformer\"][\"decoder_hidden\"]",
        "detail": "klaam.klaam.external.FastSpeech2.transformer.Models",
        "documentation": {}
    },
    {
        "label": "get_sinusoid_encoding_table",
        "kind": 2,
        "importPath": "klaam.klaam.external.FastSpeech2.transformer.Models",
        "description": "klaam.klaam.external.FastSpeech2.transformer.Models",
        "peekOfCode": "def get_sinusoid_encoding_table(n_position, d_hid, padding_idx=None):\n    \"\"\"Sinusoid position encoding table\"\"\"\n    def cal_angle(position, hid_idx):\n        return position / np.power(10000, 2 * (hid_idx // 2) / d_hid)\n    def get_posi_angle_vec(position):\n        return [cal_angle(position, hid_j) for hid_j in range(d_hid)]\n    sinusoid_table = np.array([get_posi_angle_vec(pos_i) for pos_i in range(n_position)])\n    sinusoid_table[:, 0::2] = np.sin(sinusoid_table[:, 0::2])  # dim 2i\n    sinusoid_table[:, 1::2] = np.cos(sinusoid_table[:, 1::2])  # dim 2i+1\n    if padding_idx is not None:",
        "detail": "klaam.klaam.external.FastSpeech2.transformer.Models",
        "documentation": {}
    },
    {
        "label": "ScaledDotProductAttention",
        "kind": 6,
        "importPath": "klaam.klaam.external.FastSpeech2.transformer.Modules",
        "description": "klaam.klaam.external.FastSpeech2.transformer.Modules",
        "peekOfCode": "class ScaledDotProductAttention(nn.Module):\n    \"\"\"Scaled Dot-Product Attention\"\"\"\n    def __init__(self, temperature):\n        super().__init__()\n        self.temperature = temperature\n        self.softmax = nn.Softmax(dim=2)\n    def forward(self, q, k, v, mask=None):\n        attn = torch.bmm(q, k.transpose(1, 2))\n        attn = attn / self.temperature\n        if mask is not None:",
        "detail": "klaam.klaam.external.FastSpeech2.transformer.Modules",
        "documentation": {}
    },
    {
        "label": "MultiHeadAttention",
        "kind": 6,
        "importPath": "klaam.klaam.external.FastSpeech2.transformer.SubLayers",
        "description": "klaam.klaam.external.FastSpeech2.transformer.SubLayers",
        "peekOfCode": "class MultiHeadAttention(nn.Module):\n    \"\"\"Multi-Head Attention module\"\"\"\n    def __init__(self, n_head, d_model, d_k, d_v, dropout=0.1):\n        super().__init__()\n        self.n_head = n_head\n        self.d_k = d_k\n        self.d_v = d_v\n        self.w_qs = nn.Linear(d_model, n_head * d_k)\n        self.w_ks = nn.Linear(d_model, n_head * d_k)\n        self.w_vs = nn.Linear(d_model, n_head * d_v)",
        "detail": "klaam.klaam.external.FastSpeech2.transformer.SubLayers",
        "documentation": {}
    },
    {
        "label": "PositionwiseFeedForward",
        "kind": 6,
        "importPath": "klaam.klaam.external.FastSpeech2.transformer.SubLayers",
        "description": "klaam.klaam.external.FastSpeech2.transformer.SubLayers",
        "peekOfCode": "class PositionwiseFeedForward(nn.Module):\n    \"\"\"A two-feed-forward-layer module\"\"\"\n    def __init__(self, d_in, d_hid, kernel_size, dropout=0.1):\n        super().__init__()\n        # Use Conv1D\n        # position-wise\n        self.w_1 = nn.Conv1d(\n            d_in,\n            d_hid,\n            kernel_size=kernel_size[0],",
        "detail": "klaam.klaam.external.FastSpeech2.transformer.SubLayers",
        "documentation": {}
    },
    {
        "label": "get_model",
        "kind": 2,
        "importPath": "klaam.klaam.external.FastSpeech2.utils.model",
        "description": "klaam.klaam.external.FastSpeech2.utils.model",
        "peekOfCode": "def get_model(args, configs, device, train=False):\n    (preprocess_config, model_config, train_config) = configs\n    model = FastSpeech2(preprocess_config, model_config).to(device)\n    if args.restore_step:\n        ckpt_path = os.path.join(\n            train_config[\"path\"][\"ckpt_path\"],\n            \"{}.pth.tar\".format(args.restore_step),\n        )\n        ckpt = torch.load(ckpt_path)\n        model.load_state_dict(ckpt[\"model\"])",
        "detail": "klaam.klaam.external.FastSpeech2.utils.model",
        "documentation": {}
    },
    {
        "label": "get_model_inference",
        "kind": 2,
        "importPath": "klaam.klaam.external.FastSpeech2.utils.model",
        "description": "klaam.klaam.external.FastSpeech2.utils.model",
        "peekOfCode": "def get_model_inference(configs, device, train=False):\n    (preprocess_config, model_config, train_config) = configs\n    model = FastSpeech2(preprocess_config, model_config).to(device)\n    url = \"https://drive.google.com/uc?id=1p0g28WlbHftoatXBz87GL146y5vkoORd\"\n    ckpt_path = \"model.pth.tar\"\n    if not os.path.exists(ckpt_path):\n        gdown.download(url, ckpt_path, quiet=False)\n    ckpt = torch.load(ckpt_path, map_location=torch.device(\"cpu\"))\n    model.load_state_dict(ckpt[\"model\"])\n    model.eval()",
        "detail": "klaam.klaam.external.FastSpeech2.utils.model",
        "documentation": {}
    },
    {
        "label": "get_param_num",
        "kind": 2,
        "importPath": "klaam.klaam.external.FastSpeech2.utils.model",
        "description": "klaam.klaam.external.FastSpeech2.utils.model",
        "peekOfCode": "def get_param_num(model):\n    num_param = sum(param.numel() for param in model.parameters())\n    return num_param\ndef get_vocoder(config, device, vocoder_config_path=None, speaker_pre_trained_path=None):\n    name = config[\"vocoder\"][\"model\"]\n    speaker = config[\"vocoder\"][\"speaker\"]\n    if name == \"MelGAN\":\n        if speaker == \"LJSpeech\":\n            vocoder = torch.hub.load(\"descriptinc/melgan-neurips\", \"load_melgan\", \"linda_johnson\")\n        elif speaker == \"universal\":",
        "detail": "klaam.klaam.external.FastSpeech2.utils.model",
        "documentation": {}
    },
    {
        "label": "get_vocoder",
        "kind": 2,
        "importPath": "klaam.klaam.external.FastSpeech2.utils.model",
        "description": "klaam.klaam.external.FastSpeech2.utils.model",
        "peekOfCode": "def get_vocoder(config, device, vocoder_config_path=None, speaker_pre_trained_path=None):\n    name = config[\"vocoder\"][\"model\"]\n    speaker = config[\"vocoder\"][\"speaker\"]\n    if name == \"MelGAN\":\n        if speaker == \"LJSpeech\":\n            vocoder = torch.hub.load(\"descriptinc/melgan-neurips\", \"load_melgan\", \"linda_johnson\")\n        elif speaker == \"universal\":\n            vocoder = torch.hub.load(\"descriptinc/melgan-neurips\", \"load_melgan\", \"multi_speaker\")\n        vocoder.mel2wav.eval()\n        vocoder.mel2wav.to(device)",
        "detail": "klaam.klaam.external.FastSpeech2.utils.model",
        "documentation": {}
    },
    {
        "label": "vocoder_infer",
        "kind": 2,
        "importPath": "klaam.klaam.external.FastSpeech2.utils.model",
        "description": "klaam.klaam.external.FastSpeech2.utils.model",
        "peekOfCode": "def vocoder_infer(mels, vocoder, model_config, preprocess_config, lengths=None):\n    name = model_config[\"vocoder\"][\"model\"]\n    with torch.no_grad():\n        if name == \"MelGAN\":\n            wavs = vocoder.inverse(mels / np.log(10))\n        elif name == \"HiFi-GAN\":\n            wavs = vocoder(mels).squeeze(1)\n    wavs = (wavs.cpu().numpy() * preprocess_config[\"preprocessing\"][\"audio\"][\"max_wav_value\"]).astype(\"int16\")\n    wavs = [wav for wav in wavs]\n    for i in range(len(mels)):",
        "detail": "klaam.klaam.external.FastSpeech2.utils.model",
        "documentation": {}
    },
    {
        "label": "to_device",
        "kind": 2,
        "importPath": "klaam.klaam.external.FastSpeech2.utils.tools",
        "description": "klaam.klaam.external.FastSpeech2.utils.tools",
        "peekOfCode": "def to_device(data, device):\n    if len(data) == 12:\n        (\n            ids,\n            raw_texts,\n            speakers,\n            texts,\n            src_lens,\n            max_src_len,\n            mels,",
        "detail": "klaam.klaam.external.FastSpeech2.utils.tools",
        "documentation": {}
    },
    {
        "label": "log",
        "kind": 2,
        "importPath": "klaam.klaam.external.FastSpeech2.utils.tools",
        "description": "klaam.klaam.external.FastSpeech2.utils.tools",
        "peekOfCode": "def log(logger, step=None, losses=None, fig=None, audio=None, sampling_rate=22050, tag=\"\"):\n    if losses is not None:\n        logger.add_scalar(\"Loss/total_loss\", losses[0], step)\n        logger.add_scalar(\"Loss/mel_loss\", losses[1], step)\n        logger.add_scalar(\"Loss/mel_postnet_loss\", losses[2], step)\n        logger.add_scalar(\"Loss/pitch_loss\", losses[3], step)\n        logger.add_scalar(\"Loss/energy_loss\", losses[4], step)\n        logger.add_scalar(\"Loss/duration_loss\", losses[5], step)\n    if fig is not None:\n        logger.add_figure(tag, fig)",
        "detail": "klaam.klaam.external.FastSpeech2.utils.tools",
        "documentation": {}
    },
    {
        "label": "get_mask_from_lengths",
        "kind": 2,
        "importPath": "klaam.klaam.external.FastSpeech2.utils.tools",
        "description": "klaam.klaam.external.FastSpeech2.utils.tools",
        "peekOfCode": "def get_mask_from_lengths(lengths, max_len=None):\n    batch_size = lengths.shape[0]\n    if max_len is None:\n        max_len = torch.max(lengths).item()\n    ids = torch.arange(0, max_len).unsqueeze(0).expand(batch_size, -1).to(device)\n    mask = ids >= lengths.unsqueeze(1).expand(-1, max_len)\n    return mask\ndef expand(values, durations):\n    out = []\n    for value, d in zip(values, durations):",
        "detail": "klaam.klaam.external.FastSpeech2.utils.tools",
        "documentation": {}
    },
    {
        "label": "expand",
        "kind": 2,
        "importPath": "klaam.klaam.external.FastSpeech2.utils.tools",
        "description": "klaam.klaam.external.FastSpeech2.utils.tools",
        "peekOfCode": "def expand(values, durations):\n    out = []\n    for value, d in zip(values, durations):\n        out += [value] * max(0, int(d))\n    return np.array(out)\ndef synth_one_sample(targets, predictions, vocoder, model_config, preprocess_config):\n    basename = targets[0][0]\n    src_len = predictions[8][0].item()\n    mel_len = predictions[9][0].item()\n    mel_target = targets[6][0, :mel_len].detach().transpose(0, 1)",
        "detail": "klaam.klaam.external.FastSpeech2.utils.tools",
        "documentation": {}
    },
    {
        "label": "synth_one_sample",
        "kind": 2,
        "importPath": "klaam.klaam.external.FastSpeech2.utils.tools",
        "description": "klaam.klaam.external.FastSpeech2.utils.tools",
        "peekOfCode": "def synth_one_sample(targets, predictions, vocoder, model_config, preprocess_config):\n    basename = targets[0][0]\n    src_len = predictions[8][0].item()\n    mel_len = predictions[9][0].item()\n    mel_target = targets[6][0, :mel_len].detach().transpose(0, 1)\n    mel_prediction = predictions[1][0, :mel_len].detach().transpose(0, 1)\n    duration = targets[11][0, :src_len].detach().cpu().numpy()\n    if preprocess_config[\"preprocessing\"][\"pitch\"][\"feature\"] == \"phoneme_level\":\n        pitch = targets[9][0, :src_len].detach().cpu().numpy()\n        pitch = expand(pitch, duration)",
        "detail": "klaam.klaam.external.FastSpeech2.utils.tools",
        "documentation": {}
    },
    {
        "label": "synth_samples",
        "kind": 2,
        "importPath": "klaam.klaam.external.FastSpeech2.utils.tools",
        "description": "klaam.klaam.external.FastSpeech2.utils.tools",
        "peekOfCode": "def synth_samples(targets, predictions, vocoder, model_config, preprocess_config, path):\n    basenames = targets[0]\n    for i in range(len(predictions[0])):\n        basename = basenames[i]\n        src_len = predictions[8][i].item()\n        mel_len = predictions[9][i].item()\n        mel_prediction = predictions[1][i, :mel_len].detach().transpose(0, 1)\n        duration = predictions[5][i, :src_len].detach().cpu().numpy()\n        if preprocess_config[\"preprocessing\"][\"pitch\"][\"feature\"] == \"phoneme_level\":\n            pitch = predictions[2][i, :src_len].detach().cpu().numpy()",
        "detail": "klaam.klaam.external.FastSpeech2.utils.tools",
        "documentation": {}
    },
    {
        "label": "plot_mel",
        "kind": 2,
        "importPath": "klaam.klaam.external.FastSpeech2.utils.tools",
        "description": "klaam.klaam.external.FastSpeech2.utils.tools",
        "peekOfCode": "def plot_mel(data, stats, titles):\n    fig, axes = plt.subplots(len(data), 1, squeeze=False)\n    if titles is None:\n        titles = [None for i in range(len(data))]\n    pitch_min, pitch_max, pitch_mean, pitch_std, energy_min, energy_max = stats\n    pitch_min = pitch_min * pitch_std + pitch_mean\n    pitch_max = pitch_max * pitch_std + pitch_mean\n    def add_axis(fig, old_ax):\n        ax = fig.add_axes(old_ax.get_position(), anchor=\"W\")\n        ax.set_facecolor(\"None\")",
        "detail": "klaam.klaam.external.FastSpeech2.utils.tools",
        "documentation": {}
    },
    {
        "label": "pad_1D",
        "kind": 2,
        "importPath": "klaam.klaam.external.FastSpeech2.utils.tools",
        "description": "klaam.klaam.external.FastSpeech2.utils.tools",
        "peekOfCode": "def pad_1D(inputs, PAD=0):\n    def pad_data(x, length, PAD):\n        x_padded = np.pad(x, (0, length - x.shape[0]), mode=\"constant\", constant_values=PAD)\n        return x_padded\n    max_len = max((len(x) for x in inputs))\n    padded = np.stack([pad_data(x, max_len, PAD) for x in inputs])\n    return padded\ndef pad_2D(inputs, maxlen=None):\n    def pad(x, max_len):\n        PAD = 0",
        "detail": "klaam.klaam.external.FastSpeech2.utils.tools",
        "documentation": {}
    },
    {
        "label": "pad_2D",
        "kind": 2,
        "importPath": "klaam.klaam.external.FastSpeech2.utils.tools",
        "description": "klaam.klaam.external.FastSpeech2.utils.tools",
        "peekOfCode": "def pad_2D(inputs, maxlen=None):\n    def pad(x, max_len):\n        PAD = 0\n        if np.shape(x)[0] > max_len:\n            raise ValueError(\"not max_len\")\n        s = np.shape(x)[1]\n        x_padded = np.pad(x, (0, max_len - np.shape(x)[0]), mode=\"constant\", constant_values=PAD)\n        return x_padded[:, :s]\n    if maxlen:\n        output = np.stack([pad(x, maxlen) for x in inputs])",
        "detail": "klaam.klaam.external.FastSpeech2.utils.tools",
        "documentation": {}
    },
    {
        "label": "pad",
        "kind": 2,
        "importPath": "klaam.klaam.external.FastSpeech2.utils.tools",
        "description": "klaam.klaam.external.FastSpeech2.utils.tools",
        "peekOfCode": "def pad(input_ele, mel_max_length=None):\n    if mel_max_length:\n        max_len = mel_max_length\n    else:\n        max_len = max([input_ele[i].size(0) for i in range(len(input_ele))])\n    out_list = []\n    for i, batch in enumerate(input_ele):\n        if len(batch.shape) == 1:\n            one_batch_padded = F.pad(batch, (0, max_len - batch.size(0)), \"constant\", 0.0)\n        elif len(batch.shape) == 2:",
        "detail": "klaam.klaam.external.FastSpeech2.utils.tools",
        "documentation": {}
    },
    {
        "label": "device",
        "kind": 5,
        "importPath": "klaam.klaam.external.FastSpeech2.utils.tools",
        "description": "klaam.klaam.external.FastSpeech2.utils.tools",
        "peekOfCode": "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\ndef to_device(data, device):\n    if len(data) == 12:\n        (\n            ids,\n            raw_texts,\n            speakers,\n            texts,\n            src_lens,\n            max_src_len,",
        "detail": "klaam.klaam.external.FastSpeech2.utils.tools",
        "documentation": {}
    },
    {
        "label": "ar2bw",
        "kind": 5,
        "importPath": "klaam.klaam.external.FastSpeech2.buckwalter",
        "description": "klaam.klaam.external.FastSpeech2.buckwalter",
        "peekOfCode": "ar2bw = {\n    \"\\u0628\": \"b\",\n    \"\\u0630\": \"*\",\n    \"\\u0637\": \"T\",\n    \"\\u0645\": \"m\",\n    \"\\u062a\": \"t\",\n    \"\\u0631\": \"r\",\n    \"\\u0638\": \"Z\",\n    \"\\u0646\": \"n\",\n    \"\\u062b\": \"^\",",
        "detail": "klaam.klaam.external.FastSpeech2.buckwalter",
        "documentation": {}
    },
    {
        "label": "bw2ar",
        "kind": 5,
        "importPath": "klaam.klaam.external.FastSpeech2.buckwalter",
        "description": "klaam.klaam.external.FastSpeech2.buckwalter",
        "peekOfCode": "bw2ar = {ar2bw[k]: k for k in ar2bw}",
        "detail": "klaam.klaam.external.FastSpeech2.buckwalter",
        "documentation": {}
    },
    {
        "label": "Dataset",
        "kind": 6,
        "importPath": "klaam.klaam.external.FastSpeech2.dataset",
        "description": "klaam.klaam.external.FastSpeech2.dataset",
        "peekOfCode": "class Dataset(Dataset):\n    def __init__(self, filename, preprocess_config, train_config, sort=False, drop_last=False):\n        self.dataset_name = preprocess_config[\"dataset\"]\n        self.preprocessed_path = preprocess_config[\"path\"][\"preprocessed_path\"]\n        self.cleaners = preprocess_config[\"preprocessing\"][\"text\"][\"text_cleaners\"]\n        self.batch_size = train_config[\"optimizer\"][\"batch_size\"]\n        self.basename, self.speaker, self.text, self.raw_text = self.process_meta(filename)\n        with open(os.path.join(self.preprocessed_path, \"speakers.json\")) as f:\n            self.speaker_map = json.load(f)\n        self.sort = sort",
        "detail": "klaam.klaam.external.FastSpeech2.dataset",
        "documentation": {}
    },
    {
        "label": "TextDataset",
        "kind": 6,
        "importPath": "klaam.klaam.external.FastSpeech2.dataset",
        "description": "klaam.klaam.external.FastSpeech2.dataset",
        "peekOfCode": "class TextDataset(Dataset):\n    def __init__(self, filepath, preprocess_config):\n        self.cleaners = preprocess_config[\"preprocessing\"][\"text\"][\"text_cleaners\"]\n        self.basename, self.speaker, self.text, self.raw_text = self.process_meta(filepath)\n        with open(os.path.join(preprocess_config[\"path\"][\"preprocessed_path\"], \"speakers.json\")) as f:\n            self.speaker_map = json.load(f)\n    def __len__(self):\n        return len(self.text)\n    def __getitem__(self, idx):\n        basename = self.basename[idx]",
        "detail": "klaam.klaam.external.FastSpeech2.dataset",
        "documentation": {}
    },
    {
        "label": "evaluate",
        "kind": 2,
        "importPath": "klaam.klaam.external.FastSpeech2.evaluate",
        "description": "klaam.klaam.external.FastSpeech2.evaluate",
        "peekOfCode": "def evaluate(model, step, configs, logger=None, vocoder=None):\n    preprocess_config, model_config, train_config = configs\n    # Get dataset\n    dataset = Dataset(\"val.txt\", preprocess_config, train_config, sort=False, drop_last=False)\n    batch_size = train_config[\"optimizer\"][\"batch_size\"]\n    loader = DataLoader(\n        dataset,\n        batch_size=batch_size,\n        shuffle=False,\n        collate_fn=dataset.collate_fn,",
        "detail": "klaam.klaam.external.FastSpeech2.evaluate",
        "documentation": {}
    },
    {
        "label": "device",
        "kind": 5,
        "importPath": "klaam.klaam.external.FastSpeech2.evaluate",
        "description": "klaam.klaam.external.FastSpeech2.evaluate",
        "peekOfCode": "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\ndef evaluate(model, step, configs, logger=None, vocoder=None):\n    preprocess_config, model_config, train_config = configs\n    # Get dataset\n    dataset = Dataset(\"val.txt\", preprocess_config, train_config, sort=False, drop_last=False)\n    batch_size = train_config[\"optimizer\"][\"batch_size\"]\n    loader = DataLoader(\n        dataset,\n        batch_size=batch_size,\n        shuffle=False,",
        "detail": "klaam.klaam.external.FastSpeech2.evaluate",
        "documentation": {}
    },
    {
        "label": "preprocess_arabic",
        "kind": 2,
        "importPath": "klaam.klaam.external.FastSpeech2.inference",
        "description": "klaam.klaam.external.FastSpeech2.inference",
        "peekOfCode": "def preprocess_arabic(text, preprocess_config, bw=False, ts=False):\n    if bw:\n        text = \"\".join([bw2ar[l] if l in bw2ar else l for l in text])\n    if ts:\n        vocalizer = mishkal.tashkeel.TashkeelClass()\n        text = vocalizer.tashkeel(text).strip()\n    phones = phonetise(text)[0]\n    phones = \"{\" + phones + \"}\"\n    print(\"Raw Text Sequence: {}\".format(text))\n    print(\"Phoneme Sequence: {}\".format(phones))",
        "detail": "klaam.klaam.external.FastSpeech2.inference",
        "documentation": {}
    },
    {
        "label": "synthesize",
        "kind": 2,
        "importPath": "klaam.klaam.external.FastSpeech2.inference",
        "description": "klaam.klaam.external.FastSpeech2.inference",
        "peekOfCode": "def synthesize(model, step, configs, vocoder, batchs, control_values):\n    preprocess_config, model_config, train_config = configs\n    pitch_control, energy_control, duration_control = control_values\n    for batch in batchs:\n        batch = to_device(batch, DEVICE)\n        with torch.no_grad():\n            # Forward\n            output = model(*(batch[2:]), p_control=pitch_control, e_control=energy_control, d_control=duration_control)\n            synth_samples(\n                batch,",
        "detail": "klaam.klaam.external.FastSpeech2.inference",
        "documentation": {}
    },
    {
        "label": "prepare_tts_model",
        "kind": 2,
        "importPath": "klaam.klaam.external.FastSpeech2.inference",
        "description": "klaam.klaam.external.FastSpeech2.inference",
        "peekOfCode": "def prepare_tts_model(configs, vocoder_config_path, speaker_pre_trained_path):\n    model_config = configs[1]\n    # Get model\n    model = get_model_inference(configs, DEVICE, train=False)\n    # Load vocoder\n    vocoder = get_vocoder(model_config, DEVICE, vocoder_config_path, speaker_pre_trained_path)\n    return model, vocoder, configs\ndef infer_tts(\n    text,\n    model,",
        "detail": "klaam.klaam.external.FastSpeech2.inference",
        "documentation": {}
    },
    {
        "label": "infer_tts",
        "kind": 2,
        "importPath": "klaam.klaam.external.FastSpeech2.inference",
        "description": "klaam.klaam.external.FastSpeech2.inference",
        "peekOfCode": "def infer_tts(\n    text,\n    model,\n    vocoder,\n    configs,\n    bw=True,\n    apply_tshkeel=False,\n    pitch_control=1.0,\n    energy_control=1.0,\n    duration_control=1.0,",
        "detail": "klaam.klaam.external.FastSpeech2.inference",
        "documentation": {}
    },
    {
        "label": "DEVICE",
        "kind": 5,
        "importPath": "klaam.klaam.external.FastSpeech2.inference",
        "description": "klaam.klaam.external.FastSpeech2.inference",
        "peekOfCode": "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\ndef preprocess_arabic(text, preprocess_config, bw=False, ts=False):\n    if bw:\n        text = \"\".join([bw2ar[l] if l in bw2ar else l for l in text])\n    if ts:\n        vocalizer = mishkal.tashkeel.TashkeelClass()\n        text = vocalizer.tashkeel(text).strip()\n    phones = phonetise(text)[0]\n    phones = \"{\" + phones + \"}\"\n    print(\"Raw Text Sequence: {}\".format(text))",
        "detail": "klaam.klaam.external.FastSpeech2.inference",
        "documentation": {}
    },
    {
        "label": "main",
        "kind": 2,
        "importPath": "klaam.klaam.external.FastSpeech2.prepare_align",
        "description": "klaam.klaam.external.FastSpeech2.prepare_align",
        "peekOfCode": "def main(config):\n    if \"LJSpeech\" in config[\"dataset\"]:\n        ljspeech.prepare_align(config)\n    if \"AISHELL3\" in config[\"dataset\"]:\n        aishell3.prepare_align(config)\n    if \"LibriTTS\" in config[\"dataset\"]:\n        libritts.prepare_align(config)\n    if \"Arabic\" in config[\"dataset\"]:\n        arabic.prepare_align(config)\nif __name__ == \"__main__\":",
        "detail": "klaam.klaam.external.FastSpeech2.prepare_align",
        "documentation": {}
    },
    {
        "label": "preprocess_arabic",
        "kind": 2,
        "importPath": "klaam.klaam.external.FastSpeech2.synthesize",
        "description": "klaam.klaam.external.FastSpeech2.synthesize",
        "peekOfCode": "def preprocess_arabic(text, preprocess_config, bw=False):\n    text = text.rstrip(punctuation)\n    if bw:\n        text = \"\".join([bw2ar[l] if l in bw2ar else l for l in text])\n    phones = \"\"\n    for word in text.split(\" \"):\n        if word in punctuation:\n            pass\n        elif len(word.strip()) > 0:\n            phones += phonetise(word)[0]",
        "detail": "klaam.klaam.external.FastSpeech2.synthesize",
        "documentation": {}
    },
    {
        "label": "preprocess_mandarin",
        "kind": 2,
        "importPath": "klaam.klaam.external.FastSpeech2.synthesize",
        "description": "klaam.klaam.external.FastSpeech2.synthesize",
        "peekOfCode": "def preprocess_mandarin(text, preprocess_config):\n    lexicon = read_lexicon(preprocess_config[\"path\"][\"lexicon_path\"])\n    phones = []\n    pinyins = [p[0] for p in pinyin(text, style=Style.TONE3, strict=False, neutral_tone_with_five=True)]\n    for p in pinyins:\n        if p in lexicon:\n            phones += lexicon[p]\n        else:\n            phones.append(\"sp\")\n    phones = \"{\" + \" \".join(phones) + \"}\"",
        "detail": "klaam.klaam.external.FastSpeech2.synthesize",
        "documentation": {}
    },
    {
        "label": "synthesize",
        "kind": 2,
        "importPath": "klaam.klaam.external.FastSpeech2.synthesize",
        "description": "klaam.klaam.external.FastSpeech2.synthesize",
        "peekOfCode": "def synthesize(model, step, configs, vocoder, batchs, control_values):\n    preprocess_config, model_config, train_config = configs\n    pitch_control, energy_control, duration_control = control_values\n    for batch in batchs:\n        batch = to_device(batch, device)\n        with torch.no_grad():\n            # Forward\n            output = model(*(batch[2:]), p_control=pitch_control, e_control=energy_control, d_control=duration_control)\n            synth_samples(\n                batch,",
        "detail": "klaam.klaam.external.FastSpeech2.synthesize",
        "documentation": {}
    },
    {
        "label": "device",
        "kind": 5,
        "importPath": "klaam.klaam.external.FastSpeech2.synthesize",
        "description": "klaam.klaam.external.FastSpeech2.synthesize",
        "peekOfCode": "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\ndef preprocess_arabic(text, preprocess_config, bw=False):\n    text = text.rstrip(punctuation)\n    if bw:\n        text = \"\".join([bw2ar[l] if l in bw2ar else l for l in text])\n    phones = \"\"\n    for word in text.split(\" \"):\n        if word in punctuation:\n            pass\n        elif len(word.strip()) > 0:",
        "detail": "klaam.klaam.external.FastSpeech2.synthesize",
        "documentation": {}
    },
    {
        "label": "main",
        "kind": 2,
        "importPath": "klaam.klaam.external.FastSpeech2.train",
        "description": "klaam.klaam.external.FastSpeech2.train",
        "peekOfCode": "def main(args, configs):\n    print(\"Prepare training ...\")\n    preprocess_config, model_config, train_config = configs\n    # Get dataset\n    dataset = Dataset(\"train.txt\", preprocess_config, train_config, sort=True, drop_last=True)\n    batch_size = train_config[\"optimizer\"][\"batch_size\"]\n    group_size = 4  # Set this larger than 1 to enable sorting in Dataset\n    assert batch_size * group_size < len(dataset)\n    loader = DataLoader(\n        dataset,",
        "detail": "klaam.klaam.external.FastSpeech2.train",
        "documentation": {}
    },
    {
        "label": "device",
        "kind": 5,
        "importPath": "klaam.klaam.external.FastSpeech2.train",
        "description": "klaam.klaam.external.FastSpeech2.train",
        "peekOfCode": "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\ndef main(args, configs):\n    print(\"Prepare training ...\")\n    preprocess_config, model_config, train_config = configs\n    # Get dataset\n    dataset = Dataset(\"train.txt\", preprocess_config, train_config, sort=True, drop_last=True)\n    batch_size = train_config[\"optimizer\"][\"batch_size\"]\n    group_size = 4  # Set this larger than 1 to enable sorting in Dataset\n    assert batch_size * group_size < len(dataset)\n    loader = DataLoader(",
        "detail": "klaam.klaam.external.FastSpeech2.train",
        "documentation": {}
    },
    {
        "label": "Wav2Vec2ClassificationModel",
        "kind": 6,
        "importPath": "klaam.klaam.models.wav2vec",
        "description": "klaam.klaam.models.wav2vec",
        "peekOfCode": "class Wav2Vec2ClassificationModel(Wav2Vec2PreTrainedModel):\n    def __init__(self, config):\n        super().__init__(config)\n        self.wav2vec2 = Wav2Vec2Model(config)\n        self.inner_dim = 128\n        self.feature_size = 999\n        self.tanh = nn.Tanh()\n        self.linear1 = nn.Linear(1024, self.inner_dim)\n        self.linear2 = nn.Linear(self.inner_dim * self.feature_size, 5)\n        self.init_weights()",
        "detail": "klaam.klaam.models.wav2vec",
        "documentation": {}
    },
    {
        "label": "CustomWav2Vec2Processor",
        "kind": 6,
        "importPath": "klaam.klaam.processors.wav2vec",
        "description": "klaam.klaam.processors.wav2vec",
        "peekOfCode": "class CustomWav2Vec2Processor:\n    r\"\"\"\n    Constructs a Wav2Vec2 processor which wraps a Wav2Vec2 feature extractor and a Wav2Vec2 CTC tokenizer into a single\n    processor.\n    :class:`~transformers.Wav2Vec2Processor` offers all the functionalities of\n    :class:`~transformers.Wav2Vec2FeatureExtractor` and :class:`~transformers.Wav2Vec2CTCTokenizer`. See the docstring\n    of :meth:`~transformers.Wav2Vec2Processor.__call__` and :meth:`~transformers.Wav2Vec2Processor.decode` for more\n    information.\n    Args:\n        feature_extractor (:obj:`Wav2Vec2FeatureExtractor`):",
        "detail": "klaam.klaam.processors.wav2vec",
        "documentation": {}
    },
    {
        "label": "DialectSpeechCorpusConfig",
        "kind": 6,
        "importPath": "klaam.klaam.speech_corpus.dialect",
        "description": "klaam.klaam.speech_corpus.dialect",
        "peekOfCode": "class DialectSpeechCorpusConfig(datasets.BuilderConfig):\n    \"\"\"BuilderConfig for DialectSpeechCorpusCorpus.\"\"\"\n    def __init__(self, **kwargs):\n        \"\"\"\n        Args:\n          data_dir: `string`, the path to the folder containing the files in the\n            downloaded .tar\n          citation: `string`, citation for the data set\n          url: `string`, url for information about the data set\n          **kwargs: keyword arguments forwarded to super.",
        "detail": "klaam.klaam.speech_corpus.dialect",
        "documentation": {}
    },
    {
        "label": "DialectSpeechCorpus",
        "kind": 6,
        "importPath": "klaam.klaam.speech_corpus.dialect",
        "description": "klaam.klaam.speech_corpus.dialect",
        "peekOfCode": "class DialectSpeechCorpus(datasets.GeneratorBasedBuilder):\n    \"\"\"DialectSpeechCorpus dataset.\"\"\"\n    BUILDER_CONFIGS = [\n        DialectSpeechCorpusConfig(name=\"clean\", description=\"'Clean' speech.\"),\n    ]\n    def _info(self):\n        return datasets.DatasetInfo(\n            description=_DESCRIPTION,\n            features=datasets.Features(\n                {",
        "detail": "klaam.klaam.speech_corpus.dialect",
        "documentation": {}
    },
    {
        "label": "map_to_array",
        "kind": 2,
        "importPath": "klaam.klaam.speech_corpus.dialect",
        "description": "klaam.klaam.speech_corpus.dialect",
        "peekOfCode": "def map_to_array(batch):\n    speech_array, _ = sf.read(batch[\"file\"])\n    batch[\"speech\"] = speech_array\n    return batch\ndataset = dataset.map(map_to_array, remove_columns=[\"file\"])\n```\n\"\"\"\nclass DialectSpeechCorpusConfig(datasets.BuilderConfig):\n    \"\"\"BuilderConfig for DialectSpeechCorpusCorpus.\"\"\"\n    def __init__(self, **kwargs):",
        "detail": "klaam.klaam.speech_corpus.dialect",
        "documentation": {}
    },
    {
        "label": "map_to_array",
        "kind": 2,
        "importPath": "klaam.klaam.speech_corpus.dialect",
        "description": "klaam.klaam.speech_corpus.dialect",
        "peekOfCode": "def map_to_array(batch):\n    start, stop = batch[\"segment\"].split(\"_\")\n    speech_array, _ = sf.read(batch[\"file\"], start=start, stop=stop)\n    batch[\"speech\"] = speech_array\n    return batch\nclass DialectSpeechCorpus(datasets.GeneratorBasedBuilder):\n    \"\"\"DialectSpeechCorpus dataset.\"\"\"\n    BUILDER_CONFIGS = [\n        DialectSpeechCorpusConfig(name=\"clean\", description=\"'Clean' speech.\"),\n    ]",
        "detail": "klaam.klaam.speech_corpus.dialect",
        "documentation": {}
    },
    {
        "label": "_CITATION",
        "kind": 5,
        "importPath": "klaam.klaam.speech_corpus.dialect",
        "description": "klaam.klaam.speech_corpus.dialect",
        "peekOfCode": "_CITATION = \"\"\"\n\"\"\"\n_DESCRIPTION = \"\"\"\\\n```python\nimport soundfile as sf\ndef map_to_array(batch):\n    speech_array, _ = sf.read(batch[\"file\"])\n    batch[\"speech\"] = speech_array\n    return batch\ndataset = dataset.map(map_to_array, remove_columns=[\"file\"])",
        "detail": "klaam.klaam.speech_corpus.dialect",
        "documentation": {}
    },
    {
        "label": "_DESCRIPTION",
        "kind": 5,
        "importPath": "klaam.klaam.speech_corpus.dialect",
        "description": "klaam.klaam.speech_corpus.dialect",
        "peekOfCode": "_DESCRIPTION = \"\"\"\\\n```python\nimport soundfile as sf\ndef map_to_array(batch):\n    speech_array, _ = sf.read(batch[\"file\"])\n    batch[\"speech\"] = speech_array\n    return batch\ndataset = dataset.map(map_to_array, remove_columns=[\"file\"])\n```\n\"\"\"",
        "detail": "klaam.klaam.speech_corpus.dialect",
        "documentation": {}
    },
    {
        "label": "dataset",
        "kind": 5,
        "importPath": "klaam.klaam.speech_corpus.dialect",
        "description": "klaam.klaam.speech_corpus.dialect",
        "peekOfCode": "dataset = dataset.map(map_to_array, remove_columns=[\"file\"])\n```\n\"\"\"\nclass DialectSpeechCorpusConfig(datasets.BuilderConfig):\n    \"\"\"BuilderConfig for DialectSpeechCorpusCorpus.\"\"\"\n    def __init__(self, **kwargs):\n        \"\"\"\n        Args:\n          data_dir: `string`, the path to the folder containing the files in the\n            downloaded .tar",
        "detail": "klaam.klaam.speech_corpus.dialect",
        "documentation": {}
    },
    {
        "label": "EgyptianSpeechCorpusConfig",
        "kind": 6,
        "importPath": "klaam.klaam.speech_corpus.egy",
        "description": "klaam.klaam.speech_corpus.egy",
        "peekOfCode": "class EgyptianSpeechCorpusConfig(datasets.BuilderConfig):\n    \"\"\"BuilderConfig for EgyptianSpeechCorpus.\"\"\"\n    def __init__(self, **kwargs):\n        \"\"\"\n        Args:\n          data_dir: `string`, the path to the folder containing the files in the\n            downloaded .tar\n          citation: `string`, citation for the data set\n          url: `string`, url for information about the data set\n          **kwargs: keyword arguments forwarded to super.",
        "detail": "klaam.klaam.speech_corpus.egy",
        "documentation": {}
    },
    {
        "label": "EgyptionSpeechCorpus",
        "kind": 6,
        "importPath": "klaam.klaam.speech_corpus.egy",
        "description": "klaam.klaam.speech_corpus.egy",
        "peekOfCode": "class EgyptionSpeechCorpus(datasets.GeneratorBasedBuilder):\n    \"\"\"EgyptianSpeechCorpus dataset.\"\"\"\n    BUILDER_CONFIGS = [\n        EgyptianSpeechCorpusConfig(name=\"clean\", description=\"'Clean' speech.\"),\n    ]\n    def _info(self):\n        return datasets.DatasetInfo(\n            description=_DESCRIPTION,\n            features=datasets.Features(\n                {",
        "detail": "klaam.klaam.speech_corpus.egy",
        "documentation": {}
    },
    {
        "label": "map_to_array",
        "kind": 2,
        "importPath": "klaam.klaam.speech_corpus.egy",
        "description": "klaam.klaam.speech_corpus.egy",
        "peekOfCode": "def map_to_array(batch):\n    speech_array, _ = sf.read(batch[\"file\"])\n    batch[\"speech\"] = speech_array\n    return batch\ndataset = dataset.map(map_to_array, remove_columns=[\"file\"])\n```\n\"\"\"\n_URL = \"mgb3.zip\"\ncorrupt_files = [\n    \"familyKids_02_first_12min.wav\",",
        "detail": "klaam.klaam.speech_corpus.egy",
        "documentation": {}
    },
    {
        "label": "map_to_array",
        "kind": 2,
        "importPath": "klaam.klaam.speech_corpus.egy",
        "description": "klaam.klaam.speech_corpus.egy",
        "peekOfCode": "def map_to_array(batch):\n    start, stop = batch[\"segment\"].split(\"_\")\n    speech_array, _ = sf.read(batch[\"file\"], start=start, stop=stop)\n    batch[\"speech\"] = speech_array\n    return batch\nclass EgyptionSpeechCorpus(datasets.GeneratorBasedBuilder):\n    \"\"\"EgyptianSpeechCorpus dataset.\"\"\"\n    BUILDER_CONFIGS = [\n        EgyptianSpeechCorpusConfig(name=\"clean\", description=\"'Clean' speech.\"),\n    ]",
        "detail": "klaam.klaam.speech_corpus.egy",
        "documentation": {}
    },
    {
        "label": "_CITATION",
        "kind": 5,
        "importPath": "klaam.klaam.speech_corpus.egy",
        "description": "klaam.klaam.speech_corpus.egy",
        "peekOfCode": "_CITATION = \"\"\"\n\"\"\"\n_DESCRIPTION = \"\"\"\\\n```python\nimport soundfile as sf\ndef map_to_array(batch):\n    speech_array, _ = sf.read(batch[\"file\"])\n    batch[\"speech\"] = speech_array\n    return batch\ndataset = dataset.map(map_to_array, remove_columns=[\"file\"])",
        "detail": "klaam.klaam.speech_corpus.egy",
        "documentation": {}
    },
    {
        "label": "_DESCRIPTION",
        "kind": 5,
        "importPath": "klaam.klaam.speech_corpus.egy",
        "description": "klaam.klaam.speech_corpus.egy",
        "peekOfCode": "_DESCRIPTION = \"\"\"\\\n```python\nimport soundfile as sf\ndef map_to_array(batch):\n    speech_array, _ = sf.read(batch[\"file\"])\n    batch[\"speech\"] = speech_array\n    return batch\ndataset = dataset.map(map_to_array, remove_columns=[\"file\"])\n```\n\"\"\"",
        "detail": "klaam.klaam.speech_corpus.egy",
        "documentation": {}
    },
    {
        "label": "dataset",
        "kind": 5,
        "importPath": "klaam.klaam.speech_corpus.egy",
        "description": "klaam.klaam.speech_corpus.egy",
        "peekOfCode": "dataset = dataset.map(map_to_array, remove_columns=[\"file\"])\n```\n\"\"\"\n_URL = \"mgb3.zip\"\ncorrupt_files = [\n    \"familyKids_02_first_12min.wav\",\n    \"sports_04_first_12min.wav\",\n    \"cooking_05_first_12min.wav\",\n    \"moviesDrama_07_first_12min.wav\",\n    \"science_06_first_12min.wav\",",
        "detail": "klaam.klaam.speech_corpus.egy",
        "documentation": {}
    },
    {
        "label": "_URL",
        "kind": 5,
        "importPath": "klaam.klaam.speech_corpus.egy",
        "description": "klaam.klaam.speech_corpus.egy",
        "peekOfCode": "_URL = \"mgb3.zip\"\ncorrupt_files = [\n    \"familyKids_02_first_12min.wav\",\n    \"sports_04_first_12min.wav\",\n    \"cooking_05_first_12min.wav\",\n    \"moviesDrama_07_first_12min.wav\",\n    \"science_06_first_12min.wav\",\n    \"comedy_09_first_12min.wav\",\n    \"cultural_08_first_12min.wav\",\n    \"familyKids_11_first_12min.wav\",",
        "detail": "klaam.klaam.speech_corpus.egy",
        "documentation": {}
    },
    {
        "label": "corrupt_files",
        "kind": 5,
        "importPath": "klaam.klaam.speech_corpus.egy",
        "description": "klaam.klaam.speech_corpus.egy",
        "peekOfCode": "corrupt_files = [\n    \"familyKids_02_first_12min.wav\",\n    \"sports_04_first_12min.wav\",\n    \"cooking_05_first_12min.wav\",\n    \"moviesDrama_07_first_12min.wav\",\n    \"science_06_first_12min.wav\",\n    \"comedy_09_first_12min.wav\",\n    \"cultural_08_first_12min.wav\",\n    \"familyKids_11_first_12min.wav\",\n    \"science_10_first_12min.wav\",",
        "detail": "klaam.klaam.speech_corpus.egy",
        "documentation": {}
    },
    {
        "label": "MorrocanSpeechCorpusConfig",
        "kind": 6,
        "importPath": "klaam.klaam.speech_corpus.mor",
        "description": "klaam.klaam.speech_corpus.mor",
        "peekOfCode": "class MorrocanSpeechCorpusConfig(datasets.BuilderConfig):\n    \"\"\"BuilderConfig for MorrocanSpeechCorpus.\"\"\"\n    def __init__(self, **kwargs):\n        \"\"\"\n        Args:\n          data_dir: `string`, the path to the folder containing the files in the\n            downloaded .tar\n          citation: `string`, citation for the data set\n          url: `string`, url for information about the data set\n          **kwargs: keyword arguments forwarded to super.",
        "detail": "klaam.klaam.speech_corpus.mor",
        "documentation": {}
    },
    {
        "label": "MorrocanSpeechCorpus",
        "kind": 6,
        "importPath": "klaam.klaam.speech_corpus.mor",
        "description": "klaam.klaam.speech_corpus.mor",
        "peekOfCode": "class MorrocanSpeechCorpus(datasets.GeneratorBasedBuilder):\n    \"\"\"MorrocanSpeechCorpus dataset.\"\"\"\n    BUILDER_CONFIGS = [\n        MorrocanSpeechCorpusConfig(name=\"clean\", description=\"'Clean' speech.\"),\n    ]\n    def _info(self):\n        return datasets.DatasetInfo(\n            description=_DESCRIPTION,\n            features=datasets.Features(\n                {",
        "detail": "klaam.klaam.speech_corpus.mor",
        "documentation": {}
    },
    {
        "label": "map_to_array",
        "kind": 2,
        "importPath": "klaam.klaam.speech_corpus.mor",
        "description": "klaam.klaam.speech_corpus.mor",
        "peekOfCode": "def map_to_array(batch):\n    speech_array, _ = sf.read(batch[\"file\"])\n    batch[\"speech\"] = speech_array\n    return batch\ndataset = dataset.map(map_to_array, remove_columns=[\"file\"])\n```\n\"\"\"\n_URL = \"mgb3.zip\"\nclass MorrocanSpeechCorpusConfig(datasets.BuilderConfig):\n    \"\"\"BuilderConfig for MorrocanSpeechCorpus.\"\"\"",
        "detail": "klaam.klaam.speech_corpus.mor",
        "documentation": {}
    },
    {
        "label": "_CITATION",
        "kind": 5,
        "importPath": "klaam.klaam.speech_corpus.mor",
        "description": "klaam.klaam.speech_corpus.mor",
        "peekOfCode": "_CITATION = \"\"\"\n\"\"\"\n_DESCRIPTION = \"\"\"\\\n```python\nimport soundfile as sf\ndef map_to_array(batch):\n    speech_array, _ = sf.read(batch[\"file\"])\n    batch[\"speech\"] = speech_array\n    return batch\ndataset = dataset.map(map_to_array, remove_columns=[\"file\"])",
        "detail": "klaam.klaam.speech_corpus.mor",
        "documentation": {}
    },
    {
        "label": "_DESCRIPTION",
        "kind": 5,
        "importPath": "klaam.klaam.speech_corpus.mor",
        "description": "klaam.klaam.speech_corpus.mor",
        "peekOfCode": "_DESCRIPTION = \"\"\"\\\n```python\nimport soundfile as sf\ndef map_to_array(batch):\n    speech_array, _ = sf.read(batch[\"file\"])\n    batch[\"speech\"] = speech_array\n    return batch\ndataset = dataset.map(map_to_array, remove_columns=[\"file\"])\n```\n\"\"\"",
        "detail": "klaam.klaam.speech_corpus.mor",
        "documentation": {}
    },
    {
        "label": "dataset",
        "kind": 5,
        "importPath": "klaam.klaam.speech_corpus.mor",
        "description": "klaam.klaam.speech_corpus.mor",
        "peekOfCode": "dataset = dataset.map(map_to_array, remove_columns=[\"file\"])\n```\n\"\"\"\n_URL = \"mgb3.zip\"\nclass MorrocanSpeechCorpusConfig(datasets.BuilderConfig):\n    \"\"\"BuilderConfig for MorrocanSpeechCorpus.\"\"\"\n    def __init__(self, **kwargs):\n        \"\"\"\n        Args:\n          data_dir: `string`, the path to the folder containing the files in the",
        "detail": "klaam.klaam.speech_corpus.mor",
        "documentation": {}
    },
    {
        "label": "_URL",
        "kind": 5,
        "importPath": "klaam.klaam.speech_corpus.mor",
        "description": "klaam.klaam.speech_corpus.mor",
        "peekOfCode": "_URL = \"mgb3.zip\"\nclass MorrocanSpeechCorpusConfig(datasets.BuilderConfig):\n    \"\"\"BuilderConfig for MorrocanSpeechCorpus.\"\"\"\n    def __init__(self, **kwargs):\n        \"\"\"\n        Args:\n          data_dir: `string`, the path to the folder containing the files in the\n            downloaded .tar\n          citation: `string`, citation for the data set\n          url: `string`, url for information about the data set",
        "detail": "klaam.klaam.speech_corpus.mor",
        "documentation": {}
    },
    {
        "label": "write_wav",
        "kind": 2,
        "importPath": "klaam.klaam.utils.audio",
        "description": "klaam.klaam.utils.audio",
        "peekOfCode": "def write_wav(f, sr, x, normalized=False):\n    f = wave.open(f, \"wb\")\n    f.setnchannels(1)\n    f.setsampwidth(2)\n    f.setframerate(sr)\n    wave_data = x.astype(np.short)\n    f.writeframes(wave_data.tobytes())\n    f.close()\ndef get_audio():\n    # call microphone",
        "detail": "klaam.klaam.utils.audio",
        "documentation": {}
    },
    {
        "label": "get_audio",
        "kind": 2,
        "importPath": "klaam.klaam.utils.audio",
        "description": "klaam.klaam.utils.audio",
        "peekOfCode": "def get_audio():\n    # call microphone\n    display(HTML(AUDIO_HTML))\n    data = eval_js(\"data\")\n    binary = b64decode(data.split(\",\")[1])\n    process = (\n        ffmpeg.input(\"pipe:0\")\n        .output(\"pipe:1\", format=\"wav\")\n        .run_async(pipe_stdin=True, pipe_stdout=True, pipe_stderr=True, quiet=True, overwrite_output=True)\n    )",
        "detail": "klaam.klaam.utils.audio",
        "documentation": {}
    },
    {
        "label": "AUDIO_HTML",
        "kind": 5,
        "importPath": "klaam.klaam.utils.audio",
        "description": "klaam.klaam.utils.audio",
        "peekOfCode": "AUDIO_HTML = \"\"\"\n<script>\nvar my_div = document.createElement(\"DIV\");\nvar my_p = document.createElement(\"P\");\nvar my_btn = document.createElement(\"BUTTON\");\nvar t = document.createTextNode(\"Press to start recording\");\nmy_btn.appendChild(t);\n//my_p.appendChild(my_btn);\nmy_div.appendChild(my_btn);\ndocument.body.appendChild(my_div);",
        "detail": "klaam.klaam.utils.audio",
        "documentation": {}
    },
    {
        "label": "recordButton.innerText",
        "kind": 5,
        "importPath": "klaam.klaam.utils.audio",
        "description": "klaam.klaam.utils.audio",
        "peekOfCode": "recordButton.innerText = \"Recording... press to stop\";\nnavigator.mediaDevices.getUserMedia({audio: true}).then(handleSuccess);\nfunction toggleRecording() {\n  if (recorder && recorder.state == \"recording\") {\n      recorder.stop();\n      gumStream.getAudioTracks()[0].stop();\n      recordButton.innerText = \"Saving the recording... pls wait!\"\n  }\n}\n// https://stackoverflow.com/a/951057",
        "detail": "klaam.klaam.utils.audio",
        "documentation": {}
    },
    {
        "label": "recordButton.onclick",
        "kind": 5,
        "importPath": "klaam.klaam.utils.audio",
        "description": "klaam.klaam.utils.audio",
        "peekOfCode": "recordButton.onclick = ()=>{\ntoggleRecording()\nsleep(2000).then(() => {\n  // wait 2000ms for the data to be available...\n  // ideally this should use something like await...\n  //console.log(\"Inside data:\" + base64data)\n  resolve(base64data.toString())\n});\n}\n});",
        "detail": "klaam.klaam.utils.audio",
        "documentation": {}
    },
    {
        "label": "load_file_to_data",
        "kind": 2,
        "importPath": "klaam.klaam.utils.utils",
        "description": "klaam.klaam.utils.utils",
        "peekOfCode": "def load_file_to_data(file, srate=16_000):\n    batch = {}\n    speech, sampling_rate = librosa.load(file, sr=srate)\n    batch[\"speech\"] = speech\n    batch[\"sampling_rate\"] = sampling_rate\n    return batch\ndef predict(data, model, processor, mode=\"rec\", bw=False, return_prob=False):\n    if mode == \"rec\":\n        max_length = 128000\n        features = processor(",
        "detail": "klaam.klaam.utils.utils",
        "documentation": {}
    },
    {
        "label": "predict",
        "kind": 2,
        "importPath": "klaam.klaam.utils.utils",
        "description": "klaam.klaam.utils.utils",
        "peekOfCode": "def predict(data, model, processor, mode=\"rec\", bw=False, return_prob=False):\n    if mode == \"rec\":\n        max_length = 128000\n        features = processor(\n            data[\"speech\"][:max_length],\n            sampling_rate=data[\"sampling_rate\"],\n            padding=True,\n            max_length=max_length,\n            pad_to_multiple_of=max_length,\n            return_tensors=\"pt\",",
        "detail": "klaam.klaam.utils.utils",
        "documentation": {}
    },
    {
        "label": "DEVICE",
        "kind": 5,
        "importPath": "klaam.klaam.utils.utils",
        "description": "klaam.klaam.utils.utils",
        "peekOfCode": "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\ndef load_file_to_data(file, srate=16_000):\n    batch = {}\n    speech, sampling_rate = librosa.load(file, sr=srate)\n    batch[\"speech\"] = speech\n    batch[\"sampling_rate\"] = sampling_rate\n    return batch\ndef predict(data, model, processor, mode=\"rec\", bw=False, return_prob=False):\n    if mode == \"rec\":\n        max_length = 128000",
        "detail": "klaam.klaam.utils.utils",
        "documentation": {}
    },
    {
        "label": "SpeechRecognition",
        "kind": 6,
        "importPath": "klaam.klaam.run",
        "description": "klaam.klaam.run",
        "peekOfCode": "class SpeechRecognition:\n    def __init__(self, lang=\"egy\", path=None):\n        self.lang = lang\n        self.bw = False\n        if path is None:\n            if lang == \"egy\":\n                model_dir = \"arbml/wav2vec2-large-xlsr-53-arabic-egyptian\"\n            elif lang == \"msa\":\n                model_dir = \"elgeish/wav2vec2-large-xlsr-53-arabic\"\n                self.bw = True",
        "detail": "klaam.klaam.run",
        "documentation": {}
    },
    {
        "label": "SpeechClassification",
        "kind": 6,
        "importPath": "klaam.klaam.run",
        "description": "klaam.klaam.run",
        "peekOfCode": "class SpeechClassification:\n    def __init__(self, path=None):\n        if path is None:\n            dir = \"arbml/wav2vec2-large-xlsr-dialect-classification\"\n        else:\n            dir = path\n        self.model = Wav2Vec2ClassificationModel.from_pretrained(dir).to(DEVICE)\n        self.processor = CustomWav2Vec2Processor.from_pretrained(dir)\n    def classify(self, wav_file, return_prob=False):\n        return predict(load_file_to_data(wav_file), self.model, self.processor, mode=\"cls\", return_prob=return_prob)",
        "detail": "klaam.klaam.run",
        "documentation": {}
    },
    {
        "label": "TextToSpeech",
        "kind": 6,
        "importPath": "klaam.klaam.run",
        "description": "klaam.klaam.run",
        "peekOfCode": "class TextToSpeech:\n    def __init__(\n        self,\n        prepare_tts_model_path,\n        model_config_path,\n        train_config_path,\n        vocoder_config_path=None,\n        speaker_pre_trained_path=None,\n        root_path=None,\n    ):",
        "detail": "klaam.klaam.run",
        "documentation": {}
    },
    {
        "label": "DEVICE",
        "kind": 5,
        "importPath": "klaam.klaam.run",
        "description": "klaam.klaam.run",
        "peekOfCode": "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nclass SpeechRecognition:\n    def __init__(self, lang=\"egy\", path=None):\n        self.lang = lang\n        self.bw = False\n        if path is None:\n            if lang == \"egy\":\n                model_dir = \"arbml/wav2vec2-large-xlsr-53-arabic-egyptian\"\n            elif lang == \"msa\":\n                model_dir = \"elgeish/wav2vec2-large-xlsr-53-arabic\"",
        "detail": "klaam.klaam.run",
        "documentation": {}
    },
    {
        "label": "ModelArguments",
        "kind": 6,
        "importPath": "klaam.scripts.run_classifier",
        "description": "klaam.scripts.run_classifier",
        "peekOfCode": "class ModelArguments:\n    \"\"\"\n    Arguments pertaining to which model/config/tokenizer we are going to fine-tune from.\n    \"\"\"\n    model_name_or_path: str = field(\n        metadata={\"help\": \"Path to pretrained model or model identifier from huggingface.co/models\"}\n    )\n    cache_dir: Optional[str] = field(\n        default=None,\n        metadata={\"help\": \"Where do you want to store the pretrained models downloaded from huggingface.co\"},",
        "detail": "klaam.scripts.run_classifier",
        "documentation": {}
    },
    {
        "label": "DataTrainingArguments",
        "kind": 6,
        "importPath": "klaam.scripts.run_classifier",
        "description": "klaam.scripts.run_classifier",
        "peekOfCode": "class DataTrainingArguments:\n    \"\"\"\n    Arguments pertaining to what data we are going to input our model for training and eval.\n    Using `HfArgumentParser` we can turn this class\n    into argparse arguments to be able to specify them on\n    the command line.\n    \"\"\"\n    dataset_config_name: Optional[str] = field(\n        default=None, metadata={\"help\": \"The configuration name of the dataset to use (via the datasets library).\"}\n    )",
        "detail": "klaam.scripts.run_classifier",
        "documentation": {}
    },
    {
        "label": "DataCollatorCTCWithPadding",
        "kind": 6,
        "importPath": "klaam.scripts.run_classifier",
        "description": "klaam.scripts.run_classifier",
        "peekOfCode": "class DataCollatorCTCWithPadding:\n    \"\"\"\n    Data collator that will dynamically pad the inputs received.\n    Args:\n        processor (:class:`~transformers.Wav2Vec2Processor`)\n            The processor used for proccessing the data.\n        padding (:obj:`bool`, :obj:`str` or :class:`~transformers.tokenization_utils_base.PaddingStrategy`, `optional`, defaults to :obj:`True`):\n            Select a strategy to pad the returned sequences (according to the model's padding side and padding index)\n            among:\n            * :obj:`True` or :obj:`'longest'`: Pad to the longest sequence in the batch (or no padding if only a single",
        "detail": "klaam.scripts.run_classifier",
        "documentation": {}
    },
    {
        "label": "CTCTrainer",
        "kind": 6,
        "importPath": "klaam.scripts.run_classifier",
        "description": "klaam.scripts.run_classifier",
        "peekOfCode": "class CTCTrainer(Trainer):\n    def training_step(self, model: nn.Module, inputs: Dict[str, Union[torch.Tensor, Any]]) -> torch.Tensor:\n        \"\"\"\n        Perform a training step on a batch of inputs.\n        Subclass and override to inject custom behavior.\n        Args:\n            model (:obj:`nn.Module`):\n                The model to train.\n            inputs (:obj:`Dict[str, Union[torch.Tensor, Any]]`):\n                The inputs and targets of the model.",
        "detail": "klaam.scripts.run_classifier",
        "documentation": {}
    },
    {
        "label": "list_field",
        "kind": 2,
        "importPath": "klaam.scripts.run_classifier",
        "description": "klaam.scripts.run_classifier",
        "peekOfCode": "def list_field(default=None, metadata=None):\n    return field(default_factory=lambda: default, metadata=metadata)\n@dataclass\nclass ModelArguments:\n    \"\"\"\n    Arguments pertaining to which model/config/tokenizer we are going to fine-tune from.\n    \"\"\"\n    model_name_or_path: str = field(\n        metadata={\"help\": \"Path to pretrained model or model identifier from huggingface.co/models\"}\n    )",
        "detail": "klaam.scripts.run_classifier",
        "documentation": {}
    },
    {
        "label": "main",
        "kind": 2,
        "importPath": "klaam.scripts.run_classifier",
        "description": "klaam.scripts.run_classifier",
        "peekOfCode": "def main():\n    # See all possible arguments in src/transformers/training_args.py\n    # or by passing the --help flag to this script.\n    # We now keep distinct sets of args, for a cleaner separation of concerns.\n    parser = HfArgumentParser((ModelArguments, DataTrainingArguments, TrainingArguments))\n    if len(sys.argv) == 2 and sys.argv[1].endswith(\".json\"):\n        # If we pass only one argument to the script and it's the path to a json file,\n        # let's parse it to get our arguments.\n        model_args, data_args, training_args = parser.parse_json_file(json_file=os.path.abspath(sys.argv[1]))\n    else:",
        "detail": "klaam.scripts.run_classifier",
        "documentation": {}
    },
    {
        "label": "os.environ[\"WANDB_DISABLED\"]",
        "kind": 5,
        "importPath": "klaam.scripts.run_classifier",
        "description": "klaam.scripts.run_classifier",
        "peekOfCode": "os.environ[\"WANDB_DISABLED\"] = \"true\"\nif is_apex_available():\n    pass\nif version.parse(torch.__version__) >= version.parse(\"1.6\"):\n    _is_native_amp_available = True\nlogger = logging.getLogger(__name__)\ndef list_field(default=None, metadata=None):\n    return field(default_factory=lambda: default, metadata=metadata)\n@dataclass\nclass ModelArguments:",
        "detail": "klaam.scripts.run_classifier",
        "documentation": {}
    },
    {
        "label": "logger",
        "kind": 5,
        "importPath": "klaam.scripts.run_classifier",
        "description": "klaam.scripts.run_classifier",
        "peekOfCode": "logger = logging.getLogger(__name__)\ndef list_field(default=None, metadata=None):\n    return field(default_factory=lambda: default, metadata=metadata)\n@dataclass\nclass ModelArguments:\n    \"\"\"\n    Arguments pertaining to which model/config/tokenizer we are going to fine-tune from.\n    \"\"\"\n    model_name_or_path: str = field(\n        metadata={\"help\": \"Path to pretrained model or model identifier from huggingface.co/models\"}",
        "detail": "klaam.scripts.run_classifier",
        "documentation": {}
    },
    {
        "label": "ModelArguments",
        "kind": 6,
        "importPath": "klaam.scripts.run_common_voice",
        "description": "klaam.scripts.run_common_voice",
        "peekOfCode": "class ModelArguments:\n    \"\"\"\n    Arguments pertaining to which model/config/tokenizer we are going to fine-tune from.\n    \"\"\"\n    model_name_or_path: str = field(\n        metadata={\"help\": \"Path to pretrained model or model identifier from huggingface.co/models\"}\n    )\n    cache_dir: Optional[str] = field(\n        default=None,\n        metadata={\"help\": \"Where do you want to store the pretrained models downloaded from huggingface.co\"},",
        "detail": "klaam.scripts.run_common_voice",
        "documentation": {}
    },
    {
        "label": "DataTrainingArguments",
        "kind": 6,
        "importPath": "klaam.scripts.run_common_voice",
        "description": "klaam.scripts.run_common_voice",
        "peekOfCode": "class DataTrainingArguments:\n    \"\"\"\n    Arguments pertaining to what data we are going to input our model for training and eval.\n    Using `HfArgumentParser` we can turn this class\n    into argparse arguments to be able to specify them on\n    the command line.\n    \"\"\"\n    dataset_config_name: Optional[str] = field(\n        default=None, metadata={\"help\": \"The configuration name of the dataset to use (via the datasets library).\"}\n    )",
        "detail": "klaam.scripts.run_common_voice",
        "documentation": {}
    },
    {
        "label": "DataCollatorCTCWithPadding",
        "kind": 6,
        "importPath": "klaam.scripts.run_common_voice",
        "description": "klaam.scripts.run_common_voice",
        "peekOfCode": "class DataCollatorCTCWithPadding:\n    \"\"\"\n    Data collator that will dynamically pad the inputs received.\n    Args:\n        processor (:class:`~transformers.Wav2Vec2Processor`)\n            The processor used for proccessing the data.\n        padding (:obj:`bool`, :obj:`str` or :class:`~transformers.tokenization_utils_base.PaddingStrategy`, `optional`, defaults to :obj:`True`):\n            Select a strategy to pad the returned sequences (according to the model's padding side and padding index)\n            among:\n            * :obj:`True` or :obj:`'longest'`: Pad to the longest sequence in the batch (or no padding if only a single",
        "detail": "klaam.scripts.run_common_voice",
        "documentation": {}
    },
    {
        "label": "CTCTrainer",
        "kind": 6,
        "importPath": "klaam.scripts.run_common_voice",
        "description": "klaam.scripts.run_common_voice",
        "peekOfCode": "class CTCTrainer(Trainer):\n    def training_step(self, model: nn.Module, inputs: Dict[str, Union[torch.Tensor, Any]]) -> torch.Tensor:\n        \"\"\"\n        Perform a training step on a batch of inputs.\n        Subclass and override to inject custom behavior.\n        Args:\n            model (:obj:`nn.Module`):\n                The model to train.\n            inputs (:obj:`Dict[str, Union[torch.Tensor, Any]]`):\n                The inputs and targets of the model.",
        "detail": "klaam.scripts.run_common_voice",
        "documentation": {}
    },
    {
        "label": "list_field",
        "kind": 2,
        "importPath": "klaam.scripts.run_common_voice",
        "description": "klaam.scripts.run_common_voice",
        "peekOfCode": "def list_field(default=None, metadata=None):\n    return field(default_factory=lambda: default, metadata=metadata)\n@dataclass\nclass ModelArguments:\n    \"\"\"\n    Arguments pertaining to which model/config/tokenizer we are going to fine-tune from.\n    \"\"\"\n    model_name_or_path: str = field(\n        metadata={\"help\": \"Path to pretrained model or model identifier from huggingface.co/models\"}\n    )",
        "detail": "klaam.scripts.run_common_voice",
        "documentation": {}
    },
    {
        "label": "main",
        "kind": 2,
        "importPath": "klaam.scripts.run_common_voice",
        "description": "klaam.scripts.run_common_voice",
        "peekOfCode": "def main():\n    # See all possible arguments in src/transformers/training_args.py\n    # or by passing the --help flag to this script.\n    # We now keep distinct sets of args, for a cleaner separation of concerns.\n    parser = HfArgumentParser((ModelArguments, DataTrainingArguments, TrainingArguments))\n    if len(sys.argv) == 2 and sys.argv[1].endswith(\".json\"):\n        # If we pass only one argument to the script and it's the path to a json file,\n        # let's parse it to get our arguments.\n        model_args, data_args, training_args = parser.parse_json_file(json_file=os.path.abspath(sys.argv[1]))\n    else:",
        "detail": "klaam.scripts.run_common_voice",
        "documentation": {}
    },
    {
        "label": "logger",
        "kind": 5,
        "importPath": "klaam.scripts.run_common_voice",
        "description": "klaam.scripts.run_common_voice",
        "peekOfCode": "logger = logging.getLogger(__name__)\ndef list_field(default=None, metadata=None):\n    return field(default_factory=lambda: default, metadata=metadata)\n@dataclass\nclass ModelArguments:\n    \"\"\"\n    Arguments pertaining to which model/config/tokenizer we are going to fine-tune from.\n    \"\"\"\n    model_name_or_path: str = field(\n        metadata={\"help\": \"Path to pretrained model or model identifier from huggingface.co/models\"}",
        "detail": "klaam.scripts.run_common_voice",
        "documentation": {}
    },
    {
        "label": "ModelArguments",
        "kind": 6,
        "importPath": "klaam.scripts.run_mgb3",
        "description": "klaam.scripts.run_mgb3",
        "peekOfCode": "class ModelArguments:\n    \"\"\"\n    Arguments pertaining to which model/config/tokenizer we are going to fine-tune from.\n    \"\"\"\n    model_name_or_path: str = field(\n        metadata={\"help\": \"Path to pretrained model or model identifier from huggingface.co/models\"}\n    )\n    cache_dir: Optional[str] = field(\n        default=None,\n        metadata={\"help\": \"Where do you want to store the pretrained models downloaded from huggingface.co\"},",
        "detail": "klaam.scripts.run_mgb3",
        "documentation": {}
    },
    {
        "label": "DataTrainingArguments",
        "kind": 6,
        "importPath": "klaam.scripts.run_mgb3",
        "description": "klaam.scripts.run_mgb3",
        "peekOfCode": "class DataTrainingArguments:\n    \"\"\"\n    Arguments pertaining to what data we are going to input our model for training and eval.\n    Using `HfArgumentParser` we can turn this class\n    into argparse arguments to be able to specify them on\n    the command line.\n    \"\"\"\n    dataset_config_name: Optional[str] = field(\n        default=None, metadata={\"help\": \"The configuration name of the dataset to use (via the datasets library).\"}\n    )",
        "detail": "klaam.scripts.run_mgb3",
        "documentation": {}
    },
    {
        "label": "DataCollatorCTCWithPadding",
        "kind": 6,
        "importPath": "klaam.scripts.run_mgb3",
        "description": "klaam.scripts.run_mgb3",
        "peekOfCode": "class DataCollatorCTCWithPadding:\n    \"\"\"\n    Data collator that will dynamically pad the inputs received.\n    Args:\n        processor (:class:`~transformers.Wav2Vec2Processor`)\n            The processor used for proccessing the data.\n        padding (:obj:`bool`, :obj:`str` or :class:`~transformers.tokenization_utils_base.PaddingStrategy`, `optional`, defaults to :obj:`True`):\n            Select a strategy to pad the returned sequences (according to the model's padding side and padding index)\n            among:\n            * :obj:`True` or :obj:`'longest'`: Pad to the longest sequence in the batch (or no padding if only a single",
        "detail": "klaam.scripts.run_mgb3",
        "documentation": {}
    },
    {
        "label": "CTCTrainer",
        "kind": 6,
        "importPath": "klaam.scripts.run_mgb3",
        "description": "klaam.scripts.run_mgb3",
        "peekOfCode": "class CTCTrainer(Trainer):\n    def training_step(self, model: nn.Module, inputs: Dict[str, Union[torch.Tensor, Any]]) -> torch.Tensor:\n        \"\"\"\n        Perform a training step on a batch of inputs.\n        Subclass and override to inject custom behavior.\n        Args:\n            model (:obj:`nn.Module`):\n                The model to train.\n            inputs (:obj:`Dict[str, Union[torch.Tensor, Any]]`):\n                The inputs and targets of the model.",
        "detail": "klaam.scripts.run_mgb3",
        "documentation": {}
    },
    {
        "label": "list_field",
        "kind": 2,
        "importPath": "klaam.scripts.run_mgb3",
        "description": "klaam.scripts.run_mgb3",
        "peekOfCode": "def list_field(default=None, metadata=None):\n    return field(default_factory=lambda: default, metadata=metadata)\n@dataclass\nclass ModelArguments:\n    \"\"\"\n    Arguments pertaining to which model/config/tokenizer we are going to fine-tune from.\n    \"\"\"\n    model_name_or_path: str = field(\n        metadata={\"help\": \"Path to pretrained model or model identifier from huggingface.co/models\"}\n    )",
        "detail": "klaam.scripts.run_mgb3",
        "documentation": {}
    },
    {
        "label": "main",
        "kind": 2,
        "importPath": "klaam.scripts.run_mgb3",
        "description": "klaam.scripts.run_mgb3",
        "peekOfCode": "def main():\n    # See all possible arguments in src/transformers/training_args.py\n    # or by passing the --help flag to this script.\n    # We now keep distinct sets of args, for a cleaner separation of concerns.\n    parser = HfArgumentParser((ModelArguments, DataTrainingArguments, TrainingArguments))\n    if len(sys.argv) == 2 and sys.argv[1].endswith(\".json\"):\n        # If we pass only one argument to the script and it's the path to a json file,\n        # let's parse it to get our arguments.\n        model_args, data_args, training_args = parser.parse_json_file(json_file=os.path.abspath(sys.argv[1]))\n    else:",
        "detail": "klaam.scripts.run_mgb3",
        "documentation": {}
    },
    {
        "label": "logger",
        "kind": 5,
        "importPath": "klaam.scripts.run_mgb3",
        "description": "klaam.scripts.run_mgb3",
        "peekOfCode": "logger = logging.getLogger(__name__)\ndef list_field(default=None, metadata=None):\n    return field(default_factory=lambda: default, metadata=metadata)\n@dataclass\nclass ModelArguments:\n    \"\"\"\n    Arguments pertaining to which model/config/tokenizer we are going to fine-tune from.\n    \"\"\"\n    model_name_or_path: str = field(\n        metadata={\"help\": \"Path to pretrained model or model identifier from huggingface.co/models\"}",
        "detail": "klaam.scripts.run_mgb3",
        "documentation": {}
    },
    {
        "label": "ModelArguments",
        "kind": 6,
        "importPath": "klaam.scripts.run_mgb5",
        "description": "klaam.scripts.run_mgb5",
        "peekOfCode": "class ModelArguments:\n    \"\"\"\n    Arguments pertaining to which model/config/tokenizer we are going to fine-tune from.\n    \"\"\"\n    model_name_or_path: str = field(\n        metadata={\"help\": \"Path to pretrained model or model identifier from huggingface.co/models\"}\n    )\n    cache_dir: Optional[str] = field(\n        default=None,\n        metadata={\"help\": \"Where do you want to store the pretrained models downloaded from huggingface.co\"},",
        "detail": "klaam.scripts.run_mgb5",
        "documentation": {}
    },
    {
        "label": "DataTrainingArguments",
        "kind": 6,
        "importPath": "klaam.scripts.run_mgb5",
        "description": "klaam.scripts.run_mgb5",
        "peekOfCode": "class DataTrainingArguments:\n    \"\"\"\n    Arguments pertaining to what data we are going to input our model for training and eval.\n    Using `HfArgumentParser` we can turn this class\n    into argparse arguments to be able to specify them on\n    the command line.\n    \"\"\"\n    dataset_config_name: Optional[str] = field(\n        default=None, metadata={\"help\": \"The configuration name of the dataset to use (via the datasets library).\"}\n    )",
        "detail": "klaam.scripts.run_mgb5",
        "documentation": {}
    },
    {
        "label": "DataCollatorCTCWithPadding",
        "kind": 6,
        "importPath": "klaam.scripts.run_mgb5",
        "description": "klaam.scripts.run_mgb5",
        "peekOfCode": "class DataCollatorCTCWithPadding:\n    \"\"\"\n    Data collator that will dynamically pad the inputs received.\n    Args:\n        processor (:class:`~transformers.Wav2Vec2Processor`)\n            The processor used for proccessing the data.\n        padding (:obj:`bool`, :obj:`str` or :class:`~transformers.tokenization_utils_base.PaddingStrategy`, `optional`, defaults to :obj:`True`):\n            Select a strategy to pad the returned sequences (according to the model's padding side and padding index)\n            among:\n            * :obj:`True` or :obj:`'longest'`: Pad to the longest sequence in the batch (or no padding if only a single",
        "detail": "klaam.scripts.run_mgb5",
        "documentation": {}
    },
    {
        "label": "CTCTrainer",
        "kind": 6,
        "importPath": "klaam.scripts.run_mgb5",
        "description": "klaam.scripts.run_mgb5",
        "peekOfCode": "class CTCTrainer(Trainer):\n    def training_step(self, model: nn.Module, inputs: Dict[str, Union[torch.Tensor, Any]]) -> torch.Tensor:\n        \"\"\"\n        Perform a training step on a batch of inputs.\n        Subclass and override to inject custom behavior.\n        Args:\n            model (:obj:`nn.Module`):\n                The model to train.\n            inputs (:obj:`Dict[str, Union[torch.Tensor, Any]]`):\n                The inputs and targets of the model.",
        "detail": "klaam.scripts.run_mgb5",
        "documentation": {}
    },
    {
        "label": "list_field",
        "kind": 2,
        "importPath": "klaam.scripts.run_mgb5",
        "description": "klaam.scripts.run_mgb5",
        "peekOfCode": "def list_field(default=None, metadata=None):\n    return field(default_factory=lambda: default, metadata=metadata)\n@dataclass\nclass ModelArguments:\n    \"\"\"\n    Arguments pertaining to which model/config/tokenizer we are going to fine-tune from.\n    \"\"\"\n    model_name_or_path: str = field(\n        metadata={\"help\": \"Path to pretrained model or model identifier from huggingface.co/models\"}\n    )",
        "detail": "klaam.scripts.run_mgb5",
        "documentation": {}
    },
    {
        "label": "main",
        "kind": 2,
        "importPath": "klaam.scripts.run_mgb5",
        "description": "klaam.scripts.run_mgb5",
        "peekOfCode": "def main():\n    # See all possible arguments in src/transformers/training_args.py\n    # or by passing the --help flag to this script.\n    # We now keep distinct sets of args, for a cleaner separation of concerns.\n    parser = HfArgumentParser((ModelArguments, DataTrainingArguments, TrainingArguments))\n    if len(sys.argv) == 2 and sys.argv[1].endswith(\".json\"):\n        # If we pass only one argument to the script and it's the path to a json file,\n        # let's parse it to get our arguments.\n        model_args, data_args, training_args = parser.parse_json_file(json_file=os.path.abspath(sys.argv[1]))\n    else:",
        "detail": "klaam.scripts.run_mgb5",
        "documentation": {}
    },
    {
        "label": "logger",
        "kind": 5,
        "importPath": "klaam.scripts.run_mgb5",
        "description": "klaam.scripts.run_mgb5",
        "peekOfCode": "logger = logging.getLogger(__name__)\ndef list_field(default=None, metadata=None):\n    return field(default_factory=lambda: default, metadata=metadata)\n@dataclass\nclass ModelArguments:\n    \"\"\"\n    Arguments pertaining to which model/config/tokenizer we are going to fine-tune from.\n    \"\"\"\n    model_name_or_path: str = field(\n        metadata={\"help\": \"Path to pretrained model or model identifier from huggingface.co/models\"}",
        "detail": "klaam.scripts.run_mgb5",
        "documentation": {}
    },
    {
        "label": "naive_way_of_sorting",
        "kind": 2,
        "importPath": "klaam.tests.components.test_simple",
        "description": "klaam.tests.components.test_simple",
        "peekOfCode": "def naive_way_of_sorting(arr):\n    for i in range(len(arr)):\n        for j in range(i + 1, len(arr)):\n            if arr[i] > arr[j]:\n                arr[i], arr[j] = arr[j], arr[i]\n    return arr\n@pytest.mark.parametrize(\n    \"size\",\n    [\n        0,",
        "detail": "klaam.tests.components.test_simple",
        "documentation": {}
    },
    {
        "label": "test_random_horizontal_shift",
        "kind": 2,
        "importPath": "klaam.tests.components.test_simple",
        "description": "klaam.tests.components.test_simple",
        "peekOfCode": "def test_random_horizontal_shift(size):\n    arr = [random.randint(0, size) for _ in range(size)]\n    # naive implementation that I know forsure it works\n    expected_sorted_arr = naive_way_of_sorting(arr)\n    # the targeted implementation to be compared to\n    actual_sorted_arr = sorted(arr)\n    # comparing expectation vs. actual value\n    assert expected_sorted_arr == actual_sorted_arr",
        "detail": "klaam.tests.components.test_simple",
        "documentation": {}
    },
    {
        "label": "set_seed",
        "kind": 2,
        "importPath": "klaam.tests.fixtures",
        "description": "klaam.tests.fixtures",
        "peekOfCode": "def set_seed():\n    seed = 42",
        "detail": "klaam.tests.fixtures",
        "documentation": {}
    },
    {
        "label": "compute_bleu",
        "kind": 2,
        "importPath": "nmt.nmt.scripts.bleu",
        "description": "nmt.nmt.scripts.bleu",
        "peekOfCode": "def compute_bleu(reference_corpus, translation_corpus, max_order=4,\n                 smooth=False):\n  \"\"\"Computes BLEU score of translated segments against one or more references.\n  Args:\n    reference_corpus: list of lists of references for each translation. Each\n        reference should be tokenized into a list of tokens.\n    translation_corpus: list of translations to score. Each translation\n        should be tokenized into a list of tokens.\n    max_order: Maximum n-gram order to use when computing BLEU score.\n    smooth: Whether or not to apply Lin et al. 2004 smoothing.",
        "detail": "nmt.nmt.scripts.bleu",
        "documentation": {}
    },
    {
        "label": "rouge_n",
        "kind": 2,
        "importPath": "nmt.nmt.scripts.rouge",
        "description": "nmt.nmt.scripts.rouge",
        "peekOfCode": "def rouge_n(evaluated_sentences, reference_sentences, n=2):\n  \"\"\"\n  Computes ROUGE-N of two text collections of sentences.\n  Sourece: http://research.microsoft.com/en-us/um/people/cyl/download/\n  papers/rouge-working-note-v1.3.1.pdf\n  Args:\n    evaluated_sentences: The sentences that have been picked by the summarizer\n    reference_sentences: The sentences from the referene set\n    n: Size of ngram.  Defaults to 2.\n  Returns:",
        "detail": "nmt.nmt.scripts.rouge",
        "documentation": {}
    },
    {
        "label": "rouge_l_sentence_level",
        "kind": 2,
        "importPath": "nmt.nmt.scripts.rouge",
        "description": "nmt.nmt.scripts.rouge",
        "peekOfCode": "def rouge_l_sentence_level(evaluated_sentences, reference_sentences):\n  \"\"\"\n  Computes ROUGE-L (sentence level) of two text collections of sentences.\n  http://research.microsoft.com/en-us/um/people/cyl/download/papers/\n  rouge-working-note-v1.3.1.pdf\n  Calculated according to:\n  R_lcs = LCS(X,Y)/m\n  P_lcs = LCS(X,Y)/n\n  F_lcs = ((1 + beta^2)*R_lcs*P_lcs) / (R_lcs + (beta^2) * P_lcs)\n  where:",
        "detail": "nmt.nmt.scripts.rouge",
        "documentation": {}
    },
    {
        "label": "rouge_l_summary_level",
        "kind": 2,
        "importPath": "nmt.nmt.scripts.rouge",
        "description": "nmt.nmt.scripts.rouge",
        "peekOfCode": "def rouge_l_summary_level(evaluated_sentences, reference_sentences):\n  \"\"\"\n  Computes ROUGE-L (summary level) of two text collections of sentences.\n  http://research.microsoft.com/en-us/um/people/cyl/download/papers/\n  rouge-working-note-v1.3.1.pdf\n  Calculated according to:\n  R_lcs = SUM(1, u)[LCS<union>(r_i,C)]/m\n  P_lcs = SUM(1, u)[LCS<union>(r_i,C)]/n\n  F_lcs = ((1 + beta^2)*R_lcs*P_lcs) / (R_lcs + (beta^2) * P_lcs)\n  where:",
        "detail": "nmt.nmt.scripts.rouge",
        "documentation": {}
    },
    {
        "label": "rouge",
        "kind": 2,
        "importPath": "nmt.nmt.scripts.rouge",
        "description": "nmt.nmt.scripts.rouge",
        "peekOfCode": "def rouge(hypotheses, references):\n  \"\"\"Calculates average rouge scores for a list of hypotheses and\n  references\"\"\"\n  # Filter out hyps that are of 0 length\n  # hyps_and_refs = zip(hypotheses, references)\n  # hyps_and_refs = [_ for _ in hyps_and_refs if len(_[0]) > 0]\n  # hypotheses, references = zip(*hyps_and_refs)\n  # Calculate ROUGE-1 F1, precision, recall scores\n  rouge_1 = [\n      rouge_n([hyp], [ref], 1) for hyp, ref in zip(hypotheses, references)",
        "detail": "nmt.nmt.scripts.rouge",
        "documentation": {}
    },
    {
        "label": "create_test_hparams",
        "kind": 2,
        "importPath": "nmt.nmt.utils.common_test_utils",
        "description": "nmt.nmt.utils.common_test_utils",
        "peekOfCode": "def create_test_hparams(unit_type=\"lstm\",\n                        encoder_type=\"uni\",\n                        num_layers=4,\n                        attention=\"\",\n                        attention_architecture=None,\n                        use_residual=False,\n                        inference_indices=None,\n                        num_translations_per_input=1,\n                        beam_width=0,\n                        init_op=\"uniform\"):",
        "detail": "nmt.nmt.utils.common_test_utils",
        "documentation": {}
    },
    {
        "label": "create_test_iterator",
        "kind": 2,
        "importPath": "nmt.nmt.utils.common_test_utils",
        "description": "nmt.nmt.utils.common_test_utils",
        "peekOfCode": "def create_test_iterator(hparams, mode):\n  \"\"\"Create test iterator.\"\"\"\n  src_vocab_table = lookup_ops.index_table_from_tensor(\n      tf.constant([hparams.eos, \"a\", \"b\", \"c\", \"d\"]))\n  tgt_vocab_mapping = tf.constant([hparams.sos, hparams.eos, \"a\", \"b\", \"c\"])\n  tgt_vocab_table = lookup_ops.index_table_from_tensor(tgt_vocab_mapping)\n  if mode == tf.contrib.learn.ModeKeys.INFER:\n    reverse_tgt_vocab_table = lookup_ops.index_to_string_table_from_tensor(\n        tgt_vocab_mapping)\n  src_dataset = tf.data.Dataset.from_tensor_slices(",
        "detail": "nmt.nmt.utils.common_test_utils",
        "documentation": {}
    },
    {
        "label": "evaluate",
        "kind": 2,
        "importPath": "nmt.nmt.utils.evaluation_utils",
        "description": "nmt.nmt.utils.evaluation_utils",
        "peekOfCode": "def evaluate(ref_file, trans_file, metric, subword_option=None):\n  \"\"\"Pick a metric and evaluate depending on task.\"\"\"\n  # BLEU scores for translation task\n  if metric.lower() == \"bleu\":\n    evaluation_score = _bleu(ref_file, trans_file,\n                             subword_option=subword_option)\n  # ROUGE scores for summarization tasks\n  elif metric.lower() == \"rouge\":\n    evaluation_score = _rouge(ref_file, trans_file,\n                              subword_option=subword_option)",
        "detail": "nmt.nmt.utils.evaluation_utils",
        "documentation": {}
    },
    {
        "label": "__all__",
        "kind": 5,
        "importPath": "nmt.nmt.utils.evaluation_utils",
        "description": "nmt.nmt.utils.evaluation_utils",
        "peekOfCode": "__all__ = [\"evaluate\"]\ndef evaluate(ref_file, trans_file, metric, subword_option=None):\n  \"\"\"Pick a metric and evaluate depending on task.\"\"\"\n  # BLEU scores for translation task\n  if metric.lower() == \"bleu\":\n    evaluation_score = _bleu(ref_file, trans_file,\n                             subword_option=subword_option)\n  # ROUGE scores for summarization tasks\n  elif metric.lower() == \"rouge\":\n    evaluation_score = _rouge(ref_file, trans_file,",
        "detail": "nmt.nmt.utils.evaluation_utils",
        "documentation": {}
    },
    {
        "label": "EvaluationUtilsTest",
        "kind": 6,
        "importPath": "nmt.nmt.utils.evaluation_utils_test",
        "description": "nmt.nmt.utils.evaluation_utils_test",
        "peekOfCode": "class EvaluationUtilsTest(tf.test.TestCase):\n  def testEvaluate(self):\n    output = \"nmt/testdata/deen_output\"\n    ref_bpe = \"nmt/testdata/deen_ref_bpe\"\n    ref_spm = \"nmt/testdata/deen_ref_spm\"\n    expected_bleu_score = 22.5855084573\n    expected_rouge_score = 50.8429782599\n    bpe_bleu_score = evaluation_utils.evaluate(\n        ref_bpe, output, \"bleu\", \"bpe\")\n    bpe_rouge_score = evaluation_utils.evaluate(",
        "detail": "nmt.nmt.utils.evaluation_utils_test",
        "documentation": {}
    },
    {
        "label": "BatchedInput",
        "kind": 6,
        "importPath": "nmt.nmt.utils.iterator_utils",
        "description": "nmt.nmt.utils.iterator_utils",
        "peekOfCode": "class BatchedInput(\n    collections.namedtuple(\"BatchedInput\",\n                           (\"initializer\", \"source\", \"target_input\",\n                            \"target_output\", \"source_sequence_length\",\n                            \"target_sequence_length\"))):\n  pass\ndef get_infer_iterator(src_dataset,\n                       src_vocab_table,\n                       batch_size,\n                       eos,",
        "detail": "nmt.nmt.utils.iterator_utils",
        "documentation": {}
    },
    {
        "label": "get_infer_iterator",
        "kind": 2,
        "importPath": "nmt.nmt.utils.iterator_utils",
        "description": "nmt.nmt.utils.iterator_utils",
        "peekOfCode": "def get_infer_iterator(src_dataset,\n                       src_vocab_table,\n                       batch_size,\n                       eos,\n                       src_max_len=None,\n                       use_char_encode=False):\n  if use_char_encode:\n    src_eos_id = vocab_utils.EOS_CHAR_ID\n  else:\n    src_eos_id = tf.cast(src_vocab_table.lookup(tf.constant(eos)), tf.int32)",
        "detail": "nmt.nmt.utils.iterator_utils",
        "documentation": {}
    },
    {
        "label": "get_iterator",
        "kind": 2,
        "importPath": "nmt.nmt.utils.iterator_utils",
        "description": "nmt.nmt.utils.iterator_utils",
        "peekOfCode": "def get_iterator(src_dataset,\n                 tgt_dataset,\n                 src_vocab_table,\n                 tgt_vocab_table,\n                 batch_size,\n                 sos,\n                 eos,\n                 random_seed,\n                 num_buckets,\n                 src_max_len=None,",
        "detail": "nmt.nmt.utils.iterator_utils",
        "documentation": {}
    },
    {
        "label": "__all__",
        "kind": 5,
        "importPath": "nmt.nmt.utils.iterator_utils",
        "description": "nmt.nmt.utils.iterator_utils",
        "peekOfCode": "__all__ = [\"BatchedInput\", \"get_iterator\", \"get_infer_iterator\"]\n# NOTE(ebrevdo): When we subclass this, instances' __dict__ becomes empty.\nclass BatchedInput(\n    collections.namedtuple(\"BatchedInput\",\n                           (\"initializer\", \"source\", \"target_input\",\n                            \"target_output\", \"source_sequence_length\",\n                            \"target_sequence_length\"))):\n  pass\ndef get_infer_iterator(src_dataset,\n                       src_vocab_table,",
        "detail": "nmt.nmt.utils.iterator_utils",
        "documentation": {}
    },
    {
        "label": "IteratorUtilsTest",
        "kind": 6,
        "importPath": "nmt.nmt.utils.iterator_utils_test",
        "description": "nmt.nmt.utils.iterator_utils_test",
        "peekOfCode": "class IteratorUtilsTest(tf.test.TestCase):\n  def testGetIterator(self):\n    tf.set_random_seed(1)\n    tgt_vocab_table = src_vocab_table = lookup_ops.index_table_from_tensor(\n        tf.constant([\"a\", \"b\", \"c\", \"eos\", \"sos\"]))\n    src_dataset = tf.data.Dataset.from_tensor_slices(\n        tf.constant([\"f e a g\", \"c c a\", \"d\", \"c a\"]))\n    tgt_dataset = tf.data.Dataset.from_tensor_slices(\n        tf.constant([\"c c\", \"a b\", \"\", \"b c\"]))\n    hparams = tf.contrib.training.HParams(",
        "detail": "nmt.nmt.utils.iterator_utils_test",
        "documentation": {}
    },
    {
        "label": "check_tensorflow_version",
        "kind": 2,
        "importPath": "nmt.nmt.utils.misc_utils",
        "description": "nmt.nmt.utils.misc_utils",
        "peekOfCode": "def check_tensorflow_version():\n  # LINT.IfChange\n  min_tf_version = \"1.12.0\"\n  # LINT.ThenChange(<pwd>/nmt/copy.bara.sky)\n  if (version.LooseVersion(tf.__version__) <\n      version.LooseVersion(min_tf_version)):\n    raise EnvironmentError(\"Tensorflow version must >= %s\" % min_tf_version)\ndef safe_exp(value):\n  \"\"\"Exponentiation with catching of overflow error.\"\"\"\n  try:",
        "detail": "nmt.nmt.utils.misc_utils",
        "documentation": {}
    },
    {
        "label": "safe_exp",
        "kind": 2,
        "importPath": "nmt.nmt.utils.misc_utils",
        "description": "nmt.nmt.utils.misc_utils",
        "peekOfCode": "def safe_exp(value):\n  \"\"\"Exponentiation with catching of overflow error.\"\"\"\n  try:\n    ans = math.exp(value)\n  except OverflowError:\n    ans = float(\"inf\")\n  return ans\ndef print_time(s, start_time):\n  \"\"\"Take a start time, print elapsed duration, and return a new time.\"\"\"\n  print(\"%s, time %ds, %s.\" % (s, (time.time() - start_time), time.ctime()))",
        "detail": "nmt.nmt.utils.misc_utils",
        "documentation": {}
    },
    {
        "label": "print_time",
        "kind": 2,
        "importPath": "nmt.nmt.utils.misc_utils",
        "description": "nmt.nmt.utils.misc_utils",
        "peekOfCode": "def print_time(s, start_time):\n  \"\"\"Take a start time, print elapsed duration, and return a new time.\"\"\"\n  print(\"%s, time %ds, %s.\" % (s, (time.time() - start_time), time.ctime()))\n  sys.stdout.flush()\n  return time.time()\ndef print_out(s, f=None, new_line=True):\n  \"\"\"Similar to print but with support to flush and output to a file.\"\"\"\n  if isinstance(s, bytes):\n    s = s.decode(\"utf-8\")\n  if f:",
        "detail": "nmt.nmt.utils.misc_utils",
        "documentation": {}
    },
    {
        "label": "print_out",
        "kind": 2,
        "importPath": "nmt.nmt.utils.misc_utils",
        "description": "nmt.nmt.utils.misc_utils",
        "peekOfCode": "def print_out(s, f=None, new_line=True):\n  \"\"\"Similar to print but with support to flush and output to a file.\"\"\"\n  if isinstance(s, bytes):\n    s = s.decode(\"utf-8\")\n  if f:\n    f.write(s.encode(\"utf-8\"))\n    if new_line:\n      f.write(b\"\\n\")\n  # stdout\n  if six.PY2:",
        "detail": "nmt.nmt.utils.misc_utils",
        "documentation": {}
    },
    {
        "label": "print_hparams",
        "kind": 2,
        "importPath": "nmt.nmt.utils.misc_utils",
        "description": "nmt.nmt.utils.misc_utils",
        "peekOfCode": "def print_hparams(hparams, skip_patterns=None, header=None):\n  \"\"\"Print hparams, can skip keys based on pattern.\"\"\"\n  if header: print_out(\"%s\" % header)\n  values = hparams.values()\n  for key in sorted(values.keys()):\n    if not skip_patterns or all(\n        [skip_pattern not in key for skip_pattern in skip_patterns]):\n      print_out(\"  %s=%s\" % (key, str(values[key])))\ndef load_hparams(model_dir):\n  \"\"\"Load hparams from an existing model directory.\"\"\"",
        "detail": "nmt.nmt.utils.misc_utils",
        "documentation": {}
    },
    {
        "label": "load_hparams",
        "kind": 2,
        "importPath": "nmt.nmt.utils.misc_utils",
        "description": "nmt.nmt.utils.misc_utils",
        "peekOfCode": "def load_hparams(model_dir):\n  \"\"\"Load hparams from an existing model directory.\"\"\"\n  hparams_file = os.path.join(model_dir, \"hparams\")\n  if tf.gfile.Exists(hparams_file):\n    print_out(\"# Loading hparams from %s\" % hparams_file)\n    with codecs.getreader(\"utf-8\")(tf.gfile.GFile(hparams_file, \"rb\")) as f:\n      try:\n        hparams_values = json.load(f)\n        hparams = tf.contrib.training.HParams(**hparams_values)\n      except ValueError:",
        "detail": "nmt.nmt.utils.misc_utils",
        "documentation": {}
    },
    {
        "label": "maybe_parse_standard_hparams",
        "kind": 2,
        "importPath": "nmt.nmt.utils.misc_utils",
        "description": "nmt.nmt.utils.misc_utils",
        "peekOfCode": "def maybe_parse_standard_hparams(hparams, hparams_path):\n  \"\"\"Override hparams values with existing standard hparams config.\"\"\"\n  if hparams_path and tf.gfile.Exists(hparams_path):\n    print_out(\"# Loading standard hparams from %s\" % hparams_path)\n    with codecs.getreader(\"utf-8\")(tf.gfile.GFile(hparams_path, \"rb\")) as f:\n      hparams.parse_json(f.read())\n  return hparams\ndef save_hparams(out_dir, hparams):\n  \"\"\"Save hparams.\"\"\"\n  hparams_file = os.path.join(out_dir, \"hparams\")",
        "detail": "nmt.nmt.utils.misc_utils",
        "documentation": {}
    },
    {
        "label": "save_hparams",
        "kind": 2,
        "importPath": "nmt.nmt.utils.misc_utils",
        "description": "nmt.nmt.utils.misc_utils",
        "peekOfCode": "def save_hparams(out_dir, hparams):\n  \"\"\"Save hparams.\"\"\"\n  hparams_file = os.path.join(out_dir, \"hparams\")\n  print_out(\"  saving hparams to %s\" % hparams_file)\n  with codecs.getwriter(\"utf-8\")(tf.gfile.GFile(hparams_file, \"wb\")) as f:\n    f.write(hparams.to_json(indent=4, sort_keys=True))\ndef debug_tensor(s, msg=None, summarize=10):\n  \"\"\"Print the shape and value of a tensor at test time. Return a new tensor.\"\"\"\n  if not msg:\n    msg = s.name",
        "detail": "nmt.nmt.utils.misc_utils",
        "documentation": {}
    },
    {
        "label": "debug_tensor",
        "kind": 2,
        "importPath": "nmt.nmt.utils.misc_utils",
        "description": "nmt.nmt.utils.misc_utils",
        "peekOfCode": "def debug_tensor(s, msg=None, summarize=10):\n  \"\"\"Print the shape and value of a tensor at test time. Return a new tensor.\"\"\"\n  if not msg:\n    msg = s.name\n  return tf.Print(s, [tf.shape(s), s], msg + \" \", summarize=summarize)\ndef add_summary(summary_writer, global_step, tag, value):\n  \"\"\"Add a new summary to the current summary_writer.\n  Useful to log things that are not part of the training graph, e.g., tag=BLEU.\n  \"\"\"\n  summary = tf.Summary(value=[tf.Summary.Value(tag=tag, simple_value=value)])",
        "detail": "nmt.nmt.utils.misc_utils",
        "documentation": {}
    },
    {
        "label": "add_summary",
        "kind": 2,
        "importPath": "nmt.nmt.utils.misc_utils",
        "description": "nmt.nmt.utils.misc_utils",
        "peekOfCode": "def add_summary(summary_writer, global_step, tag, value):\n  \"\"\"Add a new summary to the current summary_writer.\n  Useful to log things that are not part of the training graph, e.g., tag=BLEU.\n  \"\"\"\n  summary = tf.Summary(value=[tf.Summary.Value(tag=tag, simple_value=value)])\n  summary_writer.add_summary(summary, global_step)\ndef get_config_proto(log_device_placement=False, allow_soft_placement=True,\n                     num_intra_threads=0, num_inter_threads=0):\n  # GPU options:\n  # https://www.tensorflow.org/versions/r0.10/how_tos/using_gpu/index.html",
        "detail": "nmt.nmt.utils.misc_utils",
        "documentation": {}
    },
    {
        "label": "get_config_proto",
        "kind": 2,
        "importPath": "nmt.nmt.utils.misc_utils",
        "description": "nmt.nmt.utils.misc_utils",
        "peekOfCode": "def get_config_proto(log_device_placement=False, allow_soft_placement=True,\n                     num_intra_threads=0, num_inter_threads=0):\n  # GPU options:\n  # https://www.tensorflow.org/versions/r0.10/how_tos/using_gpu/index.html\n  config_proto = tf.ConfigProto(\n      log_device_placement=log_device_placement,\n      allow_soft_placement=allow_soft_placement)\n  config_proto.gpu_options.allow_growth = True\n  # CPU threads options\n  if num_intra_threads:",
        "detail": "nmt.nmt.utils.misc_utils",
        "documentation": {}
    },
    {
        "label": "format_text",
        "kind": 2,
        "importPath": "nmt.nmt.utils.misc_utils",
        "description": "nmt.nmt.utils.misc_utils",
        "peekOfCode": "def format_text(words):\n  \"\"\"Convert a sequence words into sentence.\"\"\"\n  if (not hasattr(words, \"__len__\") and  # for numpy array\n      not isinstance(words, collections.Iterable)):\n    words = [words]\n  return b\" \".join(words)\ndef format_bpe_text(symbols, delimiter=b\"@@\"):\n  \"\"\"Convert a sequence of bpe words into sentence.\"\"\"\n  words = []\n  word = b\"\"",
        "detail": "nmt.nmt.utils.misc_utils",
        "documentation": {}
    },
    {
        "label": "format_bpe_text",
        "kind": 2,
        "importPath": "nmt.nmt.utils.misc_utils",
        "description": "nmt.nmt.utils.misc_utils",
        "peekOfCode": "def format_bpe_text(symbols, delimiter=b\"@@\"):\n  \"\"\"Convert a sequence of bpe words into sentence.\"\"\"\n  words = []\n  word = b\"\"\n  if isinstance(symbols, str):\n    symbols = symbols.encode()\n  delimiter_len = len(delimiter)\n  for symbol in symbols:\n    if len(symbol) >= delimiter_len and symbol[-delimiter_len:] == delimiter:\n      word += symbol[:-delimiter_len]",
        "detail": "nmt.nmt.utils.misc_utils",
        "documentation": {}
    },
    {
        "label": "format_spm_text",
        "kind": 2,
        "importPath": "nmt.nmt.utils.misc_utils",
        "description": "nmt.nmt.utils.misc_utils",
        "peekOfCode": "def format_spm_text(symbols):\n  \"\"\"Decode a text in SPM (https://github.com/google/sentencepiece) format.\"\"\"\n  return u\"\".join(format_text(symbols).decode(\"utf-8\").split()).replace(\n      u\"\\u2581\", u\" \").strip().encode(\"utf-8\")",
        "detail": "nmt.nmt.utils.misc_utils",
        "documentation": {}
    },
    {
        "label": "MiscUtilsTest",
        "kind": 6,
        "importPath": "nmt.nmt.utils.misc_utils_test",
        "description": "nmt.nmt.utils.misc_utils_test",
        "peekOfCode": "class MiscUtilsTest(tf.test.TestCase):\n  def testFormatBpeText(self):\n    bpe_line = (\n        b\"En@@ ough to make already reluc@@ tant men hesitate to take screening\"\n        b\" tests .\"\n    )\n    expected_result = (\n        b\"Enough to make already reluctant men hesitate to take screening tests\"\n        b\" .\"\n    )",
        "detail": "nmt.nmt.utils.misc_utils_test",
        "documentation": {}
    },
    {
        "label": "decode_and_evaluate",
        "kind": 2,
        "importPath": "nmt.nmt.utils.nmt_utils",
        "description": "nmt.nmt.utils.nmt_utils",
        "peekOfCode": "def decode_and_evaluate(name,\n                        model,\n                        sess,\n                        trans_file,\n                        ref_file,\n                        metrics,\n                        subword_option,\n                        beam_width,\n                        tgt_eos,\n                        num_translations_per_input=1,",
        "detail": "nmt.nmt.utils.nmt_utils",
        "documentation": {}
    },
    {
        "label": "get_translation",
        "kind": 2,
        "importPath": "nmt.nmt.utils.nmt_utils",
        "description": "nmt.nmt.utils.nmt_utils",
        "peekOfCode": "def get_translation(nmt_outputs, sent_id, tgt_eos, subword_option):\n  \"\"\"Given batch decoding outputs, select a sentence and turn to text.\"\"\"\n  if tgt_eos: tgt_eos = tgt_eos.encode(\"utf-8\")\n  # Select a sentence\n  output = nmt_outputs[sent_id, :].tolist()\n  # If there is an eos symbol in outputs, cut them at that point.\n  if tgt_eos and tgt_eos in output:\n    output = output[:output.index(tgt_eos)]\n  if subword_option == \"bpe\":  # BPE\n    translation = utils.format_bpe_text(output)",
        "detail": "nmt.nmt.utils.nmt_utils",
        "documentation": {}
    },
    {
        "label": "__all__",
        "kind": 5,
        "importPath": "nmt.nmt.utils.nmt_utils",
        "description": "nmt.nmt.utils.nmt_utils",
        "peekOfCode": "__all__ = [\"decode_and_evaluate\", \"get_translation\"]\ndef decode_and_evaluate(name,\n                        model,\n                        sess,\n                        trans_file,\n                        ref_file,\n                        metrics,\n                        subword_option,\n                        beam_width,\n                        tgt_eos,",
        "detail": "nmt.nmt.utils.nmt_utils",
        "documentation": {}
    },
    {
        "label": "create_standard_hparams",
        "kind": 2,
        "importPath": "nmt.nmt.utils.standard_hparams_utils",
        "description": "nmt.nmt.utils.standard_hparams_utils",
        "peekOfCode": "def create_standard_hparams():\n  return tf.contrib.training.HParams(\n      # Data\n      src=\"\",\n      tgt=\"\",\n      train_prefix=\"\",\n      dev_prefix=\"\",\n      test_prefix=\"\",\n      vocab_prefix=\"\",\n      embed_prefix=\"\",",
        "detail": "nmt.nmt.utils.standard_hparams_utils",
        "documentation": {}
    },
    {
        "label": "tokens_to_bytes",
        "kind": 2,
        "importPath": "nmt.nmt.utils.vocab_utils",
        "description": "nmt.nmt.utils.vocab_utils",
        "peekOfCode": "def tokens_to_bytes(tokens):\n  \"\"\"Given a sequence of strings, map to sequence of bytes.\n  Args:\n    tokens: A tf.string tensor\n  Returns:\n    A tensor of shape words.shape + [bytes_per_word] containing byte versions\n    of each word.\n  \"\"\"\n  bytes_per_word = DEFAULT_CHAR_MAXLEN\n  with tf.device(\"/cpu:0\"):",
        "detail": "nmt.nmt.utils.vocab_utils",
        "documentation": {}
    },
    {
        "label": "load_vocab",
        "kind": 2,
        "importPath": "nmt.nmt.utils.vocab_utils",
        "description": "nmt.nmt.utils.vocab_utils",
        "peekOfCode": "def load_vocab(vocab_file):\n  vocab = []\n  with codecs.getreader(\"utf-8\")(tf.gfile.GFile(vocab_file, \"rb\")) as f:\n    vocab_size = 0\n    for word in f:\n      vocab_size += 1\n      vocab.append(word.strip())\n  return vocab, vocab_size\ndef check_vocab(vocab_file, out_dir, check_special_token=True, sos=None,\n                eos=None, unk=None):",
        "detail": "nmt.nmt.utils.vocab_utils",
        "documentation": {}
    },
    {
        "label": "check_vocab",
        "kind": 2,
        "importPath": "nmt.nmt.utils.vocab_utils",
        "description": "nmt.nmt.utils.vocab_utils",
        "peekOfCode": "def check_vocab(vocab_file, out_dir, check_special_token=True, sos=None,\n                eos=None, unk=None):\n  \"\"\"Check if vocab_file doesn't exist, create from corpus_file.\"\"\"\n  if tf.gfile.Exists(vocab_file):\n    utils.print_out(\"# Vocab file %s exists\" % vocab_file)\n    vocab, vocab_size = load_vocab(vocab_file)\n    if check_special_token:\n      # Verify if the vocab starts with unk, sos, eos\n      # If not, prepend those tokens & generate a new vocab file\n      if not unk: unk = UNK",
        "detail": "nmt.nmt.utils.vocab_utils",
        "documentation": {}
    },
    {
        "label": "create_vocab_tables",
        "kind": 2,
        "importPath": "nmt.nmt.utils.vocab_utils",
        "description": "nmt.nmt.utils.vocab_utils",
        "peekOfCode": "def create_vocab_tables(src_vocab_file, tgt_vocab_file, share_vocab):\n  \"\"\"Creates vocab tables for src_vocab_file and tgt_vocab_file.\"\"\"\n  src_vocab_table = lookup_ops.index_table_from_file(\n      src_vocab_file, default_value=UNK_ID)\n  if share_vocab:\n    tgt_vocab_table = src_vocab_table\n  else:\n    tgt_vocab_table = lookup_ops.index_table_from_file(\n        tgt_vocab_file, default_value=UNK_ID)\n  return src_vocab_table, tgt_vocab_table",
        "detail": "nmt.nmt.utils.vocab_utils",
        "documentation": {}
    },
    {
        "label": "load_embed_txt",
        "kind": 2,
        "importPath": "nmt.nmt.utils.vocab_utils",
        "description": "nmt.nmt.utils.vocab_utils",
        "peekOfCode": "def load_embed_txt(embed_file):\n  \"\"\"Load embed_file into a python dictionary.\n  Note: the embed_file should be a Glove/word2vec formatted txt file. Assuming\n  Here is an exampe assuming embed_size=5:\n  the -0.071549 0.093459 0.023738 -0.090339 0.056123\n  to 0.57346 0.5417 -0.23477 -0.3624 0.4037\n  and 0.20327 0.47348 0.050877 0.002103 0.060547\n  For word2vec format, the first line will be: <num_words> <emb_size>.\n  Args:\n    embed_file: file path to the embedding file.",
        "detail": "nmt.nmt.utils.vocab_utils",
        "documentation": {}
    },
    {
        "label": "UNK",
        "kind": 5,
        "importPath": "nmt.nmt.utils.vocab_utils",
        "description": "nmt.nmt.utils.vocab_utils",
        "peekOfCode": "UNK = \"<unk>\"\nSOS = \"<s>\"\nEOS = \"</s>\"\nUNK_ID = 0\n# char ids 0-255 come from utf-8 encoding bytes\n# assign 256-300 to special chars\nBOS_CHAR_ID = 256  # <begin sentence>\nEOS_CHAR_ID = 257  # <end sentence>\nBOW_CHAR_ID = 258  # <begin word>\nEOW_CHAR_ID = 259  # <end word>",
        "detail": "nmt.nmt.utils.vocab_utils",
        "documentation": {}
    },
    {
        "label": "SOS",
        "kind": 5,
        "importPath": "nmt.nmt.utils.vocab_utils",
        "description": "nmt.nmt.utils.vocab_utils",
        "peekOfCode": "SOS = \"<s>\"\nEOS = \"</s>\"\nUNK_ID = 0\n# char ids 0-255 come from utf-8 encoding bytes\n# assign 256-300 to special chars\nBOS_CHAR_ID = 256  # <begin sentence>\nEOS_CHAR_ID = 257  # <end sentence>\nBOW_CHAR_ID = 258  # <begin word>\nEOW_CHAR_ID = 259  # <end word>\nPAD_CHAR_ID = 260  # <padding>",
        "detail": "nmt.nmt.utils.vocab_utils",
        "documentation": {}
    },
    {
        "label": "EOS",
        "kind": 5,
        "importPath": "nmt.nmt.utils.vocab_utils",
        "description": "nmt.nmt.utils.vocab_utils",
        "peekOfCode": "EOS = \"</s>\"\nUNK_ID = 0\n# char ids 0-255 come from utf-8 encoding bytes\n# assign 256-300 to special chars\nBOS_CHAR_ID = 256  # <begin sentence>\nEOS_CHAR_ID = 257  # <end sentence>\nBOW_CHAR_ID = 258  # <begin word>\nEOW_CHAR_ID = 259  # <end word>\nPAD_CHAR_ID = 260  # <padding>\nDEFAULT_CHAR_MAXLEN = 50  # max number of chars for each word.",
        "detail": "nmt.nmt.utils.vocab_utils",
        "documentation": {}
    },
    {
        "label": "UNK_ID",
        "kind": 5,
        "importPath": "nmt.nmt.utils.vocab_utils",
        "description": "nmt.nmt.utils.vocab_utils",
        "peekOfCode": "UNK_ID = 0\n# char ids 0-255 come from utf-8 encoding bytes\n# assign 256-300 to special chars\nBOS_CHAR_ID = 256  # <begin sentence>\nEOS_CHAR_ID = 257  # <end sentence>\nBOW_CHAR_ID = 258  # <begin word>\nEOW_CHAR_ID = 259  # <end word>\nPAD_CHAR_ID = 260  # <padding>\nDEFAULT_CHAR_MAXLEN = 50  # max number of chars for each word.\ndef _string_to_bytes(text, max_length):",
        "detail": "nmt.nmt.utils.vocab_utils",
        "documentation": {}
    },
    {
        "label": "BOS_CHAR_ID",
        "kind": 5,
        "importPath": "nmt.nmt.utils.vocab_utils",
        "description": "nmt.nmt.utils.vocab_utils",
        "peekOfCode": "BOS_CHAR_ID = 256  # <begin sentence>\nEOS_CHAR_ID = 257  # <end sentence>\nBOW_CHAR_ID = 258  # <begin word>\nEOW_CHAR_ID = 259  # <end word>\nPAD_CHAR_ID = 260  # <padding>\nDEFAULT_CHAR_MAXLEN = 50  # max number of chars for each word.\ndef _string_to_bytes(text, max_length):\n  \"\"\"Given string and length, convert to byte seq of at most max_length.\n  This process mimics docqa/elmo's preprocessing:\n  https://github.com/allenai/document-qa/blob/master/docqa/elmo/data.py",
        "detail": "nmt.nmt.utils.vocab_utils",
        "documentation": {}
    },
    {
        "label": "EOS_CHAR_ID",
        "kind": 5,
        "importPath": "nmt.nmt.utils.vocab_utils",
        "description": "nmt.nmt.utils.vocab_utils",
        "peekOfCode": "EOS_CHAR_ID = 257  # <end sentence>\nBOW_CHAR_ID = 258  # <begin word>\nEOW_CHAR_ID = 259  # <end word>\nPAD_CHAR_ID = 260  # <padding>\nDEFAULT_CHAR_MAXLEN = 50  # max number of chars for each word.\ndef _string_to_bytes(text, max_length):\n  \"\"\"Given string and length, convert to byte seq of at most max_length.\n  This process mimics docqa/elmo's preprocessing:\n  https://github.com/allenai/document-qa/blob/master/docqa/elmo/data.py\n  Note that we make use of BOS_CHAR_ID and EOS_CHAR_ID in iterator_utils.py & ",
        "detail": "nmt.nmt.utils.vocab_utils",
        "documentation": {}
    },
    {
        "label": "BOW_CHAR_ID",
        "kind": 5,
        "importPath": "nmt.nmt.utils.vocab_utils",
        "description": "nmt.nmt.utils.vocab_utils",
        "peekOfCode": "BOW_CHAR_ID = 258  # <begin word>\nEOW_CHAR_ID = 259  # <end word>\nPAD_CHAR_ID = 260  # <padding>\nDEFAULT_CHAR_MAXLEN = 50  # max number of chars for each word.\ndef _string_to_bytes(text, max_length):\n  \"\"\"Given string and length, convert to byte seq of at most max_length.\n  This process mimics docqa/elmo's preprocessing:\n  https://github.com/allenai/document-qa/blob/master/docqa/elmo/data.py\n  Note that we make use of BOS_CHAR_ID and EOS_CHAR_ID in iterator_utils.py & \n  our usage differs from docqa/elmo.",
        "detail": "nmt.nmt.utils.vocab_utils",
        "documentation": {}
    },
    {
        "label": "EOW_CHAR_ID",
        "kind": 5,
        "importPath": "nmt.nmt.utils.vocab_utils",
        "description": "nmt.nmt.utils.vocab_utils",
        "peekOfCode": "EOW_CHAR_ID = 259  # <end word>\nPAD_CHAR_ID = 260  # <padding>\nDEFAULT_CHAR_MAXLEN = 50  # max number of chars for each word.\ndef _string_to_bytes(text, max_length):\n  \"\"\"Given string and length, convert to byte seq of at most max_length.\n  This process mimics docqa/elmo's preprocessing:\n  https://github.com/allenai/document-qa/blob/master/docqa/elmo/data.py\n  Note that we make use of BOS_CHAR_ID and EOS_CHAR_ID in iterator_utils.py & \n  our usage differs from docqa/elmo.\n  Args:",
        "detail": "nmt.nmt.utils.vocab_utils",
        "documentation": {}
    },
    {
        "label": "PAD_CHAR_ID",
        "kind": 5,
        "importPath": "nmt.nmt.utils.vocab_utils",
        "description": "nmt.nmt.utils.vocab_utils",
        "peekOfCode": "PAD_CHAR_ID = 260  # <padding>\nDEFAULT_CHAR_MAXLEN = 50  # max number of chars for each word.\ndef _string_to_bytes(text, max_length):\n  \"\"\"Given string and length, convert to byte seq of at most max_length.\n  This process mimics docqa/elmo's preprocessing:\n  https://github.com/allenai/document-qa/blob/master/docqa/elmo/data.py\n  Note that we make use of BOS_CHAR_ID and EOS_CHAR_ID in iterator_utils.py & \n  our usage differs from docqa/elmo.\n  Args:\n    text: tf.string tensor of shape []",
        "detail": "nmt.nmt.utils.vocab_utils",
        "documentation": {}
    },
    {
        "label": "DEFAULT_CHAR_MAXLEN",
        "kind": 5,
        "importPath": "nmt.nmt.utils.vocab_utils",
        "description": "nmt.nmt.utils.vocab_utils",
        "peekOfCode": "DEFAULT_CHAR_MAXLEN = 50  # max number of chars for each word.\ndef _string_to_bytes(text, max_length):\n  \"\"\"Given string and length, convert to byte seq of at most max_length.\n  This process mimics docqa/elmo's preprocessing:\n  https://github.com/allenai/document-qa/blob/master/docqa/elmo/data.py\n  Note that we make use of BOS_CHAR_ID and EOS_CHAR_ID in iterator_utils.py & \n  our usage differs from docqa/elmo.\n  Args:\n    text: tf.string tensor of shape []\n    max_length: max number of chars for each word.",
        "detail": "nmt.nmt.utils.vocab_utils",
        "documentation": {}
    },
    {
        "label": "VocabUtilsTest",
        "kind": 6,
        "importPath": "nmt.nmt.utils.vocab_utils_test",
        "description": "nmt.nmt.utils.vocab_utils_test",
        "peekOfCode": "class VocabUtilsTest(tf.test.TestCase):\n  def testCheckVocab(self):\n    # Create a vocab file\n    vocab_dir = os.path.join(tf.test.get_temp_dir(), \"vocab_dir\")\n    os.makedirs(vocab_dir)\n    vocab_file = os.path.join(vocab_dir, \"vocab_file\")\n    vocab = [\"a\", \"b\", \"c\"]\n    with codecs.getwriter(\"utf-8\")(tf.gfile.GFile(vocab_file, \"wb\")) as f:\n      for word in vocab:\n        f.write(\"%s\\n\" % word)",
        "detail": "nmt.nmt.utils.vocab_utils_test",
        "documentation": {}
    },
    {
        "label": "AttentionModel",
        "kind": 6,
        "importPath": "nmt.nmt.attention_model",
        "description": "nmt.nmt.attention_model",
        "peekOfCode": "class AttentionModel(model.Model):\n  \"\"\"Sequence-to-sequence dynamic model with attention.\n  This class implements a multi-layer recurrent neural network as encoder,\n  and an attention-based decoder. This is the same as the model described in\n  (Luong et al., EMNLP'2015) paper: https://arxiv.org/pdf/1508.04025v5.pdf.\n  This class also allows to use GRU cells in addition to LSTM cells with\n  support for dropout.\n  \"\"\"\n  def __init__(self,\n               hparams,",
        "detail": "nmt.nmt.attention_model",
        "documentation": {}
    },
    {
        "label": "create_attention_mechanism",
        "kind": 2,
        "importPath": "nmt.nmt.attention_model",
        "description": "nmt.nmt.attention_model",
        "peekOfCode": "def create_attention_mechanism(attention_option, num_units, memory,\n                               source_sequence_length, mode):\n  \"\"\"Create attention mechanism based on the attention_option.\"\"\"\n  del mode  # unused\n  # Mechanism\n  if attention_option == \"luong\":\n    attention_mechanism = tf.contrib.seq2seq.LuongAttention(\n        num_units, memory, memory_sequence_length=source_sequence_length)\n  elif attention_option == \"scaled_luong\":\n    attention_mechanism = tf.contrib.seq2seq.LuongAttention(",
        "detail": "nmt.nmt.attention_model",
        "documentation": {}
    },
    {
        "label": "__all__",
        "kind": 5,
        "importPath": "nmt.nmt.attention_model",
        "description": "nmt.nmt.attention_model",
        "peekOfCode": "__all__ = [\"AttentionModel\"]\nclass AttentionModel(model.Model):\n  \"\"\"Sequence-to-sequence dynamic model with attention.\n  This class implements a multi-layer recurrent neural network as encoder,\n  and an attention-based decoder. This is the same as the model described in\n  (Luong et al., EMNLP'2015) paper: https://arxiv.org/pdf/1508.04025v5.pdf.\n  This class also allows to use GRU cells in addition to LSTM cells with\n  support for dropout.\n  \"\"\"\n  def __init__(self,",
        "detail": "nmt.nmt.attention_model",
        "documentation": {}
    },
    {
        "label": "GNMTModel",
        "kind": 6,
        "importPath": "nmt.nmt.gnmt_model",
        "description": "nmt.nmt.gnmt_model",
        "peekOfCode": "class GNMTModel(attention_model.AttentionModel):\n  \"\"\"Sequence-to-sequence dynamic model with GNMT attention architecture.\n  \"\"\"\n  def __init__(self,\n               hparams,\n               mode,\n               iterator,\n               source_vocab_table,\n               target_vocab_table,\n               reverse_target_vocab_table=None,",
        "detail": "nmt.nmt.gnmt_model",
        "documentation": {}
    },
    {
        "label": "GNMTAttentionMultiCell",
        "kind": 6,
        "importPath": "nmt.nmt.gnmt_model",
        "description": "nmt.nmt.gnmt_model",
        "peekOfCode": "class GNMTAttentionMultiCell(tf.nn.rnn_cell.MultiRNNCell):\n  \"\"\"A MultiCell with GNMT attention style.\"\"\"\n  def __init__(self, attention_cell, cells, use_new_attention=False):\n    \"\"\"Creates a GNMTAttentionMultiCell.\n    Args:\n      attention_cell: An instance of AttentionWrapper.\n      cells: A list of RNNCell wrapped with AttentionInputWrapper.\n      use_new_attention: Whether to use the attention generated from current\n        step bottom layer's output. Default is False.\n    \"\"\"",
        "detail": "nmt.nmt.gnmt_model",
        "documentation": {}
    },
    {
        "label": "gnmt_residual_fn",
        "kind": 2,
        "importPath": "nmt.nmt.gnmt_model",
        "description": "nmt.nmt.gnmt_model",
        "peekOfCode": "def gnmt_residual_fn(inputs, outputs):\n  \"\"\"Residual function that handles different inputs and outputs inner dims.\n  Args:\n    inputs: cell inputs, this is actual inputs concatenated with the attention\n      vector.\n    outputs: cell outputs\n  Returns:\n    outputs + actual inputs\n  \"\"\"\n  def split_input(inp, out):",
        "detail": "nmt.nmt.gnmt_model",
        "documentation": {}
    },
    {
        "label": "__all__",
        "kind": 5,
        "importPath": "nmt.nmt.gnmt_model",
        "description": "nmt.nmt.gnmt_model",
        "peekOfCode": "__all__ = [\"GNMTModel\"]\nclass GNMTModel(attention_model.AttentionModel):\n  \"\"\"Sequence-to-sequence dynamic model with GNMT attention architecture.\n  \"\"\"\n  def __init__(self,\n               hparams,\n               mode,\n               iterator,\n               source_vocab_table,\n               target_vocab_table,",
        "detail": "nmt.nmt.gnmt_model",
        "documentation": {}
    },
    {
        "label": "load_data",
        "kind": 2,
        "importPath": "nmt.nmt.inference",
        "description": "nmt.nmt.inference",
        "peekOfCode": "def load_data(inference_input_file, hparams=None):\n  \"\"\"Load inference data.\"\"\"\n  with codecs.getreader(\"utf-8\")(\n      tf.gfile.GFile(inference_input_file, mode=\"rb\")) as f:\n    inference_data = f.read().splitlines()\n  if hparams and hparams.inference_indices:\n    inference_data = [inference_data[i] for i in hparams.inference_indices]\n  return inference_data\ndef get_model_creator(hparams):\n  \"\"\"Get the right model class depending on configuration.\"\"\"",
        "detail": "nmt.nmt.inference",
        "documentation": {}
    },
    {
        "label": "get_model_creator",
        "kind": 2,
        "importPath": "nmt.nmt.inference",
        "description": "nmt.nmt.inference",
        "peekOfCode": "def get_model_creator(hparams):\n  \"\"\"Get the right model class depending on configuration.\"\"\"\n  if (hparams.encoder_type == \"gnmt\" or\n      hparams.attention_architecture in [\"gnmt\", \"gnmt_v2\"]):\n    model_creator = gnmt_model.GNMTModel\n  elif hparams.attention_architecture == \"standard\":\n    model_creator = attention_model.AttentionModel\n  elif not hparams.attention:\n    model_creator = nmt_model.Model\n  else:",
        "detail": "nmt.nmt.inference",
        "documentation": {}
    },
    {
        "label": "start_sess_and_load_model",
        "kind": 2,
        "importPath": "nmt.nmt.inference",
        "description": "nmt.nmt.inference",
        "peekOfCode": "def start_sess_and_load_model(infer_model, ckpt_path):\n  \"\"\"Start session and load model.\"\"\"\n  sess = tf.Session(\n      graph=infer_model.graph, config=utils.get_config_proto())\n  with infer_model.graph.as_default():\n    loaded_infer_model = model_helper.load_model(\n        infer_model.model, ckpt_path, sess, \"infer\")\n  return sess, loaded_infer_model\ndef inference(ckpt_path,\n              inference_input_file,",
        "detail": "nmt.nmt.inference",
        "documentation": {}
    },
    {
        "label": "inference",
        "kind": 2,
        "importPath": "nmt.nmt.inference",
        "description": "nmt.nmt.inference",
        "peekOfCode": "def inference(ckpt_path,\n              inference_input_file,\n              inference_output_file,\n              hparams,\n              num_workers=1,\n              jobid=0,\n              scope=None):\n  \"\"\"Perform translation.\"\"\"\n  if hparams.inference_indices:\n    assert num_workers == 1",
        "detail": "nmt.nmt.inference",
        "documentation": {}
    },
    {
        "label": "single_worker_inference",
        "kind": 2,
        "importPath": "nmt.nmt.inference",
        "description": "nmt.nmt.inference",
        "peekOfCode": "def single_worker_inference(sess,\n                            infer_model,\n                            loaded_infer_model,\n                            inference_input_file,\n                            inference_output_file,\n                            hparams):\n  \"\"\"Inference with a single worker.\"\"\"\n  output_infer = inference_output_file\n  # Read data\n  infer_data = load_data(inference_input_file, hparams)",
        "detail": "nmt.nmt.inference",
        "documentation": {}
    },
    {
        "label": "multi_worker_inference",
        "kind": 2,
        "importPath": "nmt.nmt.inference",
        "description": "nmt.nmt.inference",
        "peekOfCode": "def multi_worker_inference(sess,\n                           infer_model,\n                           loaded_infer_model,\n                           inference_input_file,\n                           inference_output_file,\n                           hparams,\n                           num_workers,\n                           jobid):\n  \"\"\"Inference using multiple workers.\"\"\"\n  assert num_workers > 1",
        "detail": "nmt.nmt.inference",
        "documentation": {}
    },
    {
        "label": "__all__",
        "kind": 5,
        "importPath": "nmt.nmt.inference",
        "description": "nmt.nmt.inference",
        "peekOfCode": "__all__ = [\"load_data\", \"inference\",\n           \"single_worker_inference\", \"multi_worker_inference\"]\ndef _decode_inference_indices(model, sess, output_infer,\n                              output_infer_summary_prefix,\n                              inference_indices,\n                              tgt_eos,\n                              subword_option):\n  \"\"\"Decoding only a specific set of sentences.\"\"\"\n  utils.print_out(\"  decoding to output %s , num sents %d.\" %\n                  (output_infer, len(inference_indices)))",
        "detail": "nmt.nmt.inference",
        "documentation": {}
    },
    {
        "label": "InferenceTest",
        "kind": 6,
        "importPath": "nmt.nmt.inference_test",
        "description": "nmt.nmt.inference_test",
        "peekOfCode": "class InferenceTest(tf.test.TestCase):\n  def _createTestInferCheckpoint(self, hparams, name):\n    # Prepare\n    hparams.vocab_prefix = (\n        \"nmt/testdata/test_infer_vocab\")\n    hparams.src_vocab_file = hparams.vocab_prefix + \".\" + hparams.src\n    hparams.tgt_vocab_file = hparams.vocab_prefix + \".\" + hparams.tgt\n    out_dir = os.path.join(tf.test.get_temp_dir(), name)\n    os.makedirs(out_dir)\n    hparams.out_dir = out_dir",
        "detail": "nmt.nmt.inference_test",
        "documentation": {}
    },
    {
        "label": "float32",
        "kind": 5,
        "importPath": "nmt.nmt.inference_test",
        "description": "nmt.nmt.inference_test",
        "peekOfCode": "float32 = np.float32\nint32 = np.int32\narray = np.array\nclass InferenceTest(tf.test.TestCase):\n  def _createTestInferCheckpoint(self, hparams, name):\n    # Prepare\n    hparams.vocab_prefix = (\n        \"nmt/testdata/test_infer_vocab\")\n    hparams.src_vocab_file = hparams.vocab_prefix + \".\" + hparams.src\n    hparams.tgt_vocab_file = hparams.vocab_prefix + \".\" + hparams.tgt",
        "detail": "nmt.nmt.inference_test",
        "documentation": {}
    },
    {
        "label": "int32",
        "kind": 5,
        "importPath": "nmt.nmt.inference_test",
        "description": "nmt.nmt.inference_test",
        "peekOfCode": "int32 = np.int32\narray = np.array\nclass InferenceTest(tf.test.TestCase):\n  def _createTestInferCheckpoint(self, hparams, name):\n    # Prepare\n    hparams.vocab_prefix = (\n        \"nmt/testdata/test_infer_vocab\")\n    hparams.src_vocab_file = hparams.vocab_prefix + \".\" + hparams.src\n    hparams.tgt_vocab_file = hparams.vocab_prefix + \".\" + hparams.tgt\n    out_dir = os.path.join(tf.test.get_temp_dir(), name)",
        "detail": "nmt.nmt.inference_test",
        "documentation": {}
    },
    {
        "label": "array",
        "kind": 5,
        "importPath": "nmt.nmt.inference_test",
        "description": "nmt.nmt.inference_test",
        "peekOfCode": "array = np.array\nclass InferenceTest(tf.test.TestCase):\n  def _createTestInferCheckpoint(self, hparams, name):\n    # Prepare\n    hparams.vocab_prefix = (\n        \"nmt/testdata/test_infer_vocab\")\n    hparams.src_vocab_file = hparams.vocab_prefix + \".\" + hparams.src\n    hparams.tgt_vocab_file = hparams.vocab_prefix + \".\" + hparams.tgt\n    out_dir = os.path.join(tf.test.get_temp_dir(), name)\n    os.makedirs(out_dir)",
        "detail": "nmt.nmt.inference_test",
        "documentation": {}
    },
    {
        "label": "TrainOutputTuple",
        "kind": 6,
        "importPath": "nmt.nmt.model",
        "description": "nmt.nmt.model",
        "peekOfCode": "class TrainOutputTuple(collections.namedtuple(\n    \"TrainOutputTuple\", (\"train_summary\", \"train_loss\", \"predict_count\",\n                         \"global_step\", \"word_count\", \"batch_size\", \"grad_norm\",\n                         \"learning_rate\"))):\n  \"\"\"To allow for flexibily in returing different outputs.\"\"\"\n  pass\nclass EvalOutputTuple(collections.namedtuple(\n    \"EvalOutputTuple\", (\"eval_loss\", \"predict_count\", \"batch_size\"))):\n  \"\"\"To allow for flexibily in returing different outputs.\"\"\"\n  pass",
        "detail": "nmt.nmt.model",
        "documentation": {}
    },
    {
        "label": "EvalOutputTuple",
        "kind": 6,
        "importPath": "nmt.nmt.model",
        "description": "nmt.nmt.model",
        "peekOfCode": "class EvalOutputTuple(collections.namedtuple(\n    \"EvalOutputTuple\", (\"eval_loss\", \"predict_count\", \"batch_size\"))):\n  \"\"\"To allow for flexibily in returing different outputs.\"\"\"\n  pass\nclass InferOutputTuple(collections.namedtuple(\n    \"InferOutputTuple\", (\"infer_logits\", \"infer_summary\", \"sample_id\",\n                         \"sample_words\"))):\n  \"\"\"To allow for flexibily in returing different outputs.\"\"\"\n  pass\nclass BaseModel(object):",
        "detail": "nmt.nmt.model",
        "documentation": {}
    },
    {
        "label": "InferOutputTuple",
        "kind": 6,
        "importPath": "nmt.nmt.model",
        "description": "nmt.nmt.model",
        "peekOfCode": "class InferOutputTuple(collections.namedtuple(\n    \"InferOutputTuple\", (\"infer_logits\", \"infer_summary\", \"sample_id\",\n                         \"sample_words\"))):\n  \"\"\"To allow for flexibily in returing different outputs.\"\"\"\n  pass\nclass BaseModel(object):\n  \"\"\"Sequence-to-sequence base class.\n  \"\"\"\n  def __init__(self,\n               hparams,",
        "detail": "nmt.nmt.model",
        "documentation": {}
    },
    {
        "label": "BaseModel",
        "kind": 6,
        "importPath": "nmt.nmt.model",
        "description": "nmt.nmt.model",
        "peekOfCode": "class BaseModel(object):\n  \"\"\"Sequence-to-sequence base class.\n  \"\"\"\n  def __init__(self,\n               hparams,\n               mode,\n               iterator,\n               source_vocab_table,\n               target_vocab_table,\n               reverse_target_vocab_table=None,",
        "detail": "nmt.nmt.model",
        "documentation": {}
    },
    {
        "label": "Model",
        "kind": 6,
        "importPath": "nmt.nmt.model",
        "description": "nmt.nmt.model",
        "peekOfCode": "class Model(BaseModel):\n  \"\"\"Sequence-to-sequence dynamic model.\n  This class implements a multi-layer recurrent neural network as encoder,\n  and a multi-layer recurrent neural network decoder.\n  \"\"\"\n  def _build_encoder_from_sequence(self, hparams, sequence, sequence_length):\n    \"\"\"Build an encoder from a sequence.\n    Args:\n      hparams: hyperparameters.\n      sequence: tensor with input sequence data.",
        "detail": "nmt.nmt.model",
        "documentation": {}
    },
    {
        "label": "__all__",
        "kind": 5,
        "importPath": "nmt.nmt.model",
        "description": "nmt.nmt.model",
        "peekOfCode": "__all__ = [\"BaseModel\", \"Model\"]\nclass TrainOutputTuple(collections.namedtuple(\n    \"TrainOutputTuple\", (\"train_summary\", \"train_loss\", \"predict_count\",\n                         \"global_step\", \"word_count\", \"batch_size\", \"grad_norm\",\n                         \"learning_rate\"))):\n  \"\"\"To allow for flexibily in returing different outputs.\"\"\"\n  pass\nclass EvalOutputTuple(collections.namedtuple(\n    \"EvalOutputTuple\", (\"eval_loss\", \"predict_count\", \"batch_size\"))):\n  \"\"\"To allow for flexibily in returing different outputs.\"\"\"",
        "detail": "nmt.nmt.model",
        "documentation": {}
    },
    {
        "label": "ExtraArgs",
        "kind": 6,
        "importPath": "nmt.nmt.model_helper",
        "description": "nmt.nmt.model_helper",
        "peekOfCode": "class ExtraArgs(collections.namedtuple(\n    \"ExtraArgs\", (\"single_cell_fn\", \"model_device_fn\",\n                  \"attention_mechanism_fn\", \"encoder_emb_lookup_fn\"))):\n  pass\nclass TrainModel(\n    collections.namedtuple(\"TrainModel\", (\"graph\", \"model\", \"iterator\",\n                                          \"skip_count_placeholder\"))):\n  pass\ndef create_train_model(\n    model_creator, hparams, scope=None, num_workers=1, jobid=0,",
        "detail": "nmt.nmt.model_helper",
        "documentation": {}
    },
    {
        "label": "TrainModel",
        "kind": 6,
        "importPath": "nmt.nmt.model_helper",
        "description": "nmt.nmt.model_helper",
        "peekOfCode": "class TrainModel(\n    collections.namedtuple(\"TrainModel\", (\"graph\", \"model\", \"iterator\",\n                                          \"skip_count_placeholder\"))):\n  pass\ndef create_train_model(\n    model_creator, hparams, scope=None, num_workers=1, jobid=0,\n    extra_args=None):\n  \"\"\"Create train graph, model, and iterator.\"\"\"\n  src_file = \"%s.%s\" % (hparams.train_prefix, hparams.src)\n  tgt_file = \"%s.%s\" % (hparams.train_prefix, hparams.tgt)",
        "detail": "nmt.nmt.model_helper",
        "documentation": {}
    },
    {
        "label": "EvalModel",
        "kind": 6,
        "importPath": "nmt.nmt.model_helper",
        "description": "nmt.nmt.model_helper",
        "peekOfCode": "class EvalModel(\n    collections.namedtuple(\"EvalModel\",\n                           (\"graph\", \"model\", \"src_file_placeholder\",\n                            \"tgt_file_placeholder\", \"iterator\"))):\n  pass\ndef create_eval_model(model_creator, hparams, scope=None, extra_args=None):\n  \"\"\"Create train graph, model, src/tgt file holders, and iterator.\"\"\"\n  src_vocab_file = hparams.src_vocab_file\n  tgt_vocab_file = hparams.tgt_vocab_file\n  graph = tf.Graph()",
        "detail": "nmt.nmt.model_helper",
        "documentation": {}
    },
    {
        "label": "InferModel",
        "kind": 6,
        "importPath": "nmt.nmt.model_helper",
        "description": "nmt.nmt.model_helper",
        "peekOfCode": "class InferModel(\n    collections.namedtuple(\"InferModel\",\n                           (\"graph\", \"model\", \"src_placeholder\",\n                            \"batch_size_placeholder\", \"iterator\"))):\n  pass\ndef create_infer_model(model_creator, hparams, scope=None, extra_args=None):\n  \"\"\"Create inference model.\"\"\"\n  graph = tf.Graph()\n  src_vocab_file = hparams.src_vocab_file\n  tgt_vocab_file = hparams.tgt_vocab_file",
        "detail": "nmt.nmt.model_helper",
        "documentation": {}
    },
    {
        "label": "get_initializer",
        "kind": 2,
        "importPath": "nmt.nmt.model_helper",
        "description": "nmt.nmt.model_helper",
        "peekOfCode": "def get_initializer(init_op, seed=None, init_weight=None):\n  \"\"\"Create an initializer. init_weight is only for uniform.\"\"\"\n  if init_op == \"uniform\":\n    assert init_weight\n    return tf.random_uniform_initializer(\n        -init_weight, init_weight, seed=seed)\n  elif init_op == \"glorot_normal\":\n    return tf.keras.initializers.glorot_normal(\n        seed=seed)\n  elif init_op == \"glorot_uniform\":",
        "detail": "nmt.nmt.model_helper",
        "documentation": {}
    },
    {
        "label": "get_device_str",
        "kind": 2,
        "importPath": "nmt.nmt.model_helper",
        "description": "nmt.nmt.model_helper",
        "peekOfCode": "def get_device_str(device_id, num_gpus):\n  \"\"\"Return a device string for multi-GPU setup.\"\"\"\n  if num_gpus == 0:\n    return \"/cpu:0\"\n  device_str_output = \"/gpu:%d\" % (device_id % num_gpus)\n  return device_str_output\nclass ExtraArgs(collections.namedtuple(\n    \"ExtraArgs\", (\"single_cell_fn\", \"model_device_fn\",\n                  \"attention_mechanism_fn\", \"encoder_emb_lookup_fn\"))):\n  pass",
        "detail": "nmt.nmt.model_helper",
        "documentation": {}
    },
    {
        "label": "create_train_model",
        "kind": 2,
        "importPath": "nmt.nmt.model_helper",
        "description": "nmt.nmt.model_helper",
        "peekOfCode": "def create_train_model(\n    model_creator, hparams, scope=None, num_workers=1, jobid=0,\n    extra_args=None):\n  \"\"\"Create train graph, model, and iterator.\"\"\"\n  src_file = \"%s.%s\" % (hparams.train_prefix, hparams.src)\n  tgt_file = \"%s.%s\" % (hparams.train_prefix, hparams.tgt)\n  src_vocab_file = hparams.src_vocab_file\n  tgt_vocab_file = hparams.tgt_vocab_file\n  graph = tf.Graph()\n  with graph.as_default(), tf.container(scope or \"train\"):",
        "detail": "nmt.nmt.model_helper",
        "documentation": {}
    },
    {
        "label": "create_eval_model",
        "kind": 2,
        "importPath": "nmt.nmt.model_helper",
        "description": "nmt.nmt.model_helper",
        "peekOfCode": "def create_eval_model(model_creator, hparams, scope=None, extra_args=None):\n  \"\"\"Create train graph, model, src/tgt file holders, and iterator.\"\"\"\n  src_vocab_file = hparams.src_vocab_file\n  tgt_vocab_file = hparams.tgt_vocab_file\n  graph = tf.Graph()\n  with graph.as_default(), tf.container(scope or \"eval\"):\n    src_vocab_table, tgt_vocab_table = vocab_utils.create_vocab_tables(\n        src_vocab_file, tgt_vocab_file, hparams.share_vocab)\n    reverse_tgt_vocab_table = lookup_ops.index_to_string_table_from_file(\n        tgt_vocab_file, default_value=vocab_utils.UNK)",
        "detail": "nmt.nmt.model_helper",
        "documentation": {}
    },
    {
        "label": "create_infer_model",
        "kind": 2,
        "importPath": "nmt.nmt.model_helper",
        "description": "nmt.nmt.model_helper",
        "peekOfCode": "def create_infer_model(model_creator, hparams, scope=None, extra_args=None):\n  \"\"\"Create inference model.\"\"\"\n  graph = tf.Graph()\n  src_vocab_file = hparams.src_vocab_file\n  tgt_vocab_file = hparams.tgt_vocab_file\n  with graph.as_default(), tf.container(scope or \"infer\"):\n    src_vocab_table, tgt_vocab_table = vocab_utils.create_vocab_tables(\n        src_vocab_file, tgt_vocab_file, hparams.share_vocab)\n    reverse_tgt_vocab_table = lookup_ops.index_to_string_table_from_file(\n        tgt_vocab_file, default_value=vocab_utils.UNK)",
        "detail": "nmt.nmt.model_helper",
        "documentation": {}
    },
    {
        "label": "create_emb_for_encoder_and_decoder",
        "kind": 2,
        "importPath": "nmt.nmt.model_helper",
        "description": "nmt.nmt.model_helper",
        "peekOfCode": "def create_emb_for_encoder_and_decoder(share_vocab,\n                                       src_vocab_size,\n                                       tgt_vocab_size,\n                                       src_embed_size,\n                                       tgt_embed_size,\n                                       dtype=tf.float32,\n                                       num_enc_partitions=0,\n                                       num_dec_partitions=0,\n                                       src_vocab_file=None,\n                                       tgt_vocab_file=None,",
        "detail": "nmt.nmt.model_helper",
        "documentation": {}
    },
    {
        "label": "create_rnn_cell",
        "kind": 2,
        "importPath": "nmt.nmt.model_helper",
        "description": "nmt.nmt.model_helper",
        "peekOfCode": "def create_rnn_cell(unit_type, num_units, num_layers, num_residual_layers,\n                    forget_bias, dropout, mode, num_gpus, base_gpu=0,\n                    single_cell_fn=None):\n  \"\"\"Create multi-layer RNN cell.\n  Args:\n    unit_type: string representing the unit type, i.e. \"lstm\".\n    num_units: the depth of each unit.\n    num_layers: number of cells.\n    num_residual_layers: Number of residual layers from top to bottom. For\n      example, if `num_layers=4` and `num_residual_layers=2`, the last 2 RNN",
        "detail": "nmt.nmt.model_helper",
        "documentation": {}
    },
    {
        "label": "gradient_clip",
        "kind": 2,
        "importPath": "nmt.nmt.model_helper",
        "description": "nmt.nmt.model_helper",
        "peekOfCode": "def gradient_clip(gradients, max_gradient_norm):\n  \"\"\"Clipping gradients of a model.\"\"\"\n  clipped_gradients, gradient_norm = tf.clip_by_global_norm(\n      gradients, max_gradient_norm)\n  gradient_norm_summary = [tf.summary.scalar(\"grad_norm\", gradient_norm)]\n  gradient_norm_summary.append(\n      tf.summary.scalar(\"clipped_gradient\", tf.global_norm(clipped_gradients)))\n  return clipped_gradients, gradient_norm_summary, gradient_norm\ndef print_variables_in_ckpt(ckpt_path):\n  \"\"\"Print a list of variables in a checkpoint together with their shapes.\"\"\"",
        "detail": "nmt.nmt.model_helper",
        "documentation": {}
    },
    {
        "label": "print_variables_in_ckpt",
        "kind": 2,
        "importPath": "nmt.nmt.model_helper",
        "description": "nmt.nmt.model_helper",
        "peekOfCode": "def print_variables_in_ckpt(ckpt_path):\n  \"\"\"Print a list of variables in a checkpoint together with their shapes.\"\"\"\n  utils.print_out(\"# Variables in ckpt %s\" % ckpt_path)\n  reader = tf.train.NewCheckpointReader(ckpt_path)\n  variable_map = reader.get_variable_to_shape_map()\n  for key in sorted(variable_map.keys()):\n    utils.print_out(\"  %s: %s\" % (key, variable_map[key]))\ndef load_model(model, ckpt_path, session, name):\n  \"\"\"Load model from a checkpoint.\"\"\"\n  start_time = time.time()",
        "detail": "nmt.nmt.model_helper",
        "documentation": {}
    },
    {
        "label": "load_model",
        "kind": 2,
        "importPath": "nmt.nmt.model_helper",
        "description": "nmt.nmt.model_helper",
        "peekOfCode": "def load_model(model, ckpt_path, session, name):\n  \"\"\"Load model from a checkpoint.\"\"\"\n  start_time = time.time()\n  try:\n    model.saver.restore(session, ckpt_path)\n  except tf.errors.NotFoundError as e:\n    utils.print_out(\"Can't load checkpoint\")\n    print_variables_in_ckpt(ckpt_path)\n    utils.print_out(\"%s\" % str(e))\n  session.run(tf.tables_initializer())",
        "detail": "nmt.nmt.model_helper",
        "documentation": {}
    },
    {
        "label": "avg_checkpoints",
        "kind": 2,
        "importPath": "nmt.nmt.model_helper",
        "description": "nmt.nmt.model_helper",
        "peekOfCode": "def avg_checkpoints(model_dir, num_last_checkpoints, global_step,\n                    global_step_name):\n  \"\"\"Average the last N checkpoints in the model_dir.\"\"\"\n  checkpoint_state = tf.train.get_checkpoint_state(model_dir)\n  if not checkpoint_state:\n    utils.print_out(\"# No checkpoint file found in directory: %s\" % model_dir)\n    return None\n  # Checkpoints are ordered from oldest to newest.\n  checkpoints = (\n      checkpoint_state.all_model_checkpoint_paths[-num_last_checkpoints:])",
        "detail": "nmt.nmt.model_helper",
        "documentation": {}
    },
    {
        "label": "create_or_load_model",
        "kind": 2,
        "importPath": "nmt.nmt.model_helper",
        "description": "nmt.nmt.model_helper",
        "peekOfCode": "def create_or_load_model(model, model_dir, session, name):\n  \"\"\"Create translation model and initialize or load parameters in session.\"\"\"\n  latest_ckpt = tf.train.latest_checkpoint(model_dir)\n  if latest_ckpt:\n    model = load_model(model, latest_ckpt, session, name)\n  else:\n    start_time = time.time()\n    session.run(tf.global_variables_initializer())\n    session.run(tf.tables_initializer())\n    utils.print_out(\"  created %s model with fresh parameters, time %.2fs\" %",
        "detail": "nmt.nmt.model_helper",
        "documentation": {}
    },
    {
        "label": "compute_perplexity",
        "kind": 2,
        "importPath": "nmt.nmt.model_helper",
        "description": "nmt.nmt.model_helper",
        "peekOfCode": "def compute_perplexity(model, sess, name):\n  \"\"\"Compute perplexity of the output of the model.\n  Args:\n    model: model for compute perplexity.\n    sess: tensorflow session to use.\n    name: name of the batch.\n  Returns:\n    The perplexity of the eval outputs.\n  \"\"\"\n  total_loss = 0",
        "detail": "nmt.nmt.model_helper",
        "documentation": {}
    },
    {
        "label": "__all__",
        "kind": 5,
        "importPath": "nmt.nmt.model_helper",
        "description": "nmt.nmt.model_helper",
        "peekOfCode": "__all__ = [\n    \"get_initializer\", \"get_device_str\", \"create_train_model\",\n    \"create_eval_model\", \"create_infer_model\",\n    \"create_emb_for_encoder_and_decoder\", \"create_rnn_cell\", \"gradient_clip\",\n    \"create_or_load_model\", \"load_model\", \"avg_checkpoints\",\n    \"compute_perplexity\"\n]\n# If a vocab size is greater than this value, put the embedding on cpu instead\nVOCAB_SIZE_THRESHOLD_CPU = 50000\ndef get_initializer(init_op, seed=None, init_weight=None):",
        "detail": "nmt.nmt.model_helper",
        "documentation": {}
    },
    {
        "label": "VOCAB_SIZE_THRESHOLD_CPU",
        "kind": 5,
        "importPath": "nmt.nmt.model_helper",
        "description": "nmt.nmt.model_helper",
        "peekOfCode": "VOCAB_SIZE_THRESHOLD_CPU = 50000\ndef get_initializer(init_op, seed=None, init_weight=None):\n  \"\"\"Create an initializer. init_weight is only for uniform.\"\"\"\n  if init_op == \"uniform\":\n    assert init_weight\n    return tf.random_uniform_initializer(\n        -init_weight, init_weight, seed=seed)\n  elif init_op == \"glorot_normal\":\n    return tf.keras.initializers.glorot_normal(\n        seed=seed)",
        "detail": "nmt.nmt.model_helper",
        "documentation": {}
    },
    {
        "label": "ModelTest",
        "kind": 6,
        "importPath": "nmt.nmt.model_test",
        "description": "nmt.nmt.model_test",
        "peekOfCode": "class ModelTest(tf.test.TestCase):\n  @classmethod\n  def setUpClass(cls):\n    cls.actual_vars_values = {}\n    cls.expected_vars_values = {\n        'AttentionMechanismBahdanau/att_layer_weight/shape': (10, 5),\n        'AttentionMechanismBahdanau/att_layer_weight/sum':\n            -0.64981574,\n        'AttentionMechanismBahdanau/last_dec_weight/shape': (10, 20),\n        'AttentionMechanismBahdanau/last_dec_weight/sum':",
        "detail": "nmt.nmt.model_test",
        "documentation": {}
    },
    {
        "label": "float32",
        "kind": 5,
        "importPath": "nmt.nmt.model_test",
        "description": "nmt.nmt.model_test",
        "peekOfCode": "float32 = np.float32\nint32 = np.int32\narray = np.array\nSOS = '<s>'\nEOS = '</s>'\nclass ModelTest(tf.test.TestCase):\n  @classmethod\n  def setUpClass(cls):\n    cls.actual_vars_values = {}\n    cls.expected_vars_values = {",
        "detail": "nmt.nmt.model_test",
        "documentation": {}
    },
    {
        "label": "int32",
        "kind": 5,
        "importPath": "nmt.nmt.model_test",
        "description": "nmt.nmt.model_test",
        "peekOfCode": "int32 = np.int32\narray = np.array\nSOS = '<s>'\nEOS = '</s>'\nclass ModelTest(tf.test.TestCase):\n  @classmethod\n  def setUpClass(cls):\n    cls.actual_vars_values = {}\n    cls.expected_vars_values = {\n        'AttentionMechanismBahdanau/att_layer_weight/shape': (10, 5),",
        "detail": "nmt.nmt.model_test",
        "documentation": {}
    },
    {
        "label": "array",
        "kind": 5,
        "importPath": "nmt.nmt.model_test",
        "description": "nmt.nmt.model_test",
        "peekOfCode": "array = np.array\nSOS = '<s>'\nEOS = '</s>'\nclass ModelTest(tf.test.TestCase):\n  @classmethod\n  def setUpClass(cls):\n    cls.actual_vars_values = {}\n    cls.expected_vars_values = {\n        'AttentionMechanismBahdanau/att_layer_weight/shape': (10, 5),\n        'AttentionMechanismBahdanau/att_layer_weight/sum':",
        "detail": "nmt.nmt.model_test",
        "documentation": {}
    },
    {
        "label": "SOS",
        "kind": 5,
        "importPath": "nmt.nmt.model_test",
        "description": "nmt.nmt.model_test",
        "peekOfCode": "SOS = '<s>'\nEOS = '</s>'\nclass ModelTest(tf.test.TestCase):\n  @classmethod\n  def setUpClass(cls):\n    cls.actual_vars_values = {}\n    cls.expected_vars_values = {\n        'AttentionMechanismBahdanau/att_layer_weight/shape': (10, 5),\n        'AttentionMechanismBahdanau/att_layer_weight/sum':\n            -0.64981574,",
        "detail": "nmt.nmt.model_test",
        "documentation": {}
    },
    {
        "label": "EOS",
        "kind": 5,
        "importPath": "nmt.nmt.model_test",
        "description": "nmt.nmt.model_test",
        "peekOfCode": "EOS = '</s>'\nclass ModelTest(tf.test.TestCase):\n  @classmethod\n  def setUpClass(cls):\n    cls.actual_vars_values = {}\n    cls.expected_vars_values = {\n        'AttentionMechanismBahdanau/att_layer_weight/shape': (10, 5),\n        'AttentionMechanismBahdanau/att_layer_weight/sum':\n            -0.64981574,\n        'AttentionMechanismBahdanau/last_dec_weight/shape': (10, 20),",
        "detail": "nmt.nmt.model_test",
        "documentation": {}
    },
    {
        "label": "add_arguments",
        "kind": 2,
        "importPath": "nmt.nmt.nmt",
        "description": "nmt.nmt.nmt",
        "peekOfCode": "def add_arguments(parser):\n  \"\"\"Build ArgumentParser.\"\"\"\n  parser.register(\"type\", \"bool\", lambda v: v.lower() == \"true\")\n  # network\n  parser.add_argument(\"--num_units\", type=int, default=32, help=\"Network size.\")\n  parser.add_argument(\"--num_layers\", type=int, default=2,\n                      help=\"Network depth.\")\n  parser.add_argument(\"--num_encoder_layers\", type=int, default=None,\n                      help=\"Encoder depth, equal to num_layers if None.\")\n  parser.add_argument(\"--num_decoder_layers\", type=int, default=None,",
        "detail": "nmt.nmt.nmt",
        "documentation": {}
    },
    {
        "label": "create_hparams",
        "kind": 2,
        "importPath": "nmt.nmt.nmt",
        "description": "nmt.nmt.nmt",
        "peekOfCode": "def create_hparams(flags):\n  \"\"\"Create training hparams.\"\"\"\n  return tf.contrib.training.HParams(\n      # Data\n      src=flags.src,\n      tgt=flags.tgt,\n      train_prefix=flags.train_prefix,\n      dev_prefix=flags.dev_prefix,\n      test_prefix=flags.test_prefix,\n      vocab_prefix=flags.vocab_prefix,",
        "detail": "nmt.nmt.nmt",
        "documentation": {}
    },
    {
        "label": "extend_hparams",
        "kind": 2,
        "importPath": "nmt.nmt.nmt",
        "description": "nmt.nmt.nmt",
        "peekOfCode": "def extend_hparams(hparams):\n  \"\"\"Add new arguments to hparams.\"\"\"\n  # Sanity checks\n  if hparams.encoder_type == \"bi\" and hparams.num_encoder_layers % 2 != 0:\n    raise ValueError(\"For bi, num_encoder_layers %d should be even\" %\n                     hparams.num_encoder_layers)\n  if (hparams.attention_architecture in [\"gnmt\"] and\n      hparams.num_encoder_layers < 2):\n    raise ValueError(\"For gnmt attention architecture, \"\n                     \"num_encoder_layers %d should be >= 2\" %",
        "detail": "nmt.nmt.nmt",
        "documentation": {}
    },
    {
        "label": "ensure_compatible_hparams",
        "kind": 2,
        "importPath": "nmt.nmt.nmt",
        "description": "nmt.nmt.nmt",
        "peekOfCode": "def ensure_compatible_hparams(hparams, default_hparams, hparams_path=\"\"):\n  \"\"\"Make sure the loaded hparams is compatible with new changes.\"\"\"\n  default_hparams = utils.maybe_parse_standard_hparams(\n      default_hparams, hparams_path)\n  # Set num encoder/decoder layers (for old checkpoints)\n  if hasattr(hparams, \"num_layers\"):\n    if not hasattr(hparams, \"num_encoder_layers\"):\n      hparams.add_hparam(\"num_encoder_layers\", hparams.num_layers)\n    if not hasattr(hparams, \"num_decoder_layers\"):\n      hparams.add_hparam(\"num_decoder_layers\", hparams.num_layers)",
        "detail": "nmt.nmt.nmt",
        "documentation": {}
    },
    {
        "label": "create_or_load_hparams",
        "kind": 2,
        "importPath": "nmt.nmt.nmt",
        "description": "nmt.nmt.nmt",
        "peekOfCode": "def create_or_load_hparams(\n    out_dir, default_hparams, hparams_path, save_hparams=True):\n  \"\"\"Create hparams or load hparams from out_dir.\"\"\"\n  hparams = utils.load_hparams(out_dir)\n  if not hparams:\n    hparams = default_hparams\n    hparams = utils.maybe_parse_standard_hparams(\n        hparams, hparams_path)\n  else:\n    hparams = ensure_compatible_hparams(hparams, default_hparams, hparams_path)",
        "detail": "nmt.nmt.nmt",
        "documentation": {}
    },
    {
        "label": "run_main",
        "kind": 2,
        "importPath": "nmt.nmt.nmt",
        "description": "nmt.nmt.nmt",
        "peekOfCode": "def run_main(flags, default_hparams, train_fn, inference_fn, target_session=\"\"):\n  \"\"\"Run main.\"\"\"\n  # Job\n  jobid = flags.jobid\n  num_workers = flags.num_workers\n  utils.print_out(\"# Job id %d\" % jobid)\n  # GPU device\n  utils.print_out(\n      \"# Devices visible to TensorFlow: %s\" % repr(tf.Session().list_devices()))\n  # Random",
        "detail": "nmt.nmt.nmt",
        "documentation": {}
    },
    {
        "label": "main",
        "kind": 2,
        "importPath": "nmt.nmt.nmt",
        "description": "nmt.nmt.nmt",
        "peekOfCode": "def main(unused_argv):\n  default_hparams = create_hparams(FLAGS)\n  train_fn = train.train\n  inference_fn = inference.inference\n  run_main(FLAGS, default_hparams, train_fn, inference_fn)\nif __name__ == \"__main__\":\n  nmt_parser = argparse.ArgumentParser()\n  add_arguments(nmt_parser)\n  FLAGS, unparsed = nmt_parser.parse_known_args()\n  tf.app.run(main=main, argv=[sys.argv[0]] + unparsed)",
        "detail": "nmt.nmt.nmt",
        "documentation": {}
    },
    {
        "label": "FLAGS",
        "kind": 5,
        "importPath": "nmt.nmt.nmt",
        "description": "nmt.nmt.nmt",
        "peekOfCode": "FLAGS = None\nINFERENCE_KEYS = [\"src_max_len_infer\", \"tgt_max_len_infer\", \"subword_option\",\n                  \"infer_batch_size\", \"beam_width\",\n                  \"length_penalty_weight\", \"coverage_penalty_weight\",\n                  \"sampling_temperature\", \"num_translations_per_input\",\n                  \"infer_mode\"]\ndef add_arguments(parser):\n  \"\"\"Build ArgumentParser.\"\"\"\n  parser.register(\"type\", \"bool\", lambda v: v.lower() == \"true\")\n  # network",
        "detail": "nmt.nmt.nmt",
        "documentation": {}
    },
    {
        "label": "INFERENCE_KEYS",
        "kind": 5,
        "importPath": "nmt.nmt.nmt",
        "description": "nmt.nmt.nmt",
        "peekOfCode": "INFERENCE_KEYS = [\"src_max_len_infer\", \"tgt_max_len_infer\", \"subword_option\",\n                  \"infer_batch_size\", \"beam_width\",\n                  \"length_penalty_weight\", \"coverage_penalty_weight\",\n                  \"sampling_temperature\", \"num_translations_per_input\",\n                  \"infer_mode\"]\ndef add_arguments(parser):\n  \"\"\"Build ArgumentParser.\"\"\"\n  parser.register(\"type\", \"bool\", lambda v: v.lower() == \"true\")\n  # network\n  parser.add_argument(\"--num_units\", type=int, default=32, help=\"Network size.\")",
        "detail": "nmt.nmt.nmt",
        "documentation": {}
    },
    {
        "label": "NMTTest",
        "kind": 6,
        "importPath": "nmt.nmt.nmt_test",
        "description": "nmt.nmt.nmt_test",
        "peekOfCode": "class NMTTest(tf.test.TestCase):\n  def testTrain(self):\n    \"\"\"Test the training loop is functional with basic hparams.\"\"\"\n    nmt_parser = argparse.ArgumentParser()\n    nmt.add_arguments(nmt_parser)\n    FLAGS, unparsed = nmt_parser.parse_known_args()\n    _update_flags(FLAGS, \"nmt_train_test\")\n    default_hparams = nmt.create_hparams(FLAGS)\n    train_fn = train.train\n    nmt.run_main(FLAGS, default_hparams, train_fn, None)",
        "detail": "nmt.nmt.nmt_test",
        "documentation": {}
    },
    {
        "label": "run_sample_decode",
        "kind": 2,
        "importPath": "nmt.nmt.train",
        "description": "nmt.nmt.train",
        "peekOfCode": "def run_sample_decode(infer_model, infer_sess, model_dir, hparams,\n                      summary_writer, src_data, tgt_data):\n  \"\"\"Sample decode a random sentence from src_data.\"\"\"\n  with infer_model.graph.as_default():\n    loaded_infer_model, global_step = model_helper.create_or_load_model(\n        infer_model.model, model_dir, infer_sess, \"infer\")\n  _sample_decode(loaded_infer_model, global_step, infer_sess, hparams,\n                 infer_model.iterator, src_data, tgt_data,\n                 infer_model.src_placeholder,\n                 infer_model.batch_size_placeholder, summary_writer)",
        "detail": "nmt.nmt.train",
        "documentation": {}
    },
    {
        "label": "run_internal_eval",
        "kind": 2,
        "importPath": "nmt.nmt.train",
        "description": "nmt.nmt.train",
        "peekOfCode": "def run_internal_eval(eval_model,\n                      eval_sess,\n                      model_dir,\n                      hparams,\n                      summary_writer,\n                      use_test_set=True,\n                      dev_eval_iterator_feed_dict=None,\n                      test_eval_iterator_feed_dict=None):\n  \"\"\"Compute internal evaluation (perplexity) for both dev / test.\n  Computes development and testing perplexities for given model.",
        "detail": "nmt.nmt.train",
        "documentation": {}
    },
    {
        "label": "run_external_eval",
        "kind": 2,
        "importPath": "nmt.nmt.train",
        "description": "nmt.nmt.train",
        "peekOfCode": "def run_external_eval(infer_model,\n                      infer_sess,\n                      model_dir,\n                      hparams,\n                      summary_writer,\n                      save_best_dev=True,\n                      use_test_set=True,\n                      avg_ckpts=False,\n                      dev_infer_iterator_feed_dict=None,\n                      test_infer_iterator_feed_dict=None):",
        "detail": "nmt.nmt.train",
        "documentation": {}
    },
    {
        "label": "run_avg_external_eval",
        "kind": 2,
        "importPath": "nmt.nmt.train",
        "description": "nmt.nmt.train",
        "peekOfCode": "def run_avg_external_eval(infer_model, infer_sess, model_dir, hparams,\n                          summary_writer, global_step):\n  \"\"\"Creates an averaged checkpoint and run external eval with it.\"\"\"\n  avg_dev_scores, avg_test_scores = None, None\n  if hparams.avg_ckpts:\n    # Convert VariableName:0 to VariableName.\n    global_step_name = infer_model.model.global_step.name.split(\":\")[0]\n    avg_model_dir = model_helper.avg_checkpoints(\n        model_dir, hparams.num_keep_ckpts, global_step, global_step_name)\n    if avg_model_dir:",
        "detail": "nmt.nmt.train",
        "documentation": {}
    },
    {
        "label": "run_internal_and_external_eval",
        "kind": 2,
        "importPath": "nmt.nmt.train",
        "description": "nmt.nmt.train",
        "peekOfCode": "def run_internal_and_external_eval(model_dir,\n                                   infer_model,\n                                   infer_sess,\n                                   eval_model,\n                                   eval_sess,\n                                   hparams,\n                                   summary_writer,\n                                   avg_ckpts=False,\n                                   dev_eval_iterator_feed_dict=None,\n                                   test_eval_iterator_feed_dict=None,",
        "detail": "nmt.nmt.train",
        "documentation": {}
    },
    {
        "label": "run_full_eval",
        "kind": 2,
        "importPath": "nmt.nmt.train",
        "description": "nmt.nmt.train",
        "peekOfCode": "def run_full_eval(model_dir,\n                  infer_model,\n                  infer_sess,\n                  eval_model,\n                  eval_sess,\n                  hparams,\n                  summary_writer,\n                  sample_src_data,\n                  sample_tgt_data,\n                  avg_ckpts=False):",
        "detail": "nmt.nmt.train",
        "documentation": {}
    },
    {
        "label": "init_stats",
        "kind": 2,
        "importPath": "nmt.nmt.train",
        "description": "nmt.nmt.train",
        "peekOfCode": "def init_stats():\n  \"\"\"Initialize statistics that we want to accumulate.\"\"\"\n  return {\"step_time\": 0.0, \"train_loss\": 0.0,\n          \"predict_count\": 0.0,  # word count on the target side\n          \"word_count\": 0.0,  # word counts for both source and target\n          \"sequence_count\": 0.0,  # number of training examples processed\n          \"grad_norm\": 0.0}\ndef update_stats(stats, start_time, step_result):\n  \"\"\"Update stats: write summary and accumulate statistics.\"\"\"\n  _, output_tuple = step_result",
        "detail": "nmt.nmt.train",
        "documentation": {}
    },
    {
        "label": "update_stats",
        "kind": 2,
        "importPath": "nmt.nmt.train",
        "description": "nmt.nmt.train",
        "peekOfCode": "def update_stats(stats, start_time, step_result):\n  \"\"\"Update stats: write summary and accumulate statistics.\"\"\"\n  _, output_tuple = step_result\n  # Update statistics\n  batch_size = output_tuple.batch_size\n  stats[\"step_time\"] += time.time() - start_time\n  stats[\"train_loss\"] += output_tuple.train_loss * batch_size\n  stats[\"grad_norm\"] += output_tuple.grad_norm\n  stats[\"predict_count\"] += output_tuple.predict_count\n  stats[\"word_count\"] += output_tuple.word_count",
        "detail": "nmt.nmt.train",
        "documentation": {}
    },
    {
        "label": "print_step_info",
        "kind": 2,
        "importPath": "nmt.nmt.train",
        "description": "nmt.nmt.train",
        "peekOfCode": "def print_step_info(prefix, global_step, info, result_summary, log_f):\n  \"\"\"Print all info at the current global step.\"\"\"\n  utils.print_out(\n      \"%sstep %d lr %g step-time %.2fs wps %.2fK ppl %.2f gN %.2f %s, %s\" %\n      (prefix, global_step, info[\"learning_rate\"], info[\"avg_step_time\"],\n       info[\"speed\"], info[\"train_ppl\"], info[\"avg_grad_norm\"], result_summary,\n       time.ctime()),\n      log_f)\ndef add_info_summaries(summary_writer, global_step, info):\n  \"\"\"Add stuffs in info to summaries.\"\"\"",
        "detail": "nmt.nmt.train",
        "documentation": {}
    },
    {
        "label": "add_info_summaries",
        "kind": 2,
        "importPath": "nmt.nmt.train",
        "description": "nmt.nmt.train",
        "peekOfCode": "def add_info_summaries(summary_writer, global_step, info):\n  \"\"\"Add stuffs in info to summaries.\"\"\"\n  excluded_list = [\"learning_rate\"]\n  for key in info:\n    if key not in excluded_list:\n      utils.add_summary(summary_writer, global_step, key, info[key])\ndef process_stats(stats, info, global_step, steps_per_stats, log_f):\n  \"\"\"Update info and check for overflow.\"\"\"\n  # Per-step info\n  info[\"avg_step_time\"] = stats[\"step_time\"] / steps_per_stats",
        "detail": "nmt.nmt.train",
        "documentation": {}
    },
    {
        "label": "process_stats",
        "kind": 2,
        "importPath": "nmt.nmt.train",
        "description": "nmt.nmt.train",
        "peekOfCode": "def process_stats(stats, info, global_step, steps_per_stats, log_f):\n  \"\"\"Update info and check for overflow.\"\"\"\n  # Per-step info\n  info[\"avg_step_time\"] = stats[\"step_time\"] / steps_per_stats\n  info[\"avg_grad_norm\"] = stats[\"grad_norm\"] / steps_per_stats\n  info[\"avg_sequence_count\"] = stats[\"sequence_count\"] / steps_per_stats\n  info[\"speed\"] = stats[\"word_count\"] / (1000 * stats[\"step_time\"])\n  # Per-predict info\n  info[\"train_ppl\"] = (\n      utils.safe_exp(stats[\"train_loss\"] / stats[\"predict_count\"]))",
        "detail": "nmt.nmt.train",
        "documentation": {}
    },
    {
        "label": "before_train",
        "kind": 2,
        "importPath": "nmt.nmt.train",
        "description": "nmt.nmt.train",
        "peekOfCode": "def before_train(loaded_train_model, train_model, train_sess, global_step,\n                 hparams, log_f):\n  \"\"\"Misc tasks to do before training.\"\"\"\n  stats = init_stats()\n  info = {\"train_ppl\": 0.0, \"speed\": 0.0,\n          \"avg_step_time\": 0.0,\n          \"avg_grad_norm\": 0.0,\n          \"avg_sequence_count\": 0.0,\n          \"learning_rate\": loaded_train_model.learning_rate.eval(\n              session=train_sess)}",
        "detail": "nmt.nmt.train",
        "documentation": {}
    },
    {
        "label": "get_model_creator",
        "kind": 2,
        "importPath": "nmt.nmt.train",
        "description": "nmt.nmt.train",
        "peekOfCode": "def get_model_creator(hparams):\n  \"\"\"Get the right model class depending on configuration.\"\"\"\n  if (hparams.encoder_type == \"gnmt\" or\n      hparams.attention_architecture in [\"gnmt\", \"gnmt_v2\"]):\n    model_creator = gnmt_model.GNMTModel\n  elif hparams.attention and hparams.attention_architecture == \"standard\":\n    model_creator = attention_model.AttentionModel\n  elif not hparams.attention:\n    model_creator = nmt_model.Model\n  else:",
        "detail": "nmt.nmt.train",
        "documentation": {}
    },
    {
        "label": "train",
        "kind": 2,
        "importPath": "nmt.nmt.train",
        "description": "nmt.nmt.train",
        "peekOfCode": "def train(hparams, scope=None, target_session=\"\"):\n  \"\"\"Train a translation model.\"\"\"\n  log_device_placement = hparams.log_device_placement\n  out_dir = hparams.out_dir\n  num_train_steps = hparams.num_train_steps\n  steps_per_stats = hparams.steps_per_stats\n  steps_per_external_eval = hparams.steps_per_external_eval\n  steps_per_eval = 10 * steps_per_stats\n  avg_ckpts = hparams.avg_ckpts\n  if not steps_per_external_eval:",
        "detail": "nmt.nmt.train",
        "documentation": {}
    },
    {
        "label": "get_best_results",
        "kind": 2,
        "importPath": "nmt.nmt.train",
        "description": "nmt.nmt.train",
        "peekOfCode": "def get_best_results(hparams):\n  \"\"\"Summary of the current best results.\"\"\"\n  tokens = []\n  for metric in hparams.metrics:\n    tokens.append(\"%s %.2f\" % (metric, getattr(hparams, \"best_\" + metric)))\n  return \", \".join(tokens)\ndef _internal_eval(model, global_step, sess, iterator, iterator_feed_dict,\n                   summary_writer, label):\n  \"\"\"Computing perplexity.\"\"\"\n  sess.run(iterator.initializer, feed_dict=iterator_feed_dict)",
        "detail": "nmt.nmt.train",
        "documentation": {}
    },
    {
        "label": "__all__",
        "kind": 5,
        "importPath": "nmt.nmt.train",
        "description": "nmt.nmt.train",
        "peekOfCode": "__all__ = [\n    \"run_sample_decode\", \"run_internal_eval\", \"run_external_eval\",\n    \"run_avg_external_eval\", \"run_full_eval\", \"init_stats\", \"update_stats\",\n    \"print_step_info\", \"process_stats\", \"train\", \"get_model_creator\",\n    \"add_info_summaries\", \"get_best_results\"\n]\ndef run_sample_decode(infer_model, infer_sess, model_dir, hparams,\n                      summary_writer, src_data, tgt_data):\n  \"\"\"Sample decode a random sentence from src_data.\"\"\"\n  with infer_model.graph.as_default():",
        "detail": "nmt.nmt.train",
        "documentation": {}
    }
]