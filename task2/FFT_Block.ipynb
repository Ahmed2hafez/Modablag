{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {
        "id": "L9HoBgUZQlLn"
      },
      "outputs": [],
      "source": [
        "# fft block --> Feed Forward Transformer\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 194,
      "metadata": {
        "id": "2hjqYFYhQ9DT"
      },
      "outputs": [],
      "source": [
        "class FFTBlock(tf.keras.layers.Layer):\n",
        "  def __init__(self,**kwargs):\n",
        "    super().__init__(name='')\n",
        "\n",
        "    #self.mha = GlobalSelfAttention(num_heads=2, key_dim=2)\n",
        "    self.mha = tf.keras.layers.MultiHeadAttention(num_heads=2,key_dim=1)\n",
        "\n",
        "    self.layernorm = tf.keras.layers.LayerNormalization()\n",
        "    self.add = tf.keras.layers.Add()\n",
        "\n",
        "    self.firstconv1d = tf.keras.layers.Conv1D(filters=512,kernel_size=5,strides=4,activation=\"relu\")\n",
        "    self.secondconv1d = tf.keras.layers.Conv1D(filters=256,kernel_size=5,strides=4,activation=\"relu\")\n",
        "    self.norm = tf.keras.layers.LayerNormalization()\n",
        "\n",
        "  def call(self,x):\n",
        "    MHA = self.mha(query=x,key=x,value=x)\n",
        "    MHA = self.add([x,MHA]) \n",
        "    MHA = self.layernorm(MHA)\n",
        "    CNN = self.firstconv1d(MHA)\n",
        "    CNN = self.secondconv1d(CNN)\n",
        "    MHA = self.norm(CNN)\n",
        "    return MHA"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
